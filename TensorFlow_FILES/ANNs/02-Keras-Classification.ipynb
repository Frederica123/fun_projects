{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras TF 2.0 - Code Along Classification Project\n",
    "\n",
    "Let's explore a classification task with Keras API for TF 2.0\n",
    "\n",
    "## The Data\n",
    "\n",
    "### Breast cancer wisconsin (diagnostic) dataset\n",
    "--------------------------------------------\n",
    "\n",
    "**Data Set Characteristics:**\n",
    "\n",
    "    :Number of Instances: 569\n",
    "\n",
    "    :Number of Attributes: 30 numeric, predictive attributes and the class\n",
    "\n",
    "    :Attribute Information:\n",
    "        - radius (mean of distances from center to points on the perimeter)\n",
    "        - texture (standard deviation of gray-scale values)\n",
    "        - perimeter\n",
    "        - area\n",
    "        - smoothness (local variation in radius lengths)\n",
    "        - compactness (perimeter^2 / area - 1.0)\n",
    "        - concavity (severity of concave portions of the contour)\n",
    "        - concave points (number of concave portions of the contour)\n",
    "        - symmetry \n",
    "        - fractal dimension (\"coastline approximation\" - 1)\n",
    "\n",
    "        The mean, standard error, and \"worst\" or largest (mean of the three\n",
    "        largest values) of these features were computed for each image,\n",
    "        resulting in 30 features.  For instance, field 3 is Mean Radius, field\n",
    "        13 is Radius SE, field 23 is Worst Radius.\n",
    "\n",
    "        - class:\n",
    "                - WDBC-Malignant\n",
    "                - WDBC-Benign\n",
    "\n",
    "    :Summary Statistics:\n",
    "\n",
    "    ===================================== ====== ======\n",
    "                                           Min    Max\n",
    "    ===================================== ====== ======\n",
    "    radius (mean):                        6.981  28.11\n",
    "    texture (mean):                       9.71   39.28\n",
    "    perimeter (mean):                     43.79  188.5\n",
    "    area (mean):                          143.5  2501.0\n",
    "    smoothness (mean):                    0.053  0.163\n",
    "    compactness (mean):                   0.019  0.345\n",
    "    concavity (mean):                     0.0    0.427\n",
    "    concave points (mean):                0.0    0.201\n",
    "    symmetry (mean):                      0.106  0.304\n",
    "    fractal dimension (mean):             0.05   0.097\n",
    "    radius (standard error):              0.112  2.873\n",
    "    texture (standard error):             0.36   4.885\n",
    "    perimeter (standard error):           0.757  21.98\n",
    "    area (standard error):                6.802  542.2\n",
    "    smoothness (standard error):          0.002  0.031\n",
    "    compactness (standard error):         0.002  0.135\n",
    "    concavity (standard error):           0.0    0.396\n",
    "    concave points (standard error):      0.0    0.053\n",
    "    symmetry (standard error):            0.008  0.079\n",
    "    fractal dimension (standard error):   0.001  0.03\n",
    "    radius (worst):                       7.93   36.04\n",
    "    texture (worst):                      12.02  49.54\n",
    "    perimeter (worst):                    50.41  251.2\n",
    "    area (worst):                         185.2  4254.0\n",
    "    smoothness (worst):                   0.071  0.223\n",
    "    compactness (worst):                  0.027  1.058\n",
    "    concavity (worst):                    0.0    1.252\n",
    "    concave points (worst):               0.0    0.291\n",
    "    symmetry (worst):                     0.156  0.664\n",
    "    fractal dimension (worst):            0.055  0.208\n",
    "    ===================================== ====== ======\n",
    "\n",
    "    :Missing Attribute Values: None\n",
    "\n",
    "    :Class Distribution: 212 - Malignant, 357 - Benign\n",
    "\n",
    "    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\n",
    "\n",
    "    :Donor: Nick Street\n",
    "\n",
    "    :Date: November, 1995\n",
    "\n",
    "This is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\n",
    "https://goo.gl/U2Uwz2\n",
    "\n",
    "Features are computed from a digitized image of a fine needle\n",
    "aspirate (FNA) of a breast mass.  They describe\n",
    "characteristics of the cell nuclei present in the image.\n",
    "\n",
    "Separating plane described above was obtained using\n",
    "Multisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\n",
    "Construction Via Linear Programming.\" Proceedings of the 4th\n",
    "Midwest Artificial Intelligence and Cognitive Science Society,\n",
    "pp. 97-101, 1992], a classification method which uses linear\n",
    "programming to construct a decision tree.  Relevant features\n",
    "were selected using an exhaustive search in the space of 1-4\n",
    "features and 1-3 separating planes.\n",
    "\n",
    "The actual linear program used to obtain the separating plane\n",
    "in the 3-dimensional space is that described in:\n",
    "[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\n",
    "Programming Discrimination of Two Linearly Inseparable Sets\",\n",
    "Optimization Methods and Software 1, 1992, 23-34].\n",
    "\n",
    "This database is also available through the UW CS ftp server:\n",
    "\n",
    "ftp ftp.cs.wisc.edu\n",
    "cd math-prog/cpo-dataset/machine-learn/WDBC/\n",
    "\n",
    ".. topic:: References\n",
    "\n",
    "   - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \n",
    "     for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on \n",
    "     Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\n",
    "     San Jose, CA, 1993.\n",
    "   - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \n",
    "     prognosis via linear programming. Operations Research, 43(4), pages 570-577, \n",
    "     July-August 1995.\n",
    "   - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\n",
    "     to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \n",
    "     163-171."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../DATA/cancer_classification.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 569 entries, 0 to 568\n",
      "Data columns (total 31 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   mean radius              569 non-null    float64\n",
      " 1   mean texture             569 non-null    float64\n",
      " 2   mean perimeter           569 non-null    float64\n",
      " 3   mean area                569 non-null    float64\n",
      " 4   mean smoothness          569 non-null    float64\n",
      " 5   mean compactness         569 non-null    float64\n",
      " 6   mean concavity           569 non-null    float64\n",
      " 7   mean concave points      569 non-null    float64\n",
      " 8   mean symmetry            569 non-null    float64\n",
      " 9   mean fractal dimension   569 non-null    float64\n",
      " 10  radius error             569 non-null    float64\n",
      " 11  texture error            569 non-null    float64\n",
      " 12  perimeter error          569 non-null    float64\n",
      " 13  area error               569 non-null    float64\n",
      " 14  smoothness error         569 non-null    float64\n",
      " 15  compactness error        569 non-null    float64\n",
      " 16  concavity error          569 non-null    float64\n",
      " 17  concave points error     569 non-null    float64\n",
      " 18  symmetry error           569 non-null    float64\n",
      " 19  fractal dimension error  569 non-null    float64\n",
      " 20  worst radius             569 non-null    float64\n",
      " 21  worst texture            569 non-null    float64\n",
      " 22  worst perimeter          569 non-null    float64\n",
      " 23  worst area               569 non-null    float64\n",
      " 24  worst smoothness         569 non-null    float64\n",
      " 25  worst compactness        569 non-null    float64\n",
      " 26  worst concavity          569 non-null    float64\n",
      " 27  worst concave points     569 non-null    float64\n",
      " 28  worst symmetry           569 non-null    float64\n",
      " 29  worst fractal dimension  569 non-null    float64\n",
      " 30  benign_0__mal_1          569 non-null    int64  \n",
      "dtypes: float64(30), int64(1)\n",
      "memory usage: 137.9 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean radius</th>\n",
       "      <td>569.0</td>\n",
       "      <td>14.127292</td>\n",
       "      <td>3.524049</td>\n",
       "      <td>6.981000</td>\n",
       "      <td>11.700000</td>\n",
       "      <td>13.370000</td>\n",
       "      <td>15.780000</td>\n",
       "      <td>28.11000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean texture</th>\n",
       "      <td>569.0</td>\n",
       "      <td>19.289649</td>\n",
       "      <td>4.301036</td>\n",
       "      <td>9.710000</td>\n",
       "      <td>16.170000</td>\n",
       "      <td>18.840000</td>\n",
       "      <td>21.800000</td>\n",
       "      <td>39.28000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean perimeter</th>\n",
       "      <td>569.0</td>\n",
       "      <td>91.969033</td>\n",
       "      <td>24.298981</td>\n",
       "      <td>43.790000</td>\n",
       "      <td>75.170000</td>\n",
       "      <td>86.240000</td>\n",
       "      <td>104.100000</td>\n",
       "      <td>188.50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean area</th>\n",
       "      <td>569.0</td>\n",
       "      <td>654.889104</td>\n",
       "      <td>351.914129</td>\n",
       "      <td>143.500000</td>\n",
       "      <td>420.300000</td>\n",
       "      <td>551.100000</td>\n",
       "      <td>782.700000</td>\n",
       "      <td>2501.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean smoothness</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.096360</td>\n",
       "      <td>0.014064</td>\n",
       "      <td>0.052630</td>\n",
       "      <td>0.086370</td>\n",
       "      <td>0.095870</td>\n",
       "      <td>0.105300</td>\n",
       "      <td>0.16340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean compactness</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.104341</td>\n",
       "      <td>0.052813</td>\n",
       "      <td>0.019380</td>\n",
       "      <td>0.064920</td>\n",
       "      <td>0.092630</td>\n",
       "      <td>0.130400</td>\n",
       "      <td>0.34540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean concavity</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.088799</td>\n",
       "      <td>0.079720</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029560</td>\n",
       "      <td>0.061540</td>\n",
       "      <td>0.130700</td>\n",
       "      <td>0.42680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean concave points</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.048919</td>\n",
       "      <td>0.038803</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020310</td>\n",
       "      <td>0.033500</td>\n",
       "      <td>0.074000</td>\n",
       "      <td>0.20120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean symmetry</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.181162</td>\n",
       "      <td>0.027414</td>\n",
       "      <td>0.106000</td>\n",
       "      <td>0.161900</td>\n",
       "      <td>0.179200</td>\n",
       "      <td>0.195700</td>\n",
       "      <td>0.30400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.062798</td>\n",
       "      <td>0.007060</td>\n",
       "      <td>0.049960</td>\n",
       "      <td>0.057700</td>\n",
       "      <td>0.061540</td>\n",
       "      <td>0.066120</td>\n",
       "      <td>0.09744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>radius error</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.405172</td>\n",
       "      <td>0.277313</td>\n",
       "      <td>0.111500</td>\n",
       "      <td>0.232400</td>\n",
       "      <td>0.324200</td>\n",
       "      <td>0.478900</td>\n",
       "      <td>2.87300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>texture error</th>\n",
       "      <td>569.0</td>\n",
       "      <td>1.216853</td>\n",
       "      <td>0.551648</td>\n",
       "      <td>0.360200</td>\n",
       "      <td>0.833900</td>\n",
       "      <td>1.108000</td>\n",
       "      <td>1.474000</td>\n",
       "      <td>4.88500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>perimeter error</th>\n",
       "      <td>569.0</td>\n",
       "      <td>2.866059</td>\n",
       "      <td>2.021855</td>\n",
       "      <td>0.757000</td>\n",
       "      <td>1.606000</td>\n",
       "      <td>2.287000</td>\n",
       "      <td>3.357000</td>\n",
       "      <td>21.98000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>area error</th>\n",
       "      <td>569.0</td>\n",
       "      <td>40.337079</td>\n",
       "      <td>45.491006</td>\n",
       "      <td>6.802000</td>\n",
       "      <td>17.850000</td>\n",
       "      <td>24.530000</td>\n",
       "      <td>45.190000</td>\n",
       "      <td>542.20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smoothness error</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.007041</td>\n",
       "      <td>0.003003</td>\n",
       "      <td>0.001713</td>\n",
       "      <td>0.005169</td>\n",
       "      <td>0.006380</td>\n",
       "      <td>0.008146</td>\n",
       "      <td>0.03113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>compactness error</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.025478</td>\n",
       "      <td>0.017908</td>\n",
       "      <td>0.002252</td>\n",
       "      <td>0.013080</td>\n",
       "      <td>0.020450</td>\n",
       "      <td>0.032450</td>\n",
       "      <td>0.13540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>concavity error</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.031894</td>\n",
       "      <td>0.030186</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015090</td>\n",
       "      <td>0.025890</td>\n",
       "      <td>0.042050</td>\n",
       "      <td>0.39600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>concave points error</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.011796</td>\n",
       "      <td>0.006170</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007638</td>\n",
       "      <td>0.010930</td>\n",
       "      <td>0.014710</td>\n",
       "      <td>0.05279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>symmetry error</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.020542</td>\n",
       "      <td>0.008266</td>\n",
       "      <td>0.007882</td>\n",
       "      <td>0.015160</td>\n",
       "      <td>0.018730</td>\n",
       "      <td>0.023480</td>\n",
       "      <td>0.07895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fractal dimension error</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.003795</td>\n",
       "      <td>0.002646</td>\n",
       "      <td>0.000895</td>\n",
       "      <td>0.002248</td>\n",
       "      <td>0.003187</td>\n",
       "      <td>0.004558</td>\n",
       "      <td>0.02984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst radius</th>\n",
       "      <td>569.0</td>\n",
       "      <td>16.269190</td>\n",
       "      <td>4.833242</td>\n",
       "      <td>7.930000</td>\n",
       "      <td>13.010000</td>\n",
       "      <td>14.970000</td>\n",
       "      <td>18.790000</td>\n",
       "      <td>36.04000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst texture</th>\n",
       "      <td>569.0</td>\n",
       "      <td>25.677223</td>\n",
       "      <td>6.146258</td>\n",
       "      <td>12.020000</td>\n",
       "      <td>21.080000</td>\n",
       "      <td>25.410000</td>\n",
       "      <td>29.720000</td>\n",
       "      <td>49.54000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst perimeter</th>\n",
       "      <td>569.0</td>\n",
       "      <td>107.261213</td>\n",
       "      <td>33.602542</td>\n",
       "      <td>50.410000</td>\n",
       "      <td>84.110000</td>\n",
       "      <td>97.660000</td>\n",
       "      <td>125.400000</td>\n",
       "      <td>251.20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst area</th>\n",
       "      <td>569.0</td>\n",
       "      <td>880.583128</td>\n",
       "      <td>569.356993</td>\n",
       "      <td>185.200000</td>\n",
       "      <td>515.300000</td>\n",
       "      <td>686.500000</td>\n",
       "      <td>1084.000000</td>\n",
       "      <td>4254.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst smoothness</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.132369</td>\n",
       "      <td>0.022832</td>\n",
       "      <td>0.071170</td>\n",
       "      <td>0.116600</td>\n",
       "      <td>0.131300</td>\n",
       "      <td>0.146000</td>\n",
       "      <td>0.22260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst compactness</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.254265</td>\n",
       "      <td>0.157336</td>\n",
       "      <td>0.027290</td>\n",
       "      <td>0.147200</td>\n",
       "      <td>0.211900</td>\n",
       "      <td>0.339100</td>\n",
       "      <td>1.05800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst concavity</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.272188</td>\n",
       "      <td>0.208624</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.114500</td>\n",
       "      <td>0.226700</td>\n",
       "      <td>0.382900</td>\n",
       "      <td>1.25200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst concave points</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.114606</td>\n",
       "      <td>0.065732</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.064930</td>\n",
       "      <td>0.099930</td>\n",
       "      <td>0.161400</td>\n",
       "      <td>0.29100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst symmetry</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.290076</td>\n",
       "      <td>0.061867</td>\n",
       "      <td>0.156500</td>\n",
       "      <td>0.250400</td>\n",
       "      <td>0.282200</td>\n",
       "      <td>0.317900</td>\n",
       "      <td>0.66380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.083946</td>\n",
       "      <td>0.018061</td>\n",
       "      <td>0.055040</td>\n",
       "      <td>0.071460</td>\n",
       "      <td>0.080040</td>\n",
       "      <td>0.092080</td>\n",
       "      <td>0.20750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>benign_0__mal_1</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.627417</td>\n",
       "      <td>0.483918</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         count        mean         std         min  \\\n",
       "mean radius              569.0   14.127292    3.524049    6.981000   \n",
       "mean texture             569.0   19.289649    4.301036    9.710000   \n",
       "mean perimeter           569.0   91.969033   24.298981   43.790000   \n",
       "mean area                569.0  654.889104  351.914129  143.500000   \n",
       "mean smoothness          569.0    0.096360    0.014064    0.052630   \n",
       "mean compactness         569.0    0.104341    0.052813    0.019380   \n",
       "mean concavity           569.0    0.088799    0.079720    0.000000   \n",
       "mean concave points      569.0    0.048919    0.038803    0.000000   \n",
       "mean symmetry            569.0    0.181162    0.027414    0.106000   \n",
       "mean fractal dimension   569.0    0.062798    0.007060    0.049960   \n",
       "radius error             569.0    0.405172    0.277313    0.111500   \n",
       "texture error            569.0    1.216853    0.551648    0.360200   \n",
       "perimeter error          569.0    2.866059    2.021855    0.757000   \n",
       "area error               569.0   40.337079   45.491006    6.802000   \n",
       "smoothness error         569.0    0.007041    0.003003    0.001713   \n",
       "compactness error        569.0    0.025478    0.017908    0.002252   \n",
       "concavity error          569.0    0.031894    0.030186    0.000000   \n",
       "concave points error     569.0    0.011796    0.006170    0.000000   \n",
       "symmetry error           569.0    0.020542    0.008266    0.007882   \n",
       "fractal dimension error  569.0    0.003795    0.002646    0.000895   \n",
       "worst radius             569.0   16.269190    4.833242    7.930000   \n",
       "worst texture            569.0   25.677223    6.146258   12.020000   \n",
       "worst perimeter          569.0  107.261213   33.602542   50.410000   \n",
       "worst area               569.0  880.583128  569.356993  185.200000   \n",
       "worst smoothness         569.0    0.132369    0.022832    0.071170   \n",
       "worst compactness        569.0    0.254265    0.157336    0.027290   \n",
       "worst concavity          569.0    0.272188    0.208624    0.000000   \n",
       "worst concave points     569.0    0.114606    0.065732    0.000000   \n",
       "worst symmetry           569.0    0.290076    0.061867    0.156500   \n",
       "worst fractal dimension  569.0    0.083946    0.018061    0.055040   \n",
       "benign_0__mal_1          569.0    0.627417    0.483918    0.000000   \n",
       "\n",
       "                                25%         50%          75%         max  \n",
       "mean radius               11.700000   13.370000    15.780000    28.11000  \n",
       "mean texture              16.170000   18.840000    21.800000    39.28000  \n",
       "mean perimeter            75.170000   86.240000   104.100000   188.50000  \n",
       "mean area                420.300000  551.100000   782.700000  2501.00000  \n",
       "mean smoothness            0.086370    0.095870     0.105300     0.16340  \n",
       "mean compactness           0.064920    0.092630     0.130400     0.34540  \n",
       "mean concavity             0.029560    0.061540     0.130700     0.42680  \n",
       "mean concave points        0.020310    0.033500     0.074000     0.20120  \n",
       "mean symmetry              0.161900    0.179200     0.195700     0.30400  \n",
       "mean fractal dimension     0.057700    0.061540     0.066120     0.09744  \n",
       "radius error               0.232400    0.324200     0.478900     2.87300  \n",
       "texture error              0.833900    1.108000     1.474000     4.88500  \n",
       "perimeter error            1.606000    2.287000     3.357000    21.98000  \n",
       "area error                17.850000   24.530000    45.190000   542.20000  \n",
       "smoothness error           0.005169    0.006380     0.008146     0.03113  \n",
       "compactness error          0.013080    0.020450     0.032450     0.13540  \n",
       "concavity error            0.015090    0.025890     0.042050     0.39600  \n",
       "concave points error       0.007638    0.010930     0.014710     0.05279  \n",
       "symmetry error             0.015160    0.018730     0.023480     0.07895  \n",
       "fractal dimension error    0.002248    0.003187     0.004558     0.02984  \n",
       "worst radius              13.010000   14.970000    18.790000    36.04000  \n",
       "worst texture             21.080000   25.410000    29.720000    49.54000  \n",
       "worst perimeter           84.110000   97.660000   125.400000   251.20000  \n",
       "worst area               515.300000  686.500000  1084.000000  4254.00000  \n",
       "worst smoothness           0.116600    0.131300     0.146000     0.22260  \n",
       "worst compactness          0.147200    0.211900     0.339100     1.05800  \n",
       "worst concavity            0.114500    0.226700     0.382900     1.25200  \n",
       "worst concave points       0.064930    0.099930     0.161400     0.29100  \n",
       "worst symmetry             0.250400    0.282200     0.317900     0.66380  \n",
       "worst fractal dimension    0.071460    0.080040     0.092080     0.20750  \n",
       "benign_0__mal_1            0.000000    1.000000     1.000000     1.00000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set_style(\"white\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='benign_0__mal_1', ylabel='count'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x='benign_0__mal_1',data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(df.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "worst concave points      -0.793566\n",
       "worst perimeter           -0.782914\n",
       "mean concave points       -0.776614\n",
       "worst radius              -0.776454\n",
       "mean perimeter            -0.742636\n",
       "worst area                -0.733825\n",
       "mean radius               -0.730029\n",
       "mean area                 -0.708984\n",
       "mean concavity            -0.696360\n",
       "worst concavity           -0.659610\n",
       "mean compactness          -0.596534\n",
       "worst compactness         -0.590998\n",
       "radius error              -0.567134\n",
       "perimeter error           -0.556141\n",
       "area error                -0.548236\n",
       "worst texture             -0.456903\n",
       "worst smoothness          -0.421465\n",
       "worst symmetry            -0.416294\n",
       "mean texture              -0.415185\n",
       "concave points error      -0.408042\n",
       "mean smoothness           -0.358560\n",
       "mean symmetry             -0.330499\n",
       "worst fractal dimension   -0.323872\n",
       "compactness error         -0.292999\n",
       "concavity error           -0.253730\n",
       "fractal dimension error   -0.077972\n",
       "symmetry error             0.006522\n",
       "texture error              0.008303\n",
       "mean fractal dimension     0.012838\n",
       "smoothness error           0.067016\n",
       "benign_0__mal_1            1.000000\n",
       "Name: benign_0__mal_1, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.corr()['benign_0__mal_1'].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.corr()['benign_0__mal_1'].sort_values().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.corr()['benign_0__mal_1'][:-1].sort_values().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('benign_0__mal_1',axis=1).values\n",
    "y = df['benign_0__mal_1'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25,random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Scaling Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MinMaxScaler()"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model\n",
    "\n",
    "    # For a binary classification problem\n",
    "    model.compile(optimizer='rmsprop',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "                  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation,Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(426, 30)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw\n",
    "\n",
    "model.add(Dense(units=30,activation='relu'))\n",
    "\n",
    "model.add(Dense(units=15,activation='relu'))\n",
    "\n",
    "\n",
    "model.add(Dense(units=1,activation='sigmoid'))\n",
    "\n",
    "# For a binary classification problem\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model \n",
    "\n",
    "### Example One: Choosing too many epochs and overfitting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/600\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.6283 - val_loss: 0.5967\n",
      "Epoch 2/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5742 - val_loss: 0.5458\n",
      "Epoch 3/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5243 - val_loss: 0.4947\n",
      "Epoch 4/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4764 - val_loss: 0.4490\n",
      "Epoch 5/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4328 - val_loss: 0.4053\n",
      "Epoch 6/600\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3927 - val_loss: 0.3653\n",
      "Epoch 7/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3565 - val_loss: 0.3291\n",
      "Epoch 8/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3242 - val_loss: 0.2966\n",
      "Epoch 9/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2982 - val_loss: 0.2698\n",
      "Epoch 10/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2739 - val_loss: 0.2482\n",
      "Epoch 11/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2535 - val_loss: 0.2288\n",
      "Epoch 12/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2370 - val_loss: 0.2149\n",
      "Epoch 13/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2243 - val_loss: 0.1983\n",
      "Epoch 14/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2109 - val_loss: 0.1891\n",
      "Epoch 15/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1976 - val_loss: 0.1778\n",
      "Epoch 16/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1870 - val_loss: 0.1693\n",
      "Epoch 17/600\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.1785 - val_loss: 0.1608\n",
      "Epoch 18/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1727 - val_loss: 0.1561\n",
      "Epoch 19/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1646 - val_loss: 0.1490\n",
      "Epoch 20/600\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.1584 - val_loss: 0.1446\n",
      "Epoch 21/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1471 - val_loss: 0.1500\n",
      "Epoch 22/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1427 - val_loss: 0.1361\n",
      "Epoch 23/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1358 - val_loss: 0.1335\n",
      "Epoch 24/600\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.1307 - val_loss: 0.1328\n",
      "Epoch 25/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1253 - val_loss: 0.1281\n",
      "Epoch 26/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1210 - val_loss: 0.1270\n",
      "Epoch 27/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1159 - val_loss: 0.1234\n",
      "Epoch 28/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1141 - val_loss: 0.1195\n",
      "Epoch 29/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1139 - val_loss: 0.1216\n",
      "Epoch 30/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1074 - val_loss: 0.1212\n",
      "Epoch 31/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1003 - val_loss: 0.1145\n",
      "Epoch 32/600\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.1004 - val_loss: 0.1170\n",
      "Epoch 33/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0974 - val_loss: 0.1181\n",
      "Epoch 34/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0927 - val_loss: 0.1134\n",
      "Epoch 35/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0905 - val_loss: 0.1148\n",
      "Epoch 36/600\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0892 - val_loss: 0.1126\n",
      "Epoch 37/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0868 - val_loss: 0.1128\n",
      "Epoch 38/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0846 - val_loss: 0.1144\n",
      "Epoch 39/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0826 - val_loss: 0.1125\n",
      "Epoch 40/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0807 - val_loss: 0.1146\n",
      "Epoch 41/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0800 - val_loss: 0.1109\n",
      "Epoch 42/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0789 - val_loss: 0.1128\n",
      "Epoch 43/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0765 - val_loss: 0.1117\n",
      "Epoch 44/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0752 - val_loss: 0.1114\n",
      "Epoch 45/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0738 - val_loss: 0.1104\n",
      "Epoch 46/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0746 - val_loss: 0.1092\n",
      "Epoch 47/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0738 - val_loss: 0.1159\n",
      "Epoch 48/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0725 - val_loss: 0.1083\n",
      "Epoch 49/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0704 - val_loss: 0.1109\n",
      "Epoch 50/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0729 - val_loss: 0.1174\n",
      "Epoch 51/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0722 - val_loss: 0.1059\n",
      "Epoch 52/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0709 - val_loss: 0.1117\n",
      "Epoch 53/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0678 - val_loss: 0.1105\n",
      "Epoch 54/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0661 - val_loss: 0.1142\n",
      "Epoch 55/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0662 - val_loss: 0.1113\n",
      "Epoch 56/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0646 - val_loss: 0.1148\n",
      "Epoch 57/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0634 - val_loss: 0.1090\n",
      "Epoch 58/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0651 - val_loss: 0.1142\n",
      "Epoch 59/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0633 - val_loss: 0.1083\n",
      "Epoch 60/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0625 - val_loss: 0.1075\n",
      "Epoch 61/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0614 - val_loss: 0.1173\n",
      "Epoch 62/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0627 - val_loss: 0.1076\n",
      "Epoch 63/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0610 - val_loss: 0.1099\n",
      "Epoch 64/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0618 - val_loss: 0.1101\n",
      "Epoch 65/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0602 - val_loss: 0.1083\n",
      "Epoch 66/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0601 - val_loss: 0.1142\n",
      "Epoch 67/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0598 - val_loss: 0.1170\n",
      "Epoch 68/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0577 - val_loss: 0.1084\n",
      "Epoch 69/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0601 - val_loss: 0.1138\n",
      "Epoch 70/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0580 - val_loss: 0.1096\n",
      "Epoch 71/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0569 - val_loss: 0.1195\n",
      "Epoch 72/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0616 - val_loss: 0.1091\n",
      "Epoch 73/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0581 - val_loss: 0.1175\n",
      "Epoch 74/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0579 - val_loss: 0.1178\n",
      "Epoch 75/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0575 - val_loss: 0.1131\n",
      "Epoch 76/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0557 - val_loss: 0.1117\n",
      "Epoch 77/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0593 - val_loss: 0.1259\n",
      "Epoch 78/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0596 - val_loss: 0.1135\n",
      "Epoch 79/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0553 - val_loss: 0.1120\n",
      "Epoch 80/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0562 - val_loss: 0.1225\n",
      "Epoch 81/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0617 - val_loss: 0.1131\n",
      "Epoch 82/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0658 - val_loss: 0.1290\n",
      "Epoch 83/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0584 - val_loss: 0.1154\n",
      "Epoch 84/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0584 - val_loss: 0.1185\n",
      "Epoch 85/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0534 - val_loss: 0.1212\n",
      "Epoch 86/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0530 - val_loss: 0.1156\n",
      "Epoch 87/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0535 - val_loss: 0.1176\n",
      "Epoch 88/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0540 - val_loss: 0.1163\n",
      "Epoch 89/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0572 - val_loss: 0.1176\n",
      "Epoch 90/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0532 - val_loss: 0.1149\n",
      "Epoch 91/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0556 - val_loss: 0.1275\n",
      "Epoch 92/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0524 - val_loss: 0.1160\n",
      "Epoch 93/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0530 - val_loss: 0.1200\n",
      "Epoch 94/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0523 - val_loss: 0.1257\n",
      "Epoch 95/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0506 - val_loss: 0.1143\n",
      "Epoch 96/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0531 - val_loss: 0.1190\n",
      "Epoch 97/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0523 - val_loss: 0.1190\n",
      "Epoch 98/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0516 - val_loss: 0.1156\n",
      "Epoch 99/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0522 - val_loss: 0.1255\n",
      "Epoch 100/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0518 - val_loss: 0.1240\n",
      "Epoch 101/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0496 - val_loss: 0.1136\n",
      "Epoch 102/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0553 - val_loss: 0.1224\n",
      "Epoch 103/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0515 - val_loss: 0.1188\n",
      "Epoch 104/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0517 - val_loss: 0.1158\n",
      "Epoch 105/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0502 - val_loss: 0.1183\n",
      "Epoch 106/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0543 - val_loss: 0.1195\n",
      "Epoch 107/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0509 - val_loss: 0.1169\n",
      "Epoch 108/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0499 - val_loss: 0.1223\n",
      "Epoch 109/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0490 - val_loss: 0.1191\n",
      "Epoch 110/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0509 - val_loss: 0.1263\n",
      "Epoch 111/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0517 - val_loss: 0.1173\n",
      "Epoch 112/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0495 - val_loss: 0.1229\n",
      "Epoch 113/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0496 - val_loss: 0.1317\n",
      "Epoch 114/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0480 - val_loss: 0.1189\n",
      "Epoch 115/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0493 - val_loss: 0.1220\n",
      "Epoch 116/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0477 - val_loss: 0.1268\n",
      "Epoch 117/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0479 - val_loss: 0.1231\n",
      "Epoch 118/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0482 - val_loss: 0.1274\n",
      "Epoch 119/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0485 - val_loss: 0.1243\n",
      "Epoch 120/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0521 - val_loss: 0.1299\n",
      "Epoch 121/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0468 - val_loss: 0.1270\n",
      "Epoch 122/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0468 - val_loss: 0.1255\n",
      "Epoch 123/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0470 - val_loss: 0.1299\n",
      "Epoch 124/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0498 - val_loss: 0.1278\n",
      "Epoch 125/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0473 - val_loss: 0.1376\n",
      "Epoch 126/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0455 - val_loss: 0.1199\n",
      "Epoch 127/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0467 - val_loss: 0.1306\n",
      "Epoch 128/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0461 - val_loss: 0.1260\n",
      "Epoch 129/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0468 - val_loss: 0.1250\n",
      "Epoch 130/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0469 - val_loss: 0.1272\n",
      "Epoch 131/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0491 - val_loss: 0.1194\n",
      "Epoch 132/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0535 - val_loss: 0.1403\n",
      "Epoch 133/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0514 - val_loss: 0.1266\n",
      "Epoch 134/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0457 - val_loss: 0.1264\n",
      "Epoch 135/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0461 - val_loss: 0.1294\n",
      "Epoch 136/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0451 - val_loss: 0.1292\n",
      "Epoch 137/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0477 - val_loss: 0.1304\n",
      "Epoch 138/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0451 - val_loss: 0.1310\n",
      "Epoch 139/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0454 - val_loss: 0.1328\n",
      "Epoch 140/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0456 - val_loss: 0.1241\n",
      "Epoch 141/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0482 - val_loss: 0.1428\n",
      "Epoch 142/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0494 - val_loss: 0.1367\n",
      "Epoch 143/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0453 - val_loss: 0.1395\n",
      "Epoch 144/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0434 - val_loss: 0.1320\n",
      "Epoch 145/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0448 - val_loss: 0.1336\n",
      "Epoch 146/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0435 - val_loss: 0.1359\n",
      "Epoch 147/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0432 - val_loss: 0.1302\n",
      "Epoch 148/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0438 - val_loss: 0.1362\n",
      "Epoch 149/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0428 - val_loss: 0.1348\n",
      "Epoch 150/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0429 - val_loss: 0.1363\n",
      "Epoch 151/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0435 - val_loss: 0.1401\n",
      "Epoch 152/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0449 - val_loss: 0.1400\n",
      "Epoch 153/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0478 - val_loss: 0.1319\n",
      "Epoch 154/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0435 - val_loss: 0.1439\n",
      "Epoch 155/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0460 - val_loss: 0.1356\n",
      "Epoch 156/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0502 - val_loss: 0.1282\n",
      "Epoch 157/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0449 - val_loss: 0.1397\n",
      "Epoch 158/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0537 - val_loss: 0.1318\n",
      "Epoch 159/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0449 - val_loss: 0.1412\n",
      "Epoch 160/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0424 - val_loss: 0.1345\n",
      "Epoch 161/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0424 - val_loss: 0.1319\n",
      "Epoch 162/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0412 - val_loss: 0.1373\n",
      "Epoch 163/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0428 - val_loss: 0.1303\n",
      "Epoch 164/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0420 - val_loss: 0.1368\n",
      "Epoch 165/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0412 - val_loss: 0.1308\n",
      "Epoch 166/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0422 - val_loss: 0.1378\n",
      "Epoch 167/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0410 - val_loss: 0.1346\n",
      "Epoch 168/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0421 - val_loss: 0.1448\n",
      "Epoch 169/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0415 - val_loss: 0.1372\n",
      "Epoch 170/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0440 - val_loss: 0.1332\n",
      "Epoch 171/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0401 - val_loss: 0.1437\n",
      "Epoch 172/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0409 - val_loss: 0.1340\n",
      "Epoch 173/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0461 - val_loss: 0.1384\n",
      "Epoch 174/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0407 - val_loss: 0.1451\n",
      "Epoch 175/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0401 - val_loss: 0.1356\n",
      "Epoch 176/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0401 - val_loss: 0.1413\n",
      "Epoch 177/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0396 - val_loss: 0.1318\n",
      "Epoch 178/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0406 - val_loss: 0.1555\n",
      "Epoch 179/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0412 - val_loss: 0.1330\n",
      "Epoch 180/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0386 - val_loss: 0.1397\n",
      "Epoch 181/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0385 - val_loss: 0.1356\n",
      "Epoch 182/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0409 - val_loss: 0.1439\n",
      "Epoch 183/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0391 - val_loss: 0.1373\n",
      "Epoch 184/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0389 - val_loss: 0.1411\n",
      "Epoch 185/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0389 - val_loss: 0.1457\n",
      "Epoch 186/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0396 - val_loss: 0.1377\n",
      "Epoch 187/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0422 - val_loss: 0.1360\n",
      "Epoch 188/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0421 - val_loss: 0.1315\n",
      "Epoch 189/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0414 - val_loss: 0.1306\n",
      "Epoch 190/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0422 - val_loss: 0.1367\n",
      "Epoch 191/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0391 - val_loss: 0.1345\n",
      "Epoch 192/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0370 - val_loss: 0.1508\n",
      "Epoch 193/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0422 - val_loss: 0.1350\n",
      "Epoch 194/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0414 - val_loss: 0.1493\n",
      "Epoch 195/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0383 - val_loss: 0.1476\n",
      "Epoch 196/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0371 - val_loss: 0.1344\n",
      "Epoch 197/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0372 - val_loss: 0.1441\n",
      "Epoch 198/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0395 - val_loss: 0.1370\n",
      "Epoch 199/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0385 - val_loss: 0.1437\n",
      "Epoch 200/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0367 - val_loss: 0.1396\n",
      "Epoch 201/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0368 - val_loss: 0.1349\n",
      "Epoch 202/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0390 - val_loss: 0.1408\n",
      "Epoch 203/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0378 - val_loss: 0.1380\n",
      "Epoch 204/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0367 - val_loss: 0.1461\n",
      "Epoch 205/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0379 - val_loss: 0.1454\n",
      "Epoch 206/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0362 - val_loss: 0.1387\n",
      "Epoch 207/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0366 - val_loss: 0.1435\n",
      "Epoch 208/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0360 - val_loss: 0.1398\n",
      "Epoch 209/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0380 - val_loss: 0.1329\n",
      "Epoch 210/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0362 - val_loss: 0.1470\n",
      "Epoch 211/600\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0367 - val_loss: 0.1451\n",
      "Epoch 212/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0368 - val_loss: 0.1491\n",
      "Epoch 213/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0349 - val_loss: 0.1390\n",
      "Epoch 214/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0361 - val_loss: 0.1437\n",
      "Epoch 215/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0392 - val_loss: 0.1443\n",
      "Epoch 216/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0348 - val_loss: 0.1467\n",
      "Epoch 217/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0348 - val_loss: 0.1400\n",
      "Epoch 218/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0341 - val_loss: 0.1392\n",
      "Epoch 219/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0360 - val_loss: 0.1548\n",
      "Epoch 220/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0343 - val_loss: 0.1326\n",
      "Epoch 221/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0365 - val_loss: 0.1546\n",
      "Epoch 222/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0336 - val_loss: 0.1385\n",
      "Epoch 223/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0338 - val_loss: 0.1444\n",
      "Epoch 224/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0350 - val_loss: 0.1386\n",
      "Epoch 225/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0349 - val_loss: 0.1565\n",
      "Epoch 226/600\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0341 - val_loss: 0.1460\n",
      "Epoch 227/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0337 - val_loss: 0.1361\n",
      "Epoch 228/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0348 - val_loss: 0.1459\n",
      "Epoch 229/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0346 - val_loss: 0.1436\n",
      "Epoch 230/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0340 - val_loss: 0.1432\n",
      "Epoch 231/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0387 - val_loss: 0.1351\n",
      "Epoch 232/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0357 - val_loss: 0.1437\n",
      "Epoch 233/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0337 - val_loss: 0.1547\n",
      "Epoch 234/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0352 - val_loss: 0.1369\n",
      "Epoch 235/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0392 - val_loss: 0.1429\n",
      "Epoch 236/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0378 - val_loss: 0.1403\n",
      "Epoch 237/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0368 - val_loss: 0.1607\n",
      "Epoch 238/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0387 - val_loss: 0.1421\n",
      "Epoch 239/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0349 - val_loss: 0.1393\n",
      "Epoch 240/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0327 - val_loss: 0.1452\n",
      "Epoch 241/600\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0327 - val_loss: 0.1496\n",
      "Epoch 242/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0330 - val_loss: 0.1560\n",
      "Epoch 243/600\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0334 - val_loss: 0.1397\n",
      "Epoch 244/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0305 - val_loss: 0.1453\n",
      "Epoch 245/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0309 - val_loss: 0.1492\n",
      "Epoch 246/600\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0302 - val_loss: 0.1496\n",
      "Epoch 247/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0320 - val_loss: 0.1301\n",
      "Epoch 248/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0332 - val_loss: 0.1674\n",
      "Epoch 249/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0343 - val_loss: 0.1373\n",
      "Epoch 250/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0315 - val_loss: 0.1514\n",
      "Epoch 251/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0313 - val_loss: 0.1351\n",
      "Epoch 252/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0304 - val_loss: 0.1552\n",
      "Epoch 253/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0313 - val_loss: 0.1455\n",
      "Epoch 254/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0289 - val_loss: 0.1593\n",
      "Epoch 255/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0303 - val_loss: 0.1389\n",
      "Epoch 256/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0310 - val_loss: 0.1386\n",
      "Epoch 257/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0303 - val_loss: 0.1427\n",
      "Epoch 258/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0374 - val_loss: 0.1331\n",
      "Epoch 259/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0351 - val_loss: 0.1459\n",
      "Epoch 260/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0299 - val_loss: 0.1466\n",
      "Epoch 261/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0297 - val_loss: 0.1418\n",
      "Epoch 262/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.1573\n",
      "Epoch 263/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0281 - val_loss: 0.1416\n",
      "Epoch 264/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0288 - val_loss: 0.1539\n",
      "Epoch 265/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.1407\n",
      "Epoch 266/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0294 - val_loss: 0.1582\n",
      "Epoch 267/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0286 - val_loss: 0.1477\n",
      "Epoch 268/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.1453\n",
      "Epoch 269/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0287 - val_loss: 0.1370\n",
      "Epoch 270/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0297 - val_loss: 0.1592\n",
      "Epoch 271/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0304 - val_loss: 0.1406\n",
      "Epoch 272/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0330 - val_loss: 0.1502\n",
      "Epoch 273/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0284 - val_loss: 0.1548\n",
      "Epoch 274/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0271 - val_loss: 0.1415\n",
      "Epoch 275/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0272 - val_loss: 0.1532\n",
      "Epoch 276/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0270 - val_loss: 0.1420\n",
      "Epoch 277/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0284 - val_loss: 0.1455\n",
      "Epoch 278/600\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0310 - val_loss: 0.1459\n",
      "Epoch 279/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0264 - val_loss: 0.1507\n",
      "Epoch 280/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0263 - val_loss: 0.1489\n",
      "Epoch 281/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0257 - val_loss: 0.1444\n",
      "Epoch 282/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0268 - val_loss: 0.1581\n",
      "Epoch 283/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0271 - val_loss: 0.1479\n",
      "Epoch 284/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0257 - val_loss: 0.1466\n",
      "Epoch 285/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0274 - val_loss: 0.1487\n",
      "Epoch 286/600\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.1464\n",
      "Epoch 287/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0293 - val_loss: 0.1468\n",
      "Epoch 288/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0263 - val_loss: 0.1449\n",
      "Epoch 289/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0255 - val_loss: 0.1484\n",
      "Epoch 290/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0256 - val_loss: 0.1494\n",
      "Epoch 291/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0312 - val_loss: 0.1431\n",
      "Epoch 292/600\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0315 - val_loss: 0.1683\n",
      "Epoch 293/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0271 - val_loss: 0.1486\n",
      "Epoch 294/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0249 - val_loss: 0.1561\n",
      "Epoch 295/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0253 - val_loss: 0.1589\n",
      "Epoch 296/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0239 - val_loss: 0.1435\n",
      "Epoch 297/600\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0258 - val_loss: 0.1533\n",
      "Epoch 298/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0257 - val_loss: 0.1566\n",
      "Epoch 299/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0237 - val_loss: 0.1513\n",
      "Epoch 300/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0237 - val_loss: 0.1590\n",
      "Epoch 301/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0263 - val_loss: 0.1549\n",
      "Epoch 302/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0313 - val_loss: 0.1492\n",
      "Epoch 303/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0272 - val_loss: 0.1499\n",
      "Epoch 304/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0288 - val_loss: 0.1507\n",
      "Epoch 305/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0276 - val_loss: 0.1527\n",
      "Epoch 306/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0235 - val_loss: 0.1538\n",
      "Epoch 307/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0244 - val_loss: 0.1450\n",
      "Epoch 308/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0235 - val_loss: 0.1635\n",
      "Epoch 309/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0293 - val_loss: 0.1456\n",
      "Epoch 310/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0272 - val_loss: 0.1670\n",
      "Epoch 311/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0219 - val_loss: 0.1401\n",
      "Epoch 312/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0368 - val_loss: 0.1793\n",
      "Epoch 313/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0243 - val_loss: 0.1488\n",
      "Epoch 314/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0246 - val_loss: 0.1695\n",
      "Epoch 315/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0268 - val_loss: 0.1412\n",
      "Epoch 316/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0292 - val_loss: 0.1629\n",
      "Epoch 317/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0225 - val_loss: 0.1486\n",
      "Epoch 318/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0218 - val_loss: 0.1656\n",
      "Epoch 319/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0230 - val_loss: 0.1510\n",
      "Epoch 320/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0225 - val_loss: 0.1561\n",
      "Epoch 321/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0219 - val_loss: 0.1565\n",
      "Epoch 322/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0215 - val_loss: 0.1562\n",
      "Epoch 323/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0227 - val_loss: 0.1518\n",
      "Epoch 324/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0240 - val_loss: 0.1618\n",
      "Epoch 325/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0239 - val_loss: 0.1689\n",
      "Epoch 326/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0264 - val_loss: 0.1503\n",
      "Epoch 327/600\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0214 - val_loss: 0.1576\n",
      "Epoch 328/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0211 - val_loss: 0.1614\n",
      "Epoch 329/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0217 - val_loss: 0.1536\n",
      "Epoch 330/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0210 - val_loss: 0.1627\n",
      "Epoch 331/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0214 - val_loss: 0.1491\n",
      "Epoch 332/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0225 - val_loss: 0.1746\n",
      "Epoch 333/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0272 - val_loss: 0.1475\n",
      "Epoch 334/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0231 - val_loss: 0.1824\n",
      "Epoch 335/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0263 - val_loss: 0.1507\n",
      "Epoch 336/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0226 - val_loss: 0.1549\n",
      "Epoch 337/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0205 - val_loss: 0.1544\n",
      "Epoch 338/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0208 - val_loss: 0.1601\n",
      "Epoch 339/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0208 - val_loss: 0.1651\n",
      "Epoch 340/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0205 - val_loss: 0.1546\n",
      "Epoch 341/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0228 - val_loss: 0.1737\n",
      "Epoch 342/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0242 - val_loss: 0.1490\n",
      "Epoch 343/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0252 - val_loss: 0.1639\n",
      "Epoch 344/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0251 - val_loss: 0.1680\n",
      "Epoch 345/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0233 - val_loss: 0.1506\n",
      "Epoch 346/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0213 - val_loss: 0.1619\n",
      "Epoch 347/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0205 - val_loss: 0.1644\n",
      "Epoch 348/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0258 - val_loss: 0.1519\n",
      "Epoch 349/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0227 - val_loss: 0.1793\n",
      "Epoch 350/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0209 - val_loss: 0.1612\n",
      "Epoch 351/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0195 - val_loss: 0.1711\n",
      "Epoch 352/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0189 - val_loss: 0.1606\n",
      "Epoch 353/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0214 - val_loss: 0.1604\n",
      "Epoch 354/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0201 - val_loss: 0.1602\n",
      "Epoch 355/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0190 - val_loss: 0.1643\n",
      "Epoch 356/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0192 - val_loss: 0.1616\n",
      "Epoch 357/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0190 - val_loss: 0.1658\n",
      "Epoch 358/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0185 - val_loss: 0.1588\n",
      "Epoch 359/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0191 - val_loss: 0.1670\n",
      "Epoch 360/600\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0185 - val_loss: 0.1609\n",
      "Epoch 361/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0179 - val_loss: 0.1757\n",
      "Epoch 362/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0187 - val_loss: 0.1795\n",
      "Epoch 363/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0233 - val_loss: 0.1873\n",
      "Epoch 364/600\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.0211 - val_loss: 0.1693\n",
      "Epoch 365/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0217 - val_loss: 0.1798\n",
      "Epoch 366/600\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0188 - val_loss: 0.1667\n",
      "Epoch 367/600\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0179 - val_loss: 0.1703\n",
      "Epoch 368/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0183 - val_loss: 0.1620\n",
      "Epoch 369/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0173 - val_loss: 0.1727\n",
      "Epoch 370/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0217 - val_loss: 0.1773\n",
      "Epoch 371/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0223 - val_loss: 0.1627\n",
      "Epoch 372/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0206 - val_loss: 0.2059\n",
      "Epoch 373/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0212 - val_loss: 0.1561\n",
      "Epoch 374/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0192 - val_loss: 0.1750\n",
      "Epoch 375/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0177 - val_loss: 0.1700\n",
      "Epoch 376/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0190 - val_loss: 0.1788\n",
      "Epoch 377/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0175 - val_loss: 0.1646\n",
      "Epoch 378/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0206 - val_loss: 0.2023\n",
      "Epoch 379/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0175 - val_loss: 0.1567\n",
      "Epoch 380/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0211 - val_loss: 0.1814\n",
      "Epoch 381/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0214 - val_loss: 0.1590\n",
      "Epoch 382/600\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0179 - val_loss: 0.2018\n",
      "Epoch 383/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0192 - val_loss: 0.1676\n",
      "Epoch 384/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0171 - val_loss: 0.1715\n",
      "Epoch 385/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0181 - val_loss: 0.1745\n",
      "Epoch 386/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0168 - val_loss: 0.1656\n",
      "Epoch 387/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0185 - val_loss: 0.1708\n",
      "Epoch 388/600\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0168 - val_loss: 0.1792\n",
      "Epoch 389/600\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0165 - val_loss: 0.1702\n",
      "Epoch 390/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0165 - val_loss: 0.1676\n",
      "Epoch 391/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0175 - val_loss: 0.1845\n",
      "Epoch 392/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0180 - val_loss: 0.1740\n",
      "Epoch 393/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0161 - val_loss: 0.1792\n",
      "Epoch 394/600\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0173 - val_loss: 0.1595\n",
      "Epoch 395/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0186 - val_loss: 0.1886\n",
      "Epoch 396/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0215 - val_loss: 0.1700\n",
      "Epoch 397/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0160 - val_loss: 0.1768\n",
      "Epoch 398/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0163 - val_loss: 0.1758\n",
      "Epoch 399/600\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0161 - val_loss: 0.1862\n",
      "Epoch 400/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0156 - val_loss: 0.1808\n",
      "Epoch 401/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0163 - val_loss: 0.1837\n",
      "Epoch 402/600\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0159 - val_loss: 0.1783\n",
      "Epoch 403/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0155 - val_loss: 0.1825\n",
      "Epoch 404/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0193 - val_loss: 0.1702\n",
      "Epoch 405/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0169 - val_loss: 0.2032\n",
      "Epoch 406/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0175 - val_loss: 0.1823\n",
      "Epoch 407/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0143 - val_loss: 0.1892\n",
      "Epoch 408/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0149 - val_loss: 0.1743\n",
      "Epoch 409/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0141 - val_loss: 0.1998\n",
      "Epoch 410/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0158 - val_loss: 0.1776\n",
      "Epoch 411/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0153 - val_loss: 0.1782\n",
      "Epoch 412/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0151 - val_loss: 0.1902\n",
      "Epoch 413/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0167 - val_loss: 0.1782\n",
      "Epoch 414/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0160 - val_loss: 0.1847\n",
      "Epoch 415/600\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0149 - val_loss: 0.1926\n",
      "Epoch 416/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0160 - val_loss: 0.2002\n",
      "Epoch 417/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0148 - val_loss: 0.1873\n",
      "Epoch 418/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0164 - val_loss: 0.1932\n",
      "Epoch 419/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0145 - val_loss: 0.1870\n",
      "Epoch 420/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0153 - val_loss: 0.1781\n",
      "Epoch 421/600\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0163 - val_loss: 0.1855\n",
      "Epoch 422/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0138 - val_loss: 0.2074\n",
      "Epoch 423/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0169 - val_loss: 0.2015\n",
      "Epoch 424/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0157 - val_loss: 0.1843\n",
      "Epoch 425/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0145 - val_loss: 0.1830\n",
      "Epoch 426/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0136 - val_loss: 0.2087\n",
      "Epoch 427/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0132 - val_loss: 0.1849\n",
      "Epoch 428/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0143 - val_loss: 0.1908\n",
      "Epoch 429/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0135 - val_loss: 0.1977\n",
      "Epoch 430/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0130 - val_loss: 0.1982\n",
      "Epoch 431/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0135 - val_loss: 0.1826\n",
      "Epoch 432/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0142 - val_loss: 0.1892\n",
      "Epoch 433/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0137 - val_loss: 0.1898\n",
      "Epoch 434/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0137 - val_loss: 0.1962\n",
      "Epoch 435/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0134 - val_loss: 0.1906\n",
      "Epoch 436/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0132 - val_loss: 0.1965\n",
      "Epoch 437/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0129 - val_loss: 0.1860\n",
      "Epoch 438/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0133 - val_loss: 0.1974\n",
      "Epoch 439/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0126 - val_loss: 0.1972\n",
      "Epoch 440/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0125 - val_loss: 0.2031\n",
      "Epoch 441/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0148 - val_loss: 0.1863\n",
      "Epoch 442/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0146 - val_loss: 0.2098\n",
      "Epoch 443/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0129 - val_loss: 0.2005\n",
      "Epoch 444/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0129 - val_loss: 0.1929\n",
      "Epoch 445/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0132 - val_loss: 0.2262\n",
      "Epoch 446/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0128 - val_loss: 0.1921\n",
      "Epoch 447/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0125 - val_loss: 0.2065\n",
      "Epoch 448/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0122 - val_loss: 0.1909\n",
      "Epoch 449/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0124 - val_loss: 0.2165\n",
      "Epoch 450/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0136 - val_loss: 0.1989\n",
      "Epoch 451/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0121 - val_loss: 0.2071\n",
      "Epoch 452/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0132 - val_loss: 0.2105\n",
      "Epoch 453/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0117 - val_loss: 0.2073\n",
      "Epoch 454/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0137 - val_loss: 0.2034\n",
      "Epoch 455/600\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0153 - val_loss: 0.2048\n",
      "Epoch 456/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0138 - val_loss: 0.2047\n",
      "Epoch 457/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0140 - val_loss: 0.2093\n",
      "Epoch 458/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0115 - val_loss: 0.1979\n",
      "Epoch 459/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0117 - val_loss: 0.2118\n",
      "Epoch 460/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0135 - val_loss: 0.2177\n",
      "Epoch 461/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0131 - val_loss: 0.1918\n",
      "Epoch 462/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0128 - val_loss: 0.2186\n",
      "Epoch 463/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0129 - val_loss: 0.2047\n",
      "Epoch 464/600\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0113 - val_loss: 0.2171\n",
      "Epoch 465/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0121 - val_loss: 0.2045\n",
      "Epoch 466/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0112 - val_loss: 0.2115\n",
      "Epoch 467/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0112 - val_loss: 0.2141\n",
      "Epoch 468/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0105 - val_loss: 0.2122\n",
      "Epoch 469/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0115 - val_loss: 0.2063\n",
      "Epoch 470/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0119 - val_loss: 0.2287\n",
      "Epoch 471/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0117 - val_loss: 0.2122\n",
      "Epoch 472/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0114 - val_loss: 0.2148\n",
      "Epoch 473/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0108 - val_loss: 0.2177\n",
      "Epoch 474/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0153 - val_loss: 0.2658\n",
      "Epoch 475/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0166 - val_loss: 0.2084\n",
      "Epoch 476/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0116 - val_loss: 0.2284\n",
      "Epoch 477/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0133 - val_loss: 0.2274\n",
      "Epoch 478/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0141 - val_loss: 0.2284\n",
      "Epoch 479/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0194 - val_loss: 0.2439\n",
      "Epoch 480/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0157 - val_loss: 0.2179\n",
      "Epoch 481/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0110 - val_loss: 0.2190\n",
      "Epoch 482/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0104 - val_loss: 0.2232\n",
      "Epoch 483/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0099 - val_loss: 0.2283\n",
      "Epoch 484/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0108 - val_loss: 0.2281\n",
      "Epoch 485/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0111 - val_loss: 0.2504\n",
      "Epoch 486/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0207 - val_loss: 0.2286\n",
      "Epoch 487/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0151 - val_loss: 0.2594\n",
      "Epoch 488/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0107 - val_loss: 0.2292\n",
      "Epoch 489/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0116 - val_loss: 0.2297\n",
      "Epoch 490/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0105 - val_loss: 0.2482\n",
      "Epoch 491/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0111 - val_loss: 0.2322\n",
      "Epoch 492/600\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0106 - val_loss: 0.2367\n",
      "Epoch 493/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0106 - val_loss: 0.2401\n",
      "Epoch 494/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0126 - val_loss: 0.2461\n",
      "Epoch 495/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0109 - val_loss: 0.2341\n",
      "Epoch 496/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0143 - val_loss: 0.2452\n",
      "Epoch 497/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0093 - val_loss: 0.2435\n",
      "Epoch 498/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0100 - val_loss: 0.2321\n",
      "Epoch 499/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0105 - val_loss: 0.2514\n",
      "Epoch 500/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0106 - val_loss: 0.2438\n",
      "Epoch 501/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0088 - val_loss: 0.2465\n",
      "Epoch 502/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0092 - val_loss: 0.2442\n",
      "Epoch 503/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0099 - val_loss: 0.2565\n",
      "Epoch 504/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0107 - val_loss: 0.2402\n",
      "Epoch 505/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0100 - val_loss: 0.2589\n",
      "Epoch 506/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0112 - val_loss: 0.2557\n",
      "Epoch 507/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0117 - val_loss: 0.2455\n",
      "Epoch 508/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0084 - val_loss: 0.2502\n",
      "Epoch 509/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0098 - val_loss: 0.2502\n",
      "Epoch 510/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0093 - val_loss: 0.2545\n",
      "Epoch 511/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0082 - val_loss: 0.2397\n",
      "Epoch 512/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0090 - val_loss: 0.2589\n",
      "Epoch 513/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0100 - val_loss: 0.2531\n",
      "Epoch 514/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0088 - val_loss: 0.2688\n",
      "Epoch 515/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0097 - val_loss: 0.2521\n",
      "Epoch 516/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0089 - val_loss: 0.2589\n",
      "Epoch 517/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0098 - val_loss: 0.2631\n",
      "Epoch 518/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0093 - val_loss: 0.2539\n",
      "Epoch 519/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0092 - val_loss: 0.2658\n",
      "Epoch 520/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0089 - val_loss: 0.2623\n",
      "Epoch 521/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0118 - val_loss: 0.2912\n",
      "Epoch 522/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0106 - val_loss: 0.2608\n",
      "Epoch 523/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0087 - val_loss: 0.2569\n",
      "Epoch 524/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0079 - val_loss: 0.2608\n",
      "Epoch 525/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0083 - val_loss: 0.2707\n",
      "Epoch 526/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0087 - val_loss: 0.2636\n",
      "Epoch 527/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0077 - val_loss: 0.2695\n",
      "Epoch 528/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0085 - val_loss: 0.2623\n",
      "Epoch 529/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0085 - val_loss: 0.2876\n",
      "Epoch 530/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0083 - val_loss: 0.2734\n",
      "Epoch 531/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0076 - val_loss: 0.2718\n",
      "Epoch 532/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0079 - val_loss: 0.2737\n",
      "Epoch 533/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0137 - val_loss: 0.2894\n",
      "Epoch 534/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0099 - val_loss: 0.2663\n",
      "Epoch 535/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0084 - val_loss: 0.2937\n",
      "Epoch 536/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0084 - val_loss: 0.2782\n",
      "Epoch 537/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0082 - val_loss: 0.2809\n",
      "Epoch 538/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0123 - val_loss: 0.2708\n",
      "Epoch 539/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0082 - val_loss: 0.2898\n",
      "Epoch 540/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0077 - val_loss: 0.2904\n",
      "Epoch 541/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0074 - val_loss: 0.2811\n",
      "Epoch 542/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0072 - val_loss: 0.2867\n",
      "Epoch 543/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0074 - val_loss: 0.2885\n",
      "Epoch 544/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0071 - val_loss: 0.2784\n",
      "Epoch 545/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0079 - val_loss: 0.2896\n",
      "Epoch 546/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0085 - val_loss: 0.2763\n",
      "Epoch 547/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0089 - val_loss: 0.3034\n",
      "Epoch 548/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0077 - val_loss: 0.3000\n",
      "Epoch 549/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0073 - val_loss: 0.2951\n",
      "Epoch 550/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0069 - val_loss: 0.2863\n",
      "Epoch 551/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0093 - val_loss: 0.2904\n",
      "Epoch 552/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0074 - val_loss: 0.2944\n",
      "Epoch 553/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0078 - val_loss: 0.2983\n",
      "Epoch 554/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0070 - val_loss: 0.2971\n",
      "Epoch 555/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0068 - val_loss: 0.2948\n",
      "Epoch 556/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0074 - val_loss: 0.2947\n",
      "Epoch 557/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0072 - val_loss: 0.3016\n",
      "Epoch 558/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0066 - val_loss: 0.2996\n",
      "Epoch 559/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0067 - val_loss: 0.3039\n",
      "Epoch 560/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0063 - val_loss: 0.2989\n",
      "Epoch 561/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0067 - val_loss: 0.3047\n",
      "Epoch 562/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0063 - val_loss: 0.3118\n",
      "Epoch 563/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0074 - val_loss: 0.3066\n",
      "Epoch 564/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0067 - val_loss: 0.3056\n",
      "Epoch 565/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0064 - val_loss: 0.3094\n",
      "Epoch 566/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0067 - val_loss: 0.3223\n",
      "Epoch 567/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0073 - val_loss: 0.3145\n",
      "Epoch 568/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0069 - val_loss: 0.3180\n",
      "Epoch 569/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0072 - val_loss: 0.3083\n",
      "Epoch 570/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0079 - val_loss: 0.3131\n",
      "Epoch 571/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0066 - val_loss: 0.3258\n",
      "Epoch 572/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0075 - val_loss: 0.3067\n",
      "Epoch 573/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0119 - val_loss: 0.3249\n",
      "Epoch 574/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0081 - val_loss: 0.3329\n",
      "Epoch 575/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0076 - val_loss: 0.3090\n",
      "Epoch 576/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0063 - val_loss: 0.3146\n",
      "Epoch 577/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0075 - val_loss: 0.3119\n",
      "Epoch 578/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0066 - val_loss: 0.3382\n",
      "Epoch 579/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0061 - val_loss: 0.3333\n",
      "Epoch 580/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0060 - val_loss: 0.3183\n",
      "Epoch 581/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0058 - val_loss: 0.3225\n",
      "Epoch 582/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0057 - val_loss: 0.3283\n",
      "Epoch 583/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0065 - val_loss: 0.3242\n",
      "Epoch 584/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0065 - val_loss: 0.3189\n",
      "Epoch 585/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0079 - val_loss: 0.3216\n",
      "Epoch 586/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0071 - val_loss: 0.3515\n",
      "Epoch 587/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0057 - val_loss: 0.3300\n",
      "Epoch 588/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0063 - val_loss: 0.3440\n",
      "Epoch 589/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0073 - val_loss: 0.3323\n",
      "Epoch 590/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0054 - val_loss: 0.3441\n",
      "Epoch 591/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0055 - val_loss: 0.3398\n",
      "Epoch 592/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.3326\n",
      "Epoch 593/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0064 - val_loss: 0.3371\n",
      "Epoch 594/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0057 - val_loss: 0.3370\n",
      "Epoch 595/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0057 - val_loss: 0.3406\n",
      "Epoch 596/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0055 - val_loss: 0.3542\n",
      "Epoch 597/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0055 - val_loss: 0.3557\n",
      "Epoch 598/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0066 - val_loss: 0.3406\n",
      "Epoch 599/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0081 - val_loss: 0.3737\n",
      "Epoch 600/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0063 - val_loss: 0.3458\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1881d9f2cd0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://stats.stackexchange.com/questions/164876/tradeoff-batch-size-vs-number-of-iterations-to-train-a-neural-network\n",
    "# https://datascience.stackexchange.com/questions/18414/are-there-any-rules-for-choosing-the-size-of-a-mini-batch\n",
    "\n",
    "model.fit(x=X_train, \n",
    "          y=y_train, \n",
    "          epochs=600,\n",
    "          validation_data=(X_test, y_test), verbose=1\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.6283250451087952,\n",
       "  0.5741875171661377,\n",
       "  0.524299144744873,\n",
       "  0.4763656258583069,\n",
       "  0.4327995181083679,\n",
       "  0.3927060663700104,\n",
       "  0.35651713609695435,\n",
       "  0.3242259919643402,\n",
       "  0.29819855093955994,\n",
       "  0.273853063583374,\n",
       "  0.25345152616500854,\n",
       "  0.2369775027036667,\n",
       "  0.22426703572273254,\n",
       "  0.21085847914218903,\n",
       "  0.19755172729492188,\n",
       "  0.18701888620853424,\n",
       "  0.17854444682598114,\n",
       "  0.17266890406608582,\n",
       "  0.16457338631153107,\n",
       "  0.15839320421218872,\n",
       "  0.14707869291305542,\n",
       "  0.1427309662103653,\n",
       "  0.1357976645231247,\n",
       "  0.13067202270030975,\n",
       "  0.12527090311050415,\n",
       "  0.12102454900741577,\n",
       "  0.1159292683005333,\n",
       "  0.114115409553051,\n",
       "  0.11391067504882812,\n",
       "  0.10737400501966476,\n",
       "  0.10034889727830887,\n",
       "  0.10043063014745712,\n",
       "  0.09743808954954147,\n",
       "  0.09267257153987885,\n",
       "  0.09054809808731079,\n",
       "  0.08922364562749863,\n",
       "  0.08675560355186462,\n",
       "  0.08464521914720535,\n",
       "  0.08258842676877975,\n",
       "  0.08066094666719437,\n",
       "  0.07995395362377167,\n",
       "  0.07885187119245529,\n",
       "  0.07651182264089584,\n",
       "  0.07520565390586853,\n",
       "  0.07382531464099884,\n",
       "  0.0746014192700386,\n",
       "  0.0738334134221077,\n",
       "  0.07250066101551056,\n",
       "  0.0704399049282074,\n",
       "  0.07294835895299911,\n",
       "  0.07219763845205307,\n",
       "  0.07091087847948074,\n",
       "  0.06775105744600296,\n",
       "  0.06613119691610336,\n",
       "  0.06619273126125336,\n",
       "  0.06460992991924286,\n",
       "  0.06340788304805756,\n",
       "  0.06512676179409027,\n",
       "  0.06333368271589279,\n",
       "  0.06249342858791351,\n",
       "  0.06143597885966301,\n",
       "  0.06267152726650238,\n",
       "  0.06096124276518822,\n",
       "  0.061830002814531326,\n",
       "  0.06017473712563515,\n",
       "  0.060142144560813904,\n",
       "  0.059836070984601974,\n",
       "  0.057673655450344086,\n",
       "  0.06008152291178703,\n",
       "  0.05803613364696503,\n",
       "  0.05685236677527428,\n",
       "  0.06155761331319809,\n",
       "  0.05811837315559387,\n",
       "  0.05792002007365227,\n",
       "  0.05750565975904465,\n",
       "  0.05571684241294861,\n",
       "  0.05927515774965286,\n",
       "  0.0596499964594841,\n",
       "  0.055317021906375885,\n",
       "  0.05617436766624451,\n",
       "  0.06167061626911163,\n",
       "  0.06583421677350998,\n",
       "  0.058449115604162216,\n",
       "  0.058437999337911606,\n",
       "  0.05343293771147728,\n",
       "  0.05299051105976105,\n",
       "  0.05348183959722519,\n",
       "  0.05403871834278107,\n",
       "  0.057247456163167953,\n",
       "  0.053245771676301956,\n",
       "  0.05564649775624275,\n",
       "  0.05237610638141632,\n",
       "  0.05303520709276199,\n",
       "  0.052278902381658554,\n",
       "  0.050571903586387634,\n",
       "  0.05305023863911629,\n",
       "  0.052304044365882874,\n",
       "  0.051561418920755386,\n",
       "  0.05220229551196098,\n",
       "  0.05175211653113365,\n",
       "  0.04956284910440445,\n",
       "  0.055257752537727356,\n",
       "  0.05151533707976341,\n",
       "  0.05168528854846954,\n",
       "  0.05020641162991524,\n",
       "  0.054319821298122406,\n",
       "  0.05085044726729393,\n",
       "  0.04990971088409424,\n",
       "  0.049048393964767456,\n",
       "  0.050931304693222046,\n",
       "  0.051749181002378464,\n",
       "  0.04954839497804642,\n",
       "  0.04958150163292885,\n",
       "  0.04800938442349434,\n",
       "  0.049328096210956573,\n",
       "  0.04766694828867912,\n",
       "  0.04788173362612724,\n",
       "  0.048238031566143036,\n",
       "  0.04851887375116348,\n",
       "  0.052149154245853424,\n",
       "  0.04682283103466034,\n",
       "  0.04677870497107506,\n",
       "  0.04697686806321144,\n",
       "  0.04979869723320007,\n",
       "  0.04727312549948692,\n",
       "  0.04548795893788338,\n",
       "  0.046661127358675,\n",
       "  0.0461430624127388,\n",
       "  0.04680255055427551,\n",
       "  0.046933554112911224,\n",
       "  0.04907156154513359,\n",
       "  0.05350895971059799,\n",
       "  0.05140729993581772,\n",
       "  0.0456698015332222,\n",
       "  0.04610070213675499,\n",
       "  0.04513294994831085,\n",
       "  0.04766302555799484,\n",
       "  0.04509831964969635,\n",
       "  0.04539304971694946,\n",
       "  0.04560815915465355,\n",
       "  0.048189688473939896,\n",
       "  0.04941192641854286,\n",
       "  0.045276373624801636,\n",
       "  0.04335607960820198,\n",
       "  0.04478760436177254,\n",
       "  0.043470486998558044,\n",
       "  0.043234020471572876,\n",
       "  0.04375486448407173,\n",
       "  0.0428047701716423,\n",
       "  0.042924195528030396,\n",
       "  0.04348084330558777,\n",
       "  0.044853683561086655,\n",
       "  0.04777740314602852,\n",
       "  0.04348670318722725,\n",
       "  0.0459589920938015,\n",
       "  0.05017466098070145,\n",
       "  0.044880691915750504,\n",
       "  0.05366664379835129,\n",
       "  0.04494674131274223,\n",
       "  0.042442064732313156,\n",
       "  0.04235737398266792,\n",
       "  0.041225235909223557,\n",
       "  0.04283110424876213,\n",
       "  0.04197733476758003,\n",
       "  0.04119742289185524,\n",
       "  0.042228326201438904,\n",
       "  0.04102077707648277,\n",
       "  0.042143870145082474,\n",
       "  0.041545320302248,\n",
       "  0.04402749985456467,\n",
       "  0.04009995982050896,\n",
       "  0.0409068688750267,\n",
       "  0.046079397201538086,\n",
       "  0.04073406010866165,\n",
       "  0.04008319601416588,\n",
       "  0.04006019979715347,\n",
       "  0.03961838036775589,\n",
       "  0.040605705231428146,\n",
       "  0.041178710758686066,\n",
       "  0.03856581449508667,\n",
       "  0.038545336574316025,\n",
       "  0.04089958220720291,\n",
       "  0.03910442069172859,\n",
       "  0.03891595080494881,\n",
       "  0.038867536932229996,\n",
       "  0.03962794691324234,\n",
       "  0.042154066264629364,\n",
       "  0.042088981717824936,\n",
       "  0.04143553227186203,\n",
       "  0.04220891371369362,\n",
       "  0.039077699184417725,\n",
       "  0.03698812425136566,\n",
       "  0.04216356948018074,\n",
       "  0.04139048233628273,\n",
       "  0.038314905017614365,\n",
       "  0.037141624838113785,\n",
       "  0.037170227617025375,\n",
       "  0.03949350863695145,\n",
       "  0.03846459463238716,\n",
       "  0.036657147109508514,\n",
       "  0.036817941814661026,\n",
       "  0.039008721709251404,\n",
       "  0.03779184818267822,\n",
       "  0.036742061376571655,\n",
       "  0.03792756423354149,\n",
       "  0.036178093403577805,\n",
       "  0.03661726415157318,\n",
       "  0.035977140069007874,\n",
       "  0.03795215114951134,\n",
       "  0.03615926578640938,\n",
       "  0.03666719049215317,\n",
       "  0.03684733435511589,\n",
       "  0.034918222576379776,\n",
       "  0.0360548235476017,\n",
       "  0.03915117308497429,\n",
       "  0.03480852022767067,\n",
       "  0.034829262644052505,\n",
       "  0.03411184251308441,\n",
       "  0.03599587455391884,\n",
       "  0.03431941941380501,\n",
       "  0.036467693746089935,\n",
       "  0.03363138809800148,\n",
       "  0.033827438950538635,\n",
       "  0.03501823917031288,\n",
       "  0.034908030182123184,\n",
       "  0.03414008393883705,\n",
       "  0.033653825521469116,\n",
       "  0.03479596599936485,\n",
       "  0.03461731970310211,\n",
       "  0.03399552404880524,\n",
       "  0.03865042328834534,\n",
       "  0.03572811558842659,\n",
       "  0.03371682018041611,\n",
       "  0.035159945487976074,\n",
       "  0.03921284154057503,\n",
       "  0.037781424820423126,\n",
       "  0.03683973476290703,\n",
       "  0.03870944678783417,\n",
       "  0.03491182252764702,\n",
       "  0.03268744423985481,\n",
       "  0.032652147114276886,\n",
       "  0.0330486036837101,\n",
       "  0.03341365233063698,\n",
       "  0.030548470094799995,\n",
       "  0.030936529859900475,\n",
       "  0.030233535915613174,\n",
       "  0.03195110335946083,\n",
       "  0.03324596956372261,\n",
       "  0.03429436683654785,\n",
       "  0.03147273138165474,\n",
       "  0.03133756294846535,\n",
       "  0.03035212680697441,\n",
       "  0.03130576014518738,\n",
       "  0.028878211975097656,\n",
       "  0.030316073447465897,\n",
       "  0.03101138025522232,\n",
       "  0.03028562106192112,\n",
       "  0.03740236163139343,\n",
       "  0.035080861300230026,\n",
       "  0.02986113168299198,\n",
       "  0.0297361109405756,\n",
       "  0.028254162520170212,\n",
       "  0.028099795803427696,\n",
       "  0.02877771109342575,\n",
       "  0.027794860303401947,\n",
       "  0.02937554381787777,\n",
       "  0.028587166219949722,\n",
       "  0.027815740555524826,\n",
       "  0.028741726651787758,\n",
       "  0.02966385893523693,\n",
       "  0.030444884672760963,\n",
       "  0.032979246228933334,\n",
       "  0.028415240347385406,\n",
       "  0.027140052989125252,\n",
       "  0.02719889022409916,\n",
       "  0.026994187384843826,\n",
       "  0.02839244343340397,\n",
       "  0.031043021008372307,\n",
       "  0.026356825605034828,\n",
       "  0.026288095861673355,\n",
       "  0.025660814717411995,\n",
       "  0.026824355125427246,\n",
       "  0.027065707370638847,\n",
       "  0.025710176676511765,\n",
       "  0.02742752805352211,\n",
       "  0.03495638817548752,\n",
       "  0.029289070516824722,\n",
       "  0.026256410405039787,\n",
       "  0.02554059401154518,\n",
       "  0.02561808004975319,\n",
       "  0.031185295432806015,\n",
       "  0.031457360833883286,\n",
       "  0.0271000899374485,\n",
       "  0.024944819509983063,\n",
       "  0.025345169007778168,\n",
       "  0.023901425302028656,\n",
       "  0.025824980810284615,\n",
       "  0.0257297083735466,\n",
       "  0.02365030348300934,\n",
       "  0.023662149906158447,\n",
       "  0.02626814879477024,\n",
       "  0.03134339302778244,\n",
       "  0.027169356122612953,\n",
       "  0.02883041277527809,\n",
       "  0.027590055018663406,\n",
       "  0.023524172604084015,\n",
       "  0.02438443899154663,\n",
       "  0.0234836433082819,\n",
       "  0.029268790036439896,\n",
       "  0.027159204706549644,\n",
       "  0.021908536553382874,\n",
       "  0.03679945692420006,\n",
       "  0.024259911850094795,\n",
       "  0.024627551436424255,\n",
       "  0.026796914637088776,\n",
       "  0.029196737334132195,\n",
       "  0.02248038537800312,\n",
       "  0.021766776219010353,\n",
       "  0.022972768172621727,\n",
       "  0.022535128518939018,\n",
       "  0.021869083866477013,\n",
       "  0.021510768681764603,\n",
       "  0.022657360881567,\n",
       "  0.023997122421860695,\n",
       "  0.023949433118104935,\n",
       "  0.026417365297675133,\n",
       "  0.021361419931054115,\n",
       "  0.021135451272130013,\n",
       "  0.021684857085347176,\n",
       "  0.02095644921064377,\n",
       "  0.02140054851770401,\n",
       "  0.022470572963356972,\n",
       "  0.02717939391732216,\n",
       "  0.02305830642580986,\n",
       "  0.026298455893993378,\n",
       "  0.02259986288845539,\n",
       "  0.020520979538559914,\n",
       "  0.02081390656530857,\n",
       "  0.020832519978284836,\n",
       "  0.020537231117486954,\n",
       "  0.022801877930760384,\n",
       "  0.0241558738052845,\n",
       "  0.02517770417034626,\n",
       "  0.025135980919003487,\n",
       "  0.02334054745733738,\n",
       "  0.021259795874357224,\n",
       "  0.020484190434217453,\n",
       "  0.02582191675901413,\n",
       "  0.022656861692667007,\n",
       "  0.02087584137916565,\n",
       "  0.019543135538697243,\n",
       "  0.01887880451977253,\n",
       "  0.02135629579424858,\n",
       "  0.020063530653715134,\n",
       "  0.018986720591783524,\n",
       "  0.019181694835424423,\n",
       "  0.019048389047384262,\n",
       "  0.018516674637794495,\n",
       "  0.019071124494075775,\n",
       "  0.018519295379519463,\n",
       "  0.01792307011783123,\n",
       "  0.018654312938451767,\n",
       "  0.023313647136092186,\n",
       "  0.021075094118714333,\n",
       "  0.021731173619627953,\n",
       "  0.01883966661989689,\n",
       "  0.017865534871816635,\n",
       "  0.018335135653614998,\n",
       "  0.017273036763072014,\n",
       "  0.021720195189118385,\n",
       "  0.022252235561609268,\n",
       "  0.020640769973397255,\n",
       "  0.02117885835468769,\n",
       "  0.019223913550376892,\n",
       "  0.017692463472485542,\n",
       "  0.01897442899644375,\n",
       "  0.017534922808408737,\n",
       "  0.020612014457583427,\n",
       "  0.017490165308117867,\n",
       "  0.02111935243010521,\n",
       "  0.02141755074262619,\n",
       "  0.017907721921801567,\n",
       "  0.01916220225393772,\n",
       "  0.01713317632675171,\n",
       "  0.01813577115535736,\n",
       "  0.016812004148960114,\n",
       "  0.01852208934724331,\n",
       "  0.01676096022129059,\n",
       "  0.016519099473953247,\n",
       "  0.01648985594511032,\n",
       "  0.017502231523394585,\n",
       "  0.017999906092882156,\n",
       "  0.016148235648870468,\n",
       "  0.01733575016260147,\n",
       "  0.018612798303365707,\n",
       "  0.021542871370911598,\n",
       "  0.015962883830070496,\n",
       "  0.016277840360999107,\n",
       "  0.01613735780119896,\n",
       "  0.015609649010002613,\n",
       "  0.016268480569124222,\n",
       "  0.0159074105322361,\n",
       "  0.015461686998605728,\n",
       "  0.019340544939041138,\n",
       "  0.016913777217268944,\n",
       "  0.017513398081064224,\n",
       "  0.014314325526356697,\n",
       "  0.014942026697099209,\n",
       "  0.01408875361084938,\n",
       "  0.015838205814361572,\n",
       "  0.015334335155785084,\n",
       "  0.015119181014597416,\n",
       "  0.01665516570210457,\n",
       "  0.016007672995328903,\n",
       "  0.014898989349603653,\n",
       "  0.016010280698537827,\n",
       "  0.014826291240751743,\n",
       "  0.016431201249361038,\n",
       "  0.01445383857935667,\n",
       "  0.015254311263561249,\n",
       "  0.016270821914076805,\n",
       "  0.013814438134431839,\n",
       "  0.016861356794834137,\n",
       "  0.01570107787847519,\n",
       "  0.014477348886430264,\n",
       "  0.013607019558548927,\n",
       "  0.013240346685051918,\n",
       "  0.014272208325564861,\n",
       "  0.013502409681677818,\n",
       "  0.01302812434732914,\n",
       "  0.013541880063712597,\n",
       "  0.014179089106619358,\n",
       "  0.013720286078751087,\n",
       "  0.013721290044486523,\n",
       "  0.013423025608062744,\n",
       "  0.013218291103839874,\n",
       "  0.012880675494670868,\n",
       "  0.013329933397471905,\n",
       "  0.012583969160914421,\n",
       "  0.012458241544663906,\n",
       "  0.014803154394030571,\n",
       "  0.014556731097400188,\n",
       "  0.01291003730148077,\n",
       "  0.012920411303639412,\n",
       "  0.013226721435785294,\n",
       "  0.012841331772506237,\n",
       "  0.01254125963896513,\n",
       "  0.012234904803335667,\n",
       "  0.012416241690516472,\n",
       "  0.013593879528343678,\n",
       "  0.01210431195795536,\n",
       "  0.013232550583779812,\n",
       "  0.01166046317666769,\n",
       "  0.013677507638931274,\n",
       "  0.015338299795985222,\n",
       "  0.013824946247041225,\n",
       "  0.013964046724140644,\n",
       "  0.011544388718903065,\n",
       "  0.011692713014781475,\n",
       "  0.013532644137740135,\n",
       "  0.01305585540831089,\n",
       "  0.012823746539652348,\n",
       "  0.012882247567176819,\n",
       "  0.01127290353178978,\n",
       "  0.01212624553591013,\n",
       "  0.011154822073876858,\n",
       "  0.011240145191550255,\n",
       "  0.01051649171859026,\n",
       "  0.011544525623321533,\n",
       "  0.011855035088956356,\n",
       "  0.01174972578883171,\n",
       "  0.011438346467912197,\n",
       "  0.010842733085155487,\n",
       "  0.015296817757189274,\n",
       "  0.016555393114686012,\n",
       "  0.0115824518725276,\n",
       "  0.013282515108585358,\n",
       "  0.014103329740464687,\n",
       "  0.019393211230635643,\n",
       "  0.015699023380875587,\n",
       "  0.010989261791110039,\n",
       "  0.01044689305126667,\n",
       "  0.009946896694600582,\n",
       "  0.010817086324095726,\n",
       "  0.011109392158687115,\n",
       "  0.020654937252402306,\n",
       "  0.01513348612934351,\n",
       "  0.01069811824709177,\n",
       "  0.011591186746954918,\n",
       "  0.010505780577659607,\n",
       "  0.011066453531384468,\n",
       "  0.010609621182084084,\n",
       "  0.010641251690685749,\n",
       "  0.012640516273677349,\n",
       "  0.010932046920061111,\n",
       "  0.014297908172011375,\n",
       "  0.00926765613257885,\n",
       "  0.01002715714275837,\n",
       "  0.010543894954025745,\n",
       "  0.010561813600361347,\n",
       "  0.008801118470728397,\n",
       "  0.009231317788362503,\n",
       "  0.009928741492331028,\n",
       "  0.010659975931048393,\n",
       "  0.010011455044150352,\n",
       "  0.011175521649420261,\n",
       "  0.011710581369698048,\n",
       "  0.00841184426099062,\n",
       "  0.009798898361623287,\n",
       "  0.00934783648699522,\n",
       "  0.008151923306286335,\n",
       "  0.008956036530435085,\n",
       "  0.010025324299931526,\n",
       "  0.008774043060839176,\n",
       "  0.009691632352769375,\n",
       "  0.008874362334609032,\n",
       "  0.00981870386749506,\n",
       "  0.009276003576815128,\n",
       "  0.009177535772323608,\n",
       "  0.008926012553274632,\n",
       "  0.011783652007579803,\n",
       "  0.010568988509476185,\n",
       "  0.008664277382194996,\n",
       "  0.007948949001729488,\n",
       "  0.008252148516476154,\n",
       "  0.008664571680128574,\n",
       "  0.007747129071503878,\n",
       "  0.008463883772492409,\n",
       "  0.008493663743138313,\n",
       "  0.008286788128316402,\n",
       "  0.007592115085572004,\n",
       "  0.007886518724262714,\n",
       "  0.013681527227163315,\n",
       "  0.009892802685499191,\n",
       "  0.00837253499776125,\n",
       "  0.008402601815760136,\n",
       "  0.00819014385342598,\n",
       "  0.012294447049498558,\n",
       "  0.008213449269533157,\n",
       "  0.007706048898398876,\n",
       "  0.007389854174107313,\n",
       "  0.007227256428450346,\n",
       "  0.007400108501315117,\n",
       "  0.007107309531420469,\n",
       "  0.007881008088588715,\n",
       "  0.008540674112737179,\n",
       "  0.008943023160099983,\n",
       "  0.007715993095189333,\n",
       "  0.007267998997122049,\n",
       "  0.006853929255157709,\n",
       "  0.009293719194829464,\n",
       "  0.007402326446026564,\n",
       "  0.0077693285420536995,\n",
       "  0.006973660085350275,\n",
       "  0.006834801752120256,\n",
       "  0.007377927657216787,\n",
       "  0.007199215702712536,\n",
       "  0.006635481957346201,\n",
       "  0.006721149664372206,\n",
       "  0.006311656907200813,\n",
       "  0.006707033608108759,\n",
       "  0.006323200650513172,\n",
       "  0.007434610277414322,\n",
       "  0.006693901959806681,\n",
       "  0.006415852811187506,\n",
       "  0.006737736519426107,\n",
       "  0.007263401988893747,\n",
       "  0.006882776040583849,\n",
       "  0.007176208309829235,\n",
       "  0.007905951701104641,\n",
       "  0.006566419266164303,\n",
       "  0.007458723150193691,\n",
       "  0.011883527040481567,\n",
       "  0.008111055940389633,\n",
       "  0.0076401736587285995,\n",
       "  0.00631309300661087,\n",
       "  0.0075368150137364864,\n",
       "  0.006565122399479151,\n",
       "  0.006071304902434349,\n",
       "  0.006005158647894859,\n",
       "  0.005814522039145231,\n",
       "  0.005735569167882204,\n",
       "  0.006499269977211952,\n",
       "  0.006544205360114574,\n",
       "  0.007913460955023766,\n",
       "  0.007082880940288305,\n",
       "  0.005734868813306093,\n",
       "  0.006295478902757168,\n",
       "  0.007326206658035517,\n",
       "  0.00542406877502799,\n",
       "  0.005527134984731674,\n",
       "  0.005916163790971041,\n",
       "  0.006362415384501219,\n",
       "  0.005695939529687166,\n",
       "  0.005661689210683107,\n",
       "  0.005460390821099281,\n",
       "  0.005487358663231134,\n",
       "  0.006626848131418228,\n",
       "  0.008147233165800571,\n",
       "  0.006290128454566002],\n",
       " 'val_loss': [0.5966796278953552,\n",
       "  0.5458390116691589,\n",
       "  0.4947417378425598,\n",
       "  0.44895151257514954,\n",
       "  0.40527182817459106,\n",
       "  0.3652783930301666,\n",
       "  0.32906845211982727,\n",
       "  0.2966088652610779,\n",
       "  0.2698492407798767,\n",
       "  0.24819804728031158,\n",
       "  0.22881610691547394,\n",
       "  0.21489471197128296,\n",
       "  0.1982981264591217,\n",
       "  0.18910330533981323,\n",
       "  0.177801251411438,\n",
       "  0.16927091777324677,\n",
       "  0.16080136597156525,\n",
       "  0.1560947597026825,\n",
       "  0.14900757372379303,\n",
       "  0.14455650746822357,\n",
       "  0.15004436671733856,\n",
       "  0.1360720843076706,\n",
       "  0.1335495561361313,\n",
       "  0.1327822357416153,\n",
       "  0.12807127833366394,\n",
       "  0.1269676834344864,\n",
       "  0.12344525754451752,\n",
       "  0.11953956633806229,\n",
       "  0.12155015021562576,\n",
       "  0.12120451033115387,\n",
       "  0.11452823132276535,\n",
       "  0.11699812859296799,\n",
       "  0.11808083206415176,\n",
       "  0.11343662440776825,\n",
       "  0.11482034623622894,\n",
       "  0.1125522032380104,\n",
       "  0.11277549713850021,\n",
       "  0.11441878974437714,\n",
       "  0.11254970729351044,\n",
       "  0.11457691341638565,\n",
       "  0.11094840615987778,\n",
       "  0.11276817321777344,\n",
       "  0.11169133335351944,\n",
       "  0.11135566979646683,\n",
       "  0.1104162409901619,\n",
       "  0.10921663790941238,\n",
       "  0.11590662598609924,\n",
       "  0.1083049476146698,\n",
       "  0.11088195443153381,\n",
       "  0.11740706861019135,\n",
       "  0.10585914552211761,\n",
       "  0.11174212396144867,\n",
       "  0.11048997193574905,\n",
       "  0.11418274790048599,\n",
       "  0.11127422004938126,\n",
       "  0.11479823291301727,\n",
       "  0.1089756190776825,\n",
       "  0.11420167237520218,\n",
       "  0.10833866149187088,\n",
       "  0.10753581672906876,\n",
       "  0.11732117086648941,\n",
       "  0.10764279961585999,\n",
       "  0.10985579341650009,\n",
       "  0.11006040126085281,\n",
       "  0.10830361396074295,\n",
       "  0.11421556025743484,\n",
       "  0.11695198714733124,\n",
       "  0.10842183232307434,\n",
       "  0.11381945759057999,\n",
       "  0.10961372405290604,\n",
       "  0.11946935951709747,\n",
       "  0.10913955420255661,\n",
       "  0.11749843508005142,\n",
       "  0.11780328303575516,\n",
       "  0.11314970999956131,\n",
       "  0.11173806339502335,\n",
       "  0.12588904798030853,\n",
       "  0.11346518248319626,\n",
       "  0.11195090413093567,\n",
       "  0.12253952026367188,\n",
       "  0.11308202892541885,\n",
       "  0.12900932133197784,\n",
       "  0.11543451249599457,\n",
       "  0.11852645874023438,\n",
       "  0.12117411196231842,\n",
       "  0.11559464037418365,\n",
       "  0.11756346374750137,\n",
       "  0.11632838100194931,\n",
       "  0.11759556829929352,\n",
       "  0.1148906797170639,\n",
       "  0.12751111388206482,\n",
       "  0.11604367941617966,\n",
       "  0.11998024582862854,\n",
       "  0.12567177414894104,\n",
       "  0.1143001988530159,\n",
       "  0.11903633177280426,\n",
       "  0.11901352554559708,\n",
       "  0.11559528857469559,\n",
       "  0.12549279630184174,\n",
       "  0.12396972626447678,\n",
       "  0.11360719054937363,\n",
       "  0.12241020798683167,\n",
       "  0.11875193566083908,\n",
       "  0.11577492952346802,\n",
       "  0.11833221465349197,\n",
       "  0.11953573673963547,\n",
       "  0.11692864447832108,\n",
       "  0.12232652306556702,\n",
       "  0.11906741559505463,\n",
       "  0.1263313889503479,\n",
       "  0.1172531247138977,\n",
       "  0.12292848527431488,\n",
       "  0.1317402571439743,\n",
       "  0.11894319206476212,\n",
       "  0.1220417320728302,\n",
       "  0.12676522135734558,\n",
       "  0.12306123971939087,\n",
       "  0.12739674746990204,\n",
       "  0.12429482489824295,\n",
       "  0.12986479699611664,\n",
       "  0.12698331475257874,\n",
       "  0.12553486227989197,\n",
       "  0.12989287078380585,\n",
       "  0.12780946493148804,\n",
       "  0.137614905834198,\n",
       "  0.11993975192308426,\n",
       "  0.13058814406394958,\n",
       "  0.12603552639484406,\n",
       "  0.1250162124633789,\n",
       "  0.12719769775867462,\n",
       "  0.11940306425094604,\n",
       "  0.140275239944458,\n",
       "  0.12656167149543762,\n",
       "  0.1263595074415207,\n",
       "  0.1293725073337555,\n",
       "  0.12923087179660797,\n",
       "  0.13038168847560883,\n",
       "  0.1309877336025238,\n",
       "  0.13282756507396698,\n",
       "  0.12414254993200302,\n",
       "  0.1427571028470993,\n",
       "  0.1367001235485077,\n",
       "  0.13951878249645233,\n",
       "  0.13199596107006073,\n",
       "  0.1336267590522766,\n",
       "  0.13593789935112,\n",
       "  0.13015687465667725,\n",
       "  0.13622818887233734,\n",
       "  0.13478398323059082,\n",
       "  0.13634903728961945,\n",
       "  0.14009028673171997,\n",
       "  0.1400040090084076,\n",
       "  0.13190588355064392,\n",
       "  0.14386354386806488,\n",
       "  0.13560594618320465,\n",
       "  0.12816090881824493,\n",
       "  0.13969002664089203,\n",
       "  0.13179896771907806,\n",
       "  0.1412186473608017,\n",
       "  0.13451342284679413,\n",
       "  0.13185031712055206,\n",
       "  0.13727439939975739,\n",
       "  0.13028652966022491,\n",
       "  0.13676509261131287,\n",
       "  0.13081786036491394,\n",
       "  0.13777002692222595,\n",
       "  0.13455229997634888,\n",
       "  0.14482101798057556,\n",
       "  0.13723888993263245,\n",
       "  0.13315212726593018,\n",
       "  0.14374475181102753,\n",
       "  0.13398179411888123,\n",
       "  0.13835509121418,\n",
       "  0.14508052170276642,\n",
       "  0.13558490574359894,\n",
       "  0.14128704369068146,\n",
       "  0.1317903846502304,\n",
       "  0.15552489459514618,\n",
       "  0.13300900161266327,\n",
       "  0.13966478407382965,\n",
       "  0.13558967411518097,\n",
       "  0.1439347267150879,\n",
       "  0.13728570938110352,\n",
       "  0.14114323258399963,\n",
       "  0.1456771194934845,\n",
       "  0.13774681091308594,\n",
       "  0.13595664501190186,\n",
       "  0.1314748078584671,\n",
       "  0.1305643916130066,\n",
       "  0.13672542572021484,\n",
       "  0.1344841569662094,\n",
       "  0.15081477165222168,\n",
       "  0.13504454493522644,\n",
       "  0.1493186354637146,\n",
       "  0.14758184552192688,\n",
       "  0.1343788504600525,\n",
       "  0.14411123096942902,\n",
       "  0.1370287388563156,\n",
       "  0.14374157786369324,\n",
       "  0.1396186351776123,\n",
       "  0.1348726451396942,\n",
       "  0.14078810811042786,\n",
       "  0.1379825323820114,\n",
       "  0.14605210721492767,\n",
       "  0.14542096853256226,\n",
       "  0.1387166678905487,\n",
       "  0.14352233707904816,\n",
       "  0.13984864950180054,\n",
       "  0.13291645050048828,\n",
       "  0.1469689905643463,\n",
       "  0.14512936770915985,\n",
       "  0.1491437405347824,\n",
       "  0.13900050520896912,\n",
       "  0.1437099725008011,\n",
       "  0.1442810297012329,\n",
       "  0.14665965735912323,\n",
       "  0.13999441266059875,\n",
       "  0.1392374038696289,\n",
       "  0.15483026206493378,\n",
       "  0.13263188302516937,\n",
       "  0.15461181104183197,\n",
       "  0.13849632441997528,\n",
       "  0.14443299174308777,\n",
       "  0.13859818875789642,\n",
       "  0.1564781665802002,\n",
       "  0.1460099220275879,\n",
       "  0.13613279163837433,\n",
       "  0.1459031105041504,\n",
       "  0.1436450332403183,\n",
       "  0.14315879344940186,\n",
       "  0.1350926011800766,\n",
       "  0.14368879795074463,\n",
       "  0.15472958981990814,\n",
       "  0.13691654801368713,\n",
       "  0.14285500347614288,\n",
       "  0.1402745544910431,\n",
       "  0.16074728965759277,\n",
       "  0.14205466210842133,\n",
       "  0.1392974853515625,\n",
       "  0.14516691863536835,\n",
       "  0.14956578612327576,\n",
       "  0.15595340728759766,\n",
       "  0.13973379135131836,\n",
       "  0.14532378315925598,\n",
       "  0.14924490451812744,\n",
       "  0.1496238112449646,\n",
       "  0.13006040453910828,\n",
       "  0.16735878586769104,\n",
       "  0.13726067543029785,\n",
       "  0.1514442414045334,\n",
       "  0.13508917391300201,\n",
       "  0.15518157184123993,\n",
       "  0.1454785168170929,\n",
       "  0.15933407843112946,\n",
       "  0.13887907564640045,\n",
       "  0.1386137753725052,\n",
       "  0.14271479845046997,\n",
       "  0.13313573598861694,\n",
       "  0.14593525230884552,\n",
       "  0.1465718001127243,\n",
       "  0.14183476567268372,\n",
       "  0.15725727379322052,\n",
       "  0.14163652062416077,\n",
       "  0.1539493054151535,\n",
       "  0.1407485157251358,\n",
       "  0.15821756422519684,\n",
       "  0.14767032861709595,\n",
       "  0.145268052816391,\n",
       "  0.13701766729354858,\n",
       "  0.1592121720314026,\n",
       "  0.14055155217647552,\n",
       "  0.15015080571174622,\n",
       "  0.15478596091270447,\n",
       "  0.14148299396038055,\n",
       "  0.1531667709350586,\n",
       "  0.14202791452407837,\n",
       "  0.1455429643392563,\n",
       "  0.14592720568180084,\n",
       "  0.15066063404083252,\n",
       "  0.14890405535697937,\n",
       "  0.14444395899772644,\n",
       "  0.15806744992733002,\n",
       "  0.14790713787078857,\n",
       "  0.14661146700382233,\n",
       "  0.1487433910369873,\n",
       "  0.14640524983406067,\n",
       "  0.1468370407819748,\n",
       "  0.14490804076194763,\n",
       "  0.1484176069498062,\n",
       "  0.14943227171897888,\n",
       "  0.14311283826828003,\n",
       "  0.16828113794326782,\n",
       "  0.14862972497940063,\n",
       "  0.15605469048023224,\n",
       "  0.1589333564043045,\n",
       "  0.1434650421142578,\n",
       "  0.15333789587020874,\n",
       "  0.15657147765159607,\n",
       "  0.15129373967647552,\n",
       "  0.15896308422088623,\n",
       "  0.15489643812179565,\n",
       "  0.1491885781288147,\n",
       "  0.14991086721420288,\n",
       "  0.15071755647659302,\n",
       "  0.15271537005901337,\n",
       "  0.1538407802581787,\n",
       "  0.14500609040260315,\n",
       "  0.1634691208600998,\n",
       "  0.14559437334537506,\n",
       "  0.1669866293668747,\n",
       "  0.14014886319637299,\n",
       "  0.1792725771665573,\n",
       "  0.14879678189754486,\n",
       "  0.16953691840171814,\n",
       "  0.14124076068401337,\n",
       "  0.16294322907924652,\n",
       "  0.14863236248493195,\n",
       "  0.16557317972183228,\n",
       "  0.15097635984420776,\n",
       "  0.1561337113380432,\n",
       "  0.1564731001853943,\n",
       "  0.15615560114383698,\n",
       "  0.15184399485588074,\n",
       "  0.16181731224060059,\n",
       "  0.16890588402748108,\n",
       "  0.15033642947673798,\n",
       "  0.15760338306427002,\n",
       "  0.16142289340496063,\n",
       "  0.1535647064447403,\n",
       "  0.16274294257164001,\n",
       "  0.14911828935146332,\n",
       "  0.1746491640806198,\n",
       "  0.14749041199684143,\n",
       "  0.1823926419019699,\n",
       "  0.15071448683738708,\n",
       "  0.15489494800567627,\n",
       "  0.15441888570785522,\n",
       "  0.16012606024742126,\n",
       "  0.16508227586746216,\n",
       "  0.15461494028568268,\n",
       "  0.1736832708120346,\n",
       "  0.14896436035633087,\n",
       "  0.16392970085144043,\n",
       "  0.16801907122135162,\n",
       "  0.15064279735088348,\n",
       "  0.16187429428100586,\n",
       "  0.16444307565689087,\n",
       "  0.1518850028514862,\n",
       "  0.17929646372795105,\n",
       "  0.16119709610939026,\n",
       "  0.17110472917556763,\n",
       "  0.1606280356645584,\n",
       "  0.1604306399822235,\n",
       "  0.1601940393447876,\n",
       "  0.16434048116207123,\n",
       "  0.1616329848766327,\n",
       "  0.1657555252313614,\n",
       "  0.15876638889312744,\n",
       "  0.1670158952474594,\n",
       "  0.16087867319583893,\n",
       "  0.17568135261535645,\n",
       "  0.17953549325466156,\n",
       "  0.18725375831127167,\n",
       "  0.16927635669708252,\n",
       "  0.17979931831359863,\n",
       "  0.16670241951942444,\n",
       "  0.170304074883461,\n",
       "  0.1620243638753891,\n",
       "  0.17274430394172668,\n",
       "  0.17726324498653412,\n",
       "  0.1627194881439209,\n",
       "  0.20593632757663727,\n",
       "  0.1560569554567337,\n",
       "  0.17504456639289856,\n",
       "  0.17004987597465515,\n",
       "  0.1788383275270462,\n",
       "  0.1646159142255783,\n",
       "  0.2022881805896759,\n",
       "  0.15665949881076813,\n",
       "  0.18141354620456696,\n",
       "  0.1590368002653122,\n",
       "  0.20179520547389984,\n",
       "  0.16763892769813538,\n",
       "  0.17148877680301666,\n",
       "  0.17447052896022797,\n",
       "  0.16561485826969147,\n",
       "  0.1708117425441742,\n",
       "  0.17922943830490112,\n",
       "  0.17017610371112823,\n",
       "  0.16763652861118317,\n",
       "  0.18454234302043915,\n",
       "  0.17399661242961884,\n",
       "  0.17916429042816162,\n",
       "  0.159489706158638,\n",
       "  0.188604936003685,\n",
       "  0.17000776529312134,\n",
       "  0.1768331676721573,\n",
       "  0.1757998764514923,\n",
       "  0.1862325817346573,\n",
       "  0.18083366751670837,\n",
       "  0.1837204545736313,\n",
       "  0.17830459773540497,\n",
       "  0.182451531291008,\n",
       "  0.17017294466495514,\n",
       "  0.20324145257472992,\n",
       "  0.18225160241127014,\n",
       "  0.1892242282629013,\n",
       "  0.17429964244365692,\n",
       "  0.19984354078769684,\n",
       "  0.17763330042362213,\n",
       "  0.17818962037563324,\n",
       "  0.19021297991275787,\n",
       "  0.17816631495952606,\n",
       "  0.18469814956188202,\n",
       "  0.19264402985572815,\n",
       "  0.20023086667060852,\n",
       "  0.1873258650302887,\n",
       "  0.1932021975517273,\n",
       "  0.18696650862693787,\n",
       "  0.17806871235370636,\n",
       "  0.18551118671894073,\n",
       "  0.20739053189754486,\n",
       "  0.20153748989105225,\n",
       "  0.18432949483394623,\n",
       "  0.18296211957931519,\n",
       "  0.2087155282497406,\n",
       "  0.18488800525665283,\n",
       "  0.19077572226524353,\n",
       "  0.19772474467754364,\n",
       "  0.19819672405719757,\n",
       "  0.18255823850631714,\n",
       "  0.18916890025138855,\n",
       "  0.18978895246982574,\n",
       "  0.19621331989765167,\n",
       "  0.19059918820858002,\n",
       "  0.19646330177783966,\n",
       "  0.1860419064760208,\n",
       "  0.1973842978477478,\n",
       "  0.19719089567661285,\n",
       "  0.20306456089019775,\n",
       "  0.18633367121219635,\n",
       "  0.20981764793395996,\n",
       "  0.20049095153808594,\n",
       "  0.19292449951171875,\n",
       "  0.22619274258613586,\n",
       "  0.19208158552646637,\n",
       "  0.20650413632392883,\n",
       "  0.1909298449754715,\n",
       "  0.21654441952705383,\n",
       "  0.1988755762577057,\n",
       "  0.2070680558681488,\n",
       "  0.21052023768424988,\n",
       "  0.20725713670253754,\n",
       "  0.20340849459171295,\n",
       "  0.2047538161277771,\n",
       "  0.20471706986427307,\n",
       "  0.20930854976177216,\n",
       "  0.1979016363620758,\n",
       "  0.21182216703891754,\n",
       "  0.21771125495433807,\n",
       "  0.19179219007492065,\n",
       "  0.21863716840744019,\n",
       "  0.20473656058311462,\n",
       "  0.21709543466567993,\n",
       "  0.20452433824539185,\n",
       "  0.211451917886734,\n",
       "  0.21413598954677582,\n",
       "  0.2122337520122528,\n",
       "  0.206275075674057,\n",
       "  0.22873322665691376,\n",
       "  0.21216431260108948,\n",
       "  0.21476952731609344,\n",
       "  0.21768394112586975,\n",
       "  0.26582276821136475,\n",
       "  0.20843875408172607,\n",
       "  0.22840557992458344,\n",
       "  0.2273634523153305,\n",
       "  0.2284090220928192,\n",
       "  0.2438596785068512,\n",
       "  0.2179352343082428,\n",
       "  0.2190321683883667,\n",
       "  0.2231987565755844,\n",
       "  0.22829774022102356,\n",
       "  0.22809571027755737,\n",
       "  0.25039660930633545,\n",
       "  0.2285866141319275,\n",
       "  0.25940608978271484,\n",
       "  0.22920621931552887,\n",
       "  0.2297142893075943,\n",
       "  0.24823400378227234,\n",
       "  0.23219440877437592,\n",
       "  0.23670503497123718,\n",
       "  0.2400560975074768,\n",
       "  0.24611394107341766,\n",
       "  0.2341405153274536,\n",
       "  0.24515213072299957,\n",
       "  0.24354641139507294,\n",
       "  0.23207591474056244,\n",
       "  0.25143980979919434,\n",
       "  0.2437710016965866,\n",
       "  0.24651366472244263,\n",
       "  0.24421627819538116,\n",
       "  0.2564898729324341,\n",
       "  0.24016112089157104,\n",
       "  0.25886228680610657,\n",
       "  0.25565972924232483,\n",
       "  0.24550402164459229,\n",
       "  0.2501688003540039,\n",
       "  0.25015702843666077,\n",
       "  0.2545417249202728,\n",
       "  0.23970328271389008,\n",
       "  0.25887876749038696,\n",
       "  0.253081738948822,\n",
       "  0.2688170075416565,\n",
       "  0.25207024812698364,\n",
       "  0.2589249610900879,\n",
       "  0.26312679052352905,\n",
       "  0.25385671854019165,\n",
       "  0.26580536365509033,\n",
       "  0.2622703015804291,\n",
       "  0.2912420928478241,\n",
       "  0.2608025074005127,\n",
       "  0.25688377022743225,\n",
       "  0.26078280806541443,\n",
       "  0.2706795334815979,\n",
       "  0.2635914385318756,\n",
       "  0.2695435881614685,\n",
       "  0.2623412311077118,\n",
       "  0.2875768840312958,\n",
       "  0.273423433303833,\n",
       "  0.2717629075050354,\n",
       "  0.2737066447734833,\n",
       "  0.2894386947154999,\n",
       "  0.26629599928855896,\n",
       "  0.29365992546081543,\n",
       "  0.27823707461357117,\n",
       "  0.2808959186077118,\n",
       "  0.2708316743373871,\n",
       "  0.2897569537162781,\n",
       "  0.29043322801589966,\n",
       "  0.2811411917209625,\n",
       "  0.2867492437362671,\n",
       "  0.28851255774497986,\n",
       "  0.2783685028553009,\n",
       "  0.2896113693714142,\n",
       "  0.2763349413871765,\n",
       "  0.3034393787384033,\n",
       "  0.3000476658344269,\n",
       "  0.29508113861083984,\n",
       "  0.2863253653049469,\n",
       "  0.29044845700263977,\n",
       "  0.29441311955451965,\n",
       "  0.2983150780200958,\n",
       "  0.29707205295562744,\n",
       "  0.29478609561920166,\n",
       "  0.29467135667800903,\n",
       "  0.3015672266483307,\n",
       "  0.29957786202430725,\n",
       "  0.3038761615753174,\n",
       "  0.2988503873348236,\n",
       "  0.30470705032348633,\n",
       "  0.3117559552192688,\n",
       "  0.30662214756011963,\n",
       "  0.3056153655052185,\n",
       "  0.3093582093715668,\n",
       "  0.32231977581977844,\n",
       "  0.31452351808547974,\n",
       "  0.31802359223365784,\n",
       "  0.3083033263683319,\n",
       "  0.31309616565704346,\n",
       "  0.32581186294555664,\n",
       "  0.3067129850387573,\n",
       "  0.3248593509197235,\n",
       "  0.3328644633293152,\n",
       "  0.30902040004730225,\n",
       "  0.3146047592163086,\n",
       "  0.3119487166404724,\n",
       "  0.33818838000297546,\n",
       "  0.333324134349823,\n",
       "  0.3183160126209259,\n",
       "  0.32252630591392517,\n",
       "  0.3283107876777649,\n",
       "  0.32418885827064514,\n",
       "  0.3189243972301483,\n",
       "  0.32164955139160156,\n",
       "  0.3515256941318512,\n",
       "  0.3300398588180542,\n",
       "  0.3440018594264984,\n",
       "  0.332306444644928,\n",
       "  0.3441241383552551,\n",
       "  0.33984479308128357,\n",
       "  0.33258944749832153,\n",
       "  0.33712038397789,\n",
       "  0.33701300621032715,\n",
       "  0.3405633270740509,\n",
       "  0.3542267978191376,\n",
       "  0.35574913024902344,\n",
       "  0.3406396508216858,\n",
       "  0.3736702501773834,\n",
       "  0.3458089530467987]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loss = pd.DataFrame(model.history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.628325</td>\n",
       "      <td>0.596680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.574188</td>\n",
       "      <td>0.545839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.524299</td>\n",
       "      <td>0.494742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.476366</td>\n",
       "      <td>0.448952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.432800</td>\n",
       "      <td>0.405272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>0.005460</td>\n",
       "      <td>0.354227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>0.005487</td>\n",
       "      <td>0.355749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>0.006627</td>\n",
       "      <td>0.340640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>0.008147</td>\n",
       "      <td>0.373670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>0.006290</td>\n",
       "      <td>0.345809</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         loss  val_loss\n",
       "0    0.628325  0.596680\n",
       "1    0.574188  0.545839\n",
       "2    0.524299  0.494742\n",
       "3    0.476366  0.448952\n",
       "4    0.432800  0.405272\n",
       "..        ...       ...\n",
       "595  0.005460  0.354227\n",
       "596  0.005487  0.355749\n",
       "597  0.006627  0.340640\n",
       "598  0.008147  0.373670\n",
       "599  0.006290  0.345809\n",
       "\n",
       "[600 rows x 2 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_loss.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Two: Early Stopping\n",
    "\n",
    "We obviously trained too much! Let's use early stopping to track the val_loss and stop training once it begins increasing too much!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(units=30,activation='relu'))\n",
    "model.add(Dense(units=15,activation='relu'))\n",
    "model.add(Dense(units=1,activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class EarlyStopping in module keras.callbacks:\n",
      "\n",
      "class EarlyStopping(Callback)\n",
      " |  EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n",
      " |  \n",
      " |  Stop training when a monitored metric has stopped improving.\n",
      " |  \n",
      " |  Assuming the goal of a training is to minimize the loss. With this, the\n",
      " |  metric to be monitored would be `'loss'`, and mode would be `'min'`. A\n",
      " |  `model.fit()` training loop will check at end of every epoch whether\n",
      " |  the loss is no longer decreasing, considering the `min_delta` and\n",
      " |  `patience` if applicable. Once it's found no longer decreasing,\n",
      " |  `model.stop_training` is marked True and the training terminates.\n",
      " |  \n",
      " |  The quantity to be monitored needs to be available in `logs` dict.\n",
      " |  To make it so, pass the loss or metrics at `model.compile()`.\n",
      " |  \n",
      " |  Args:\n",
      " |    monitor: Quantity to be monitored.\n",
      " |    min_delta: Minimum change in the monitored quantity\n",
      " |        to qualify as an improvement, i.e. an absolute\n",
      " |        change of less than min_delta, will count as no\n",
      " |        improvement.\n",
      " |    patience: Number of epochs with no improvement\n",
      " |        after which training will be stopped.\n",
      " |    verbose: verbosity mode.\n",
      " |    mode: One of `{\"auto\", \"min\", \"max\"}`. In `min` mode,\n",
      " |        training will stop when the quantity\n",
      " |        monitored has stopped decreasing; in `\"max\"`\n",
      " |        mode it will stop when the quantity\n",
      " |        monitored has stopped increasing; in `\"auto\"`\n",
      " |        mode, the direction is automatically inferred\n",
      " |        from the name of the monitored quantity.\n",
      " |    baseline: Baseline value for the monitored quantity.\n",
      " |        Training will stop if the model doesn't show improvement over the\n",
      " |        baseline.\n",
      " |    restore_best_weights: Whether to restore model weights from\n",
      " |        the epoch with the best value of the monitored quantity.\n",
      " |        If False, the model weights obtained at the last step of\n",
      " |        training are used. An epoch will be restored regardless\n",
      " |        of the performance relative to the `baseline`. If no epoch\n",
      " |        improves on `baseline`, training will run for `patience`\n",
      " |        epochs and restore weights from the best epoch in that set.\n",
      " |  \n",
      " |  Example:\n",
      " |  \n",
      " |  >>> callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
      " |  >>> # This callback will stop the training when there is no improvement in\n",
      " |  >>> # the loss for three consecutive epochs.\n",
      " |  >>> model = tf.keras.models.Sequential([tf.keras.layers.Dense(10)])\n",
      " |  >>> model.compile(tf.keras.optimizers.SGD(), loss='mse')\n",
      " |  >>> history = model.fit(np.arange(100).reshape(5, 20), np.zeros(5),\n",
      " |  ...                     epochs=10, batch_size=1, callbacks=[callback],\n",
      " |  ...                     verbose=0)\n",
      " |  >>> len(history.history['loss'])  # Only 4 epochs are run.\n",
      " |  4\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      EarlyStopping\n",
      " |      Callback\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  get_monitor_value(self, logs)\n",
      " |  \n",
      " |  on_epoch_end(self, epoch, logs=None)\n",
      " |      Called at the end of an epoch.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run. This function should only\n",
      " |      be called during TRAIN mode.\n",
      " |      \n",
      " |      Args:\n",
      " |          epoch: Integer, index of epoch.\n",
      " |          logs: Dict, metric results for this training epoch, and for the\n",
      " |            validation epoch if validation is performed. Validation result keys\n",
      " |            are prefixed with `val_`. For training epoch, the values of the\n",
      " |           `Model`'s metrics are returned. Example : `{'loss': 0.2, 'accuracy':\n",
      " |             0.7}`.\n",
      " |  \n",
      " |  on_train_begin(self, logs=None)\n",
      " |      Called at the beginning of training.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Args:\n",
      " |          logs: Dict. Currently no data is passed to this argument for this method\n",
      " |            but that may change in the future.\n",
      " |  \n",
      " |  on_train_end(self, logs=None)\n",
      " |      Called at the end of training.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Args:\n",
      " |          logs: Dict. Currently the output of the last call to `on_epoch_end()`\n",
      " |            is passed to this argument for this method but that may change in\n",
      " |            the future.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from Callback:\n",
      " |  \n",
      " |  on_batch_begin(self, batch, logs=None)\n",
      " |      A backwards compatibility alias for `on_train_batch_begin`.\n",
      " |  \n",
      " |  on_batch_end(self, batch, logs=None)\n",
      " |      A backwards compatibility alias for `on_train_batch_end`.\n",
      " |  \n",
      " |  on_epoch_begin(self, epoch, logs=None)\n",
      " |      Called at the start of an epoch.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run. This function should only\n",
      " |      be called during TRAIN mode.\n",
      " |      \n",
      " |      Args:\n",
      " |          epoch: Integer, index of epoch.\n",
      " |          logs: Dict. Currently no data is passed to this argument for this method\n",
      " |            but that may change in the future.\n",
      " |  \n",
      " |  on_predict_batch_begin(self, batch, logs=None)\n",
      " |      Called at the beginning of a batch in `predict` methods.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Note that if the `steps_per_execution` argument to `compile` in\n",
      " |      `tf.keras.Model` is set to `N`, this method will only be called every `N`\n",
      " |      batches.\n",
      " |      \n",
      " |      Args:\n",
      " |          batch: Integer, index of batch within the current epoch.\n",
      " |          logs: Dict. Currently no data is passed to this argument for this method\n",
      " |            but that may change in the future.\n",
      " |  \n",
      " |  on_predict_batch_end(self, batch, logs=None)\n",
      " |      Called at the end of a batch in `predict` methods.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Note that if the `steps_per_execution` argument to `compile` in\n",
      " |      `tf.keras.Model` is set to `N`, this method will only be called every `N`\n",
      " |      batches.\n",
      " |      \n",
      " |      Args:\n",
      " |          batch: Integer, index of batch within the current epoch.\n",
      " |          logs: Dict. Aggregated metric results up until this batch.\n",
      " |  \n",
      " |  on_predict_begin(self, logs=None)\n",
      " |      Called at the beginning of prediction.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Args:\n",
      " |          logs: Dict. Currently no data is passed to this argument for this method\n",
      " |            but that may change in the future.\n",
      " |  \n",
      " |  on_predict_end(self, logs=None)\n",
      " |      Called at the end of prediction.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Args:\n",
      " |          logs: Dict. Currently no data is passed to this argument for this method\n",
      " |            but that may change in the future.\n",
      " |  \n",
      " |  on_test_batch_begin(self, batch, logs=None)\n",
      " |      Called at the beginning of a batch in `evaluate` methods.\n",
      " |      \n",
      " |      Also called at the beginning of a validation batch in the `fit`\n",
      " |      methods, if validation data is provided.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Note that if the `steps_per_execution` argument to `compile` in\n",
      " |      `tf.keras.Model` is set to `N`, this method will only be called every `N`\n",
      " |      batches.\n",
      " |      \n",
      " |      Args:\n",
      " |          batch: Integer, index of batch within the current epoch.\n",
      " |          logs: Dict. Currently no data is passed to this argument for this method\n",
      " |            but that may change in the future.\n",
      " |  \n",
      " |  on_test_batch_end(self, batch, logs=None)\n",
      " |      Called at the end of a batch in `evaluate` methods.\n",
      " |      \n",
      " |      Also called at the end of a validation batch in the `fit`\n",
      " |      methods, if validation data is provided.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Note that if the `steps_per_execution` argument to `compile` in\n",
      " |      `tf.keras.Model` is set to `N`, this method will only be called every `N`\n",
      " |      batches.\n",
      " |      \n",
      " |      Args:\n",
      " |          batch: Integer, index of batch within the current epoch.\n",
      " |          logs: Dict. Aggregated metric results up until this batch.\n",
      " |  \n",
      " |  on_test_begin(self, logs=None)\n",
      " |      Called at the beginning of evaluation or validation.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Args:\n",
      " |          logs: Dict. Currently no data is passed to this argument for this method\n",
      " |            but that may change in the future.\n",
      " |  \n",
      " |  on_test_end(self, logs=None)\n",
      " |      Called at the end of evaluation or validation.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Args:\n",
      " |          logs: Dict. Currently the output of the last call to\n",
      " |            `on_test_batch_end()` is passed to this argument for this method\n",
      " |            but that may change in the future.\n",
      " |  \n",
      " |  on_train_batch_begin(self, batch, logs=None)\n",
      " |      Called at the beginning of a training batch in `fit` methods.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Note that if the `steps_per_execution` argument to `compile` in\n",
      " |      `tf.keras.Model` is set to `N`, this method will only be called every `N`\n",
      " |      batches.\n",
      " |      \n",
      " |      Args:\n",
      " |          batch: Integer, index of batch within the current epoch.\n",
      " |          logs: Dict. Currently no data is passed to this argument for this method\n",
      " |            but that may change in the future.\n",
      " |  \n",
      " |  on_train_batch_end(self, batch, logs=None)\n",
      " |      Called at the end of a training batch in `fit` methods.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Note that if the `steps_per_execution` argument to `compile` in\n",
      " |      `tf.keras.Model` is set to `N`, this method will only be called every `N`\n",
      " |      batches.\n",
      " |      \n",
      " |      Args:\n",
      " |          batch: Integer, index of batch within the current epoch.\n",
      " |          logs: Dict. Aggregated metric results up until this batch.\n",
      " |  \n",
      " |  set_model(self, model)\n",
      " |  \n",
      " |  set_params(self, params)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from Callback:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(EarlyStopping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop training when a monitored quantity has stopped improving.\n",
    "\n",
    "    Arguments:\n",
    "        monitor: Quantity to be monitored.\n",
    "        min_delta: Minimum change in the monitored quantity\n",
    "            to qualify as an improvement, i.e. an absolute\n",
    "            change of less than min_delta, will count as no\n",
    "            improvement.\n",
    "        patience: Number of epochs with no improvement\n",
    "            after which training will be stopped.\n",
    "        verbose: verbosity mode.\n",
    "        mode: One of `{\"auto\", \"min\", \"max\"}`. In `min` mode,\n",
    "            training will stop when the quantity\n",
    "            monitored has stopped decreasing; in `max`\n",
    "            mode it will stop when the quantity\n",
    "            monitored has stopped increasing; in `auto`\n",
    "            mode, the direction is automatically inferred\n",
    "            from the name of the monitored quantity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/600\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 0.6891 - val_loss: 0.6782\n",
      "Epoch 2/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.6670 - val_loss: 0.6611\n",
      "Epoch 3/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.6488 - val_loss: 0.6416\n",
      "Epoch 4/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.6263 - val_loss: 0.6171\n",
      "Epoch 5/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.6002 - val_loss: 0.5887\n",
      "Epoch 6/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5687 - val_loss: 0.5549\n",
      "Epoch 7/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5328 - val_loss: 0.5147\n",
      "Epoch 8/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4858 - val_loss: 0.4562\n",
      "Epoch 9/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4307 - val_loss: 0.4014\n",
      "Epoch 10/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3851 - val_loss: 0.3555\n",
      "Epoch 11/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3432 - val_loss: 0.3171\n",
      "Epoch 12/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3062 - val_loss: 0.2799\n",
      "Epoch 13/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2815 - val_loss: 0.2543\n",
      "Epoch 14/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2560 - val_loss: 0.2328\n",
      "Epoch 15/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2379 - val_loss: 0.2136\n",
      "Epoch 16/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2205 - val_loss: 0.1993\n",
      "Epoch 17/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2075 - val_loss: 0.1852\n",
      "Epoch 18/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1955 - val_loss: 0.1754\n",
      "Epoch 19/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1871 - val_loss: 0.1646\n",
      "Epoch 20/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1790 - val_loss: 0.1586\n",
      "Epoch 21/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1746 - val_loss: 0.1530\n",
      "Epoch 22/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1716 - val_loss: 0.1452\n",
      "Epoch 23/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1560 - val_loss: 0.1487\n",
      "Epoch 24/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1488 - val_loss: 0.1354\n",
      "Epoch 25/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1427 - val_loss: 0.1380\n",
      "Epoch 26/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1418 - val_loss: 0.1271\n",
      "Epoch 27/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1327 - val_loss: 0.1338\n",
      "Epoch 28/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1266 - val_loss: 0.1207\n",
      "Epoch 29/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1235 - val_loss: 0.1211\n",
      "Epoch 30/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1191 - val_loss: 0.1166\n",
      "Epoch 31/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1139 - val_loss: 0.1167\n",
      "Epoch 32/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1118 - val_loss: 0.1099\n",
      "Epoch 33/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1063 - val_loss: 0.1150\n",
      "Epoch 34/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1035 - val_loss: 0.1051\n",
      "Epoch 35/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0973 - val_loss: 0.1128\n",
      "Epoch 36/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0966 - val_loss: 0.1065\n",
      "Epoch 37/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0941 - val_loss: 0.1011\n",
      "Epoch 38/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0923 - val_loss: 0.1042\n",
      "Epoch 39/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0878 - val_loss: 0.1023\n",
      "Epoch 40/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0853 - val_loss: 0.1010\n",
      "Epoch 41/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0832 - val_loss: 0.0976\n",
      "Epoch 42/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0813 - val_loss: 0.1035\n",
      "Epoch 43/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0793 - val_loss: 0.0986\n",
      "Epoch 44/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0780 - val_loss: 0.0996\n",
      "Epoch 45/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0769 - val_loss: 0.1002\n",
      "Epoch 46/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0775 - val_loss: 0.0946\n",
      "Epoch 47/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0749 - val_loss: 0.0989\n",
      "Epoch 48/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0725 - val_loss: 0.1005\n",
      "Epoch 49/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0724 - val_loss: 0.0959\n",
      "Epoch 50/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0700 - val_loss: 0.1036\n",
      "Epoch 51/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0701 - val_loss: 0.0948\n",
      "Epoch 52/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0746 - val_loss: 0.1058\n",
      "Epoch 53/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0689 - val_loss: 0.0972\n",
      "Epoch 54/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0703 - val_loss: 0.0976\n",
      "Epoch 55/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0676 - val_loss: 0.1039\n",
      "Epoch 56/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0643 - val_loss: 0.0927\n",
      "Epoch 57/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0659 - val_loss: 0.0982\n",
      "Epoch 58/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0650 - val_loss: 0.0948\n",
      "Epoch 59/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0654 - val_loss: 0.1023\n",
      "Epoch 60/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0681 - val_loss: 0.0949\n",
      "Epoch 61/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0672 - val_loss: 0.0972\n",
      "Epoch 62/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0647 - val_loss: 0.0962\n",
      "Epoch 63/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0672 - val_loss: 0.0950\n",
      "Epoch 64/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0676 - val_loss: 0.0975\n",
      "Epoch 65/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0618 - val_loss: 0.0960\n",
      "Epoch 66/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0616 - val_loss: 0.0978\n",
      "Epoch 67/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0606 - val_loss: 0.0967\n",
      "Epoch 68/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0600 - val_loss: 0.1038\n",
      "Epoch 69/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0603 - val_loss: 0.0960\n",
      "Epoch 70/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0593 - val_loss: 0.0986\n",
      "Epoch 71/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0624 - val_loss: 0.0993\n",
      "Epoch 72/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0613 - val_loss: 0.0974\n",
      "Epoch 73/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0599 - val_loss: 0.1084\n",
      "Epoch 74/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0581 - val_loss: 0.0946\n",
      "Epoch 75/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0601 - val_loss: 0.1034\n",
      "Epoch 76/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0570 - val_loss: 0.0935\n",
      "Epoch 77/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0576 - val_loss: 0.1039\n",
      "Epoch 78/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0576 - val_loss: 0.0968\n",
      "Epoch 79/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0597 - val_loss: 0.1019\n",
      "Epoch 80/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0560 - val_loss: 0.0973\n",
      "Epoch 81/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0568 - val_loss: 0.1012\n",
      "Epoch 81: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1880f1da310>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=X_train, \n",
    "          y=y_train, \n",
    "          epochs=600,\n",
    "          validation_data=(X_test, y_test), verbose=1,\n",
    "          callbacks=[early_stop]\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_loss = pd.DataFrame(model.history.history)\n",
    "model_loss.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Three: Adding in DropOut Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Dropout in module keras.layers.core.dropout:\n",
      "\n",
      "class Dropout(keras.engine.base_layer.BaseRandomLayer)\n",
      " |  Dropout(rate, noise_shape=None, seed=None, **kwargs)\n",
      " |  \n",
      " |  Applies Dropout to the input.\n",
      " |  \n",
      " |  The Dropout layer randomly sets input units to 0 with a frequency of `rate`\n",
      " |  at each step during training time, which helps prevent overfitting.\n",
      " |  Inputs not set to 0 are scaled up by 1/(1 - rate) such that the sum over\n",
      " |  all inputs is unchanged.\n",
      " |  \n",
      " |  Note that the Dropout layer only applies when `training` is set to True\n",
      " |  such that no values are dropped during inference. When using `model.fit`,\n",
      " |  `training` will be appropriately set to True automatically, and in other\n",
      " |  contexts, you can set the kwarg explicitly to True when calling the layer.\n",
      " |  \n",
      " |  (This is in contrast to setting `trainable=False` for a Dropout layer.\n",
      " |  `trainable` does not affect the layer's behavior, as Dropout does\n",
      " |  not have any variables/weights that can be frozen during training.)\n",
      " |  \n",
      " |  >>> tf.random.set_seed(0)\n",
      " |  >>> layer = tf.keras.layers.Dropout(.2, input_shape=(2,))\n",
      " |  >>> data = np.arange(10).reshape(5, 2).astype(np.float32)\n",
      " |  >>> print(data)\n",
      " |  [[0. 1.]\n",
      " |   [2. 3.]\n",
      " |   [4. 5.]\n",
      " |   [6. 7.]\n",
      " |   [8. 9.]]\n",
      " |  >>> outputs = layer(data, training=True)\n",
      " |  >>> print(outputs)\n",
      " |  tf.Tensor(\n",
      " |  [[ 0.    1.25]\n",
      " |   [ 2.5   3.75]\n",
      " |   [ 5.    6.25]\n",
      " |   [ 7.5   8.75]\n",
      " |   [10.    0.  ]], shape=(5, 2), dtype=float32)\n",
      " |  \n",
      " |  Args:\n",
      " |    rate: Float between 0 and 1. Fraction of the input units to drop.\n",
      " |    noise_shape: 1D integer tensor representing the shape of the\n",
      " |      binary dropout mask that will be multiplied with the input.\n",
      " |      For instance, if your inputs have shape\n",
      " |      `(batch_size, timesteps, features)` and\n",
      " |      you want the dropout mask to be the same for all timesteps,\n",
      " |      you can use `noise_shape=(batch_size, 1, features)`.\n",
      " |    seed: A Python integer to use as random seed.\n",
      " |  \n",
      " |  Call arguments:\n",
      " |    inputs: Input tensor (of any rank).\n",
      " |    training: Python boolean indicating whether the layer should behave in\n",
      " |      training mode (adding dropout) or in inference mode (doing nothing).\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Dropout\n",
      " |      keras.engine.base_layer.BaseRandomLayer\n",
      " |      keras.engine.base_layer.Layer\n",
      " |      tensorflow.python.module.module.Module\n",
      " |      tensorflow.python.training.tracking.autotrackable.AutoTrackable\n",
      " |      tensorflow.python.training.tracking.base.Trackable\n",
      " |      keras.utils.version_utils.LayerVersionSelector\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, rate, noise_shape=None, seed=None, **kwargs)\n",
      " |      Initialize the BaseRandomLayer.\n",
      " |      \n",
      " |      Note that the constructor is annotated with\n",
      " |      @no_automatic_dependency_tracking. This is to skip the auto\n",
      " |      tracking of self._random_generator instance, which is an AutoTrackable.\n",
      " |      The backend.RandomGenerator could contain a tf.random.Generator instance\n",
      " |      which will have tf.Variable as the internal state. We want to avoid saving\n",
      " |      that state into model.weights and checkpoints for backward compatibility\n",
      " |      reason. In the meantime, we still need to make them visible to SavedModel\n",
      " |      when it is tracing the tf.function for the `call()`.\n",
      " |      See _list_extra_dependencies_for_serialization below for more details.\n",
      " |      \n",
      " |      Args:\n",
      " |        seed: optional integer, used to create RandomGenerator.\n",
      " |        force_generator: boolean, default to False, whether to force the\n",
      " |          RandomGenerator to use the code branch of tf.random.Generator.\n",
      " |        **kwargs: other keyword arguments that will be passed to the parent class\n",
      " |  \n",
      " |  build(self, input_shape)\n",
      " |      Creates the variables of the layer (optional, for subclass implementers).\n",
      " |      \n",
      " |      This is a method that implementers of subclasses of `Layer` or `Model`\n",
      " |      can override if they need a state-creation step in-between\n",
      " |      layer instantiation and layer call. It is invoked automatically before\n",
      " |      the first execution of `call()`.\n",
      " |      \n",
      " |      This is typically used to create the weights of `Layer` subclasses\n",
      " |      (at the discretion of the subclass implementer).\n",
      " |      \n",
      " |      Args:\n",
      " |        input_shape: Instance of `TensorShape`, or list of instances of\n",
      " |          `TensorShape` if the layer expects a list of inputs\n",
      " |          (one instance per input).\n",
      " |  \n",
      " |  call(self, inputs, training=None)\n",
      " |      This is where the layer's logic lives.\n",
      " |      \n",
      " |      The `call()` method may not create state (except in its first invocation,\n",
      " |      wrapping the creation of variables or other resources in `tf.init_scope()`).\n",
      " |      It is recommended to create state in `__init__()`, or the `build()` method\n",
      " |      that is called automatically before `call()` executes the first time.\n",
      " |      \n",
      " |      Args:\n",
      " |        inputs: Input tensor, or dict/list/tuple of input tensors.\n",
      " |          The first positional `inputs` argument is subject to special rules:\n",
      " |          - `inputs` must be explicitly passed. A layer cannot have zero\n",
      " |            arguments, and `inputs` cannot be provided via the default value\n",
      " |            of a keyword argument.\n",
      " |          - NumPy array or Python scalar values in `inputs` get cast as tensors.\n",
      " |          - Keras mask metadata is only collected from `inputs`.\n",
      " |          - Layers are built (`build(input_shape)` method)\n",
      " |            using shape info from `inputs` only.\n",
      " |          - `input_spec` compatibility is only checked against `inputs`.\n",
      " |          - Mixed precision input casting is only applied to `inputs`.\n",
      " |            If a layer has tensor arguments in `*args` or `**kwargs`, their\n",
      " |            casting behavior in mixed precision should be handled manually.\n",
      " |          - The SavedModel input specification is generated using `inputs` only.\n",
      " |          - Integration with various ecosystem packages like TFMOT, TFLite,\n",
      " |            TF.js, etc is only supported for `inputs` and not for tensors in\n",
      " |            positional and keyword arguments.\n",
      " |        *args: Additional positional arguments. May contain tensors, although\n",
      " |          this is not recommended, for the reasons above.\n",
      " |        **kwargs: Additional keyword arguments. May contain tensors, although\n",
      " |          this is not recommended, for the reasons above.\n",
      " |          The following optional keyword arguments are reserved:\n",
      " |          - `training`: Boolean scalar tensor of Python boolean indicating\n",
      " |            whether the `call` is meant for training or inference.\n",
      " |          - `mask`: Boolean input mask. If the layer's `call()` method takes a\n",
      " |            `mask` argument, its default value will be set to the mask generated\n",
      " |            for `inputs` by the previous layer (if `input` did come from a layer\n",
      " |            that generated a corresponding mask, i.e. if it came from a Keras\n",
      " |            layer with masking support).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A tensor or list/tuple of tensors.\n",
      " |  \n",
      " |  compute_output_shape(self, input_shape)\n",
      " |      Computes the output shape of the layer.\n",
      " |      \n",
      " |      This method will cause the layer's state to be built, if that has not\n",
      " |      happened before. This requires that the layer will later be used with\n",
      " |      inputs that match the input shape provided here.\n",
      " |      \n",
      " |      Args:\n",
      " |          input_shape: Shape tuple (tuple of integers)\n",
      " |              or list of shape tuples (one per output tensor of the layer).\n",
      " |              Shape tuples can include None for free dimensions,\n",
      " |              instead of an integer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An input shape tuple.\n",
      " |  \n",
      " |  get_config(self)\n",
      " |      Returns the config of the layer.\n",
      " |      \n",
      " |      A layer config is a Python dictionary (serializable)\n",
      " |      containing the configuration of a layer.\n",
      " |      The same layer can be reinstantiated later\n",
      " |      (without its trained weights) from this configuration.\n",
      " |      \n",
      " |      The config of a layer does not include connectivity\n",
      " |      information, nor the layer class name. These are handled\n",
      " |      by `Network` (one layer of abstraction above).\n",
      " |      \n",
      " |      Note that `get_config()` does not guarantee to return a fresh copy of dict\n",
      " |      every time it is called. The callers should make a copy of the returned dict\n",
      " |      if they want to modify it.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Python dictionary.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  __call__(self, *args, **kwargs)\n",
      " |      Wraps `call`, applying pre- and post-processing steps.\n",
      " |      \n",
      " |      Args:\n",
      " |        *args: Positional arguments to be passed to `self.call`.\n",
      " |        **kwargs: Keyword arguments to be passed to `self.call`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor(s).\n",
      " |      \n",
      " |      Note:\n",
      " |        - The following optional keyword arguments are reserved for specific uses:\n",
      " |          * `training`: Boolean scalar tensor of Python boolean indicating\n",
      " |            whether the `call` is meant for training or inference.\n",
      " |          * `mask`: Boolean input mask.\n",
      " |        - If the layer's `call` method takes a `mask` argument (as some Keras\n",
      " |          layers do), its default value will be set to the mask generated\n",
      " |          for `inputs` by the previous layer (if `input` did come from\n",
      " |          a layer that generated a corresponding mask, i.e. if it came from\n",
      " |          a Keras layer with masking support.\n",
      " |        - If the layer is not built, the method will call `build`.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: if the layer's `call` method returns None (an invalid value).\n",
      " |        RuntimeError: if `super().__init__()` was not called in the constructor.\n",
      " |  \n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |      Support self.foo = trackable syntax.\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  add_loss(self, losses, **kwargs)\n",
      " |      Add loss tensor(s), potentially dependent on layer inputs.\n",
      " |      \n",
      " |      Some losses (for instance, activity regularization losses) may be dependent\n",
      " |      on the inputs passed when calling a layer. Hence, when reusing the same\n",
      " |      layer on different inputs `a` and `b`, some entries in `layer.losses` may\n",
      " |      be dependent on `a` and some on `b`. This method automatically keeps track\n",
      " |      of dependencies.\n",
      " |      \n",
      " |      This method can be used inside a subclassed layer or model's `call`\n",
      " |      function, in which case `losses` should be a Tensor or list of Tensors.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      class MyLayer(tf.keras.layers.Layer):\n",
      " |        def call(self, inputs):\n",
      " |          self.add_loss(tf.abs(tf.reduce_mean(inputs)))\n",
      " |          return inputs\n",
      " |      ```\n",
      " |      \n",
      " |      This method can also be called directly on a Functional Model during\n",
      " |      construction. In this case, any loss Tensors passed to this Model must\n",
      " |      be symbolic and be able to be traced back to the model's `Input`s. These\n",
      " |      losses become part of the model's topology and are tracked in `get_config`.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      # Activity regularization.\n",
      " |      model.add_loss(tf.abs(tf.reduce_mean(x)))\n",
      " |      ```\n",
      " |      \n",
      " |      If this is not the case for your loss (if, for example, your loss references\n",
      " |      a `Variable` of one of the model's layers), you can wrap your loss in a\n",
      " |      zero-argument lambda. These losses are not tracked as part of the model's\n",
      " |      topology since they can't be serialized.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      d = tf.keras.layers.Dense(10)\n",
      " |      x = d(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      # Weight regularization.\n",
      " |      model.add_loss(lambda: tf.reduce_mean(d.kernel))\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses\n",
      " |          may also be zero-argument callables which create a loss tensor.\n",
      " |        **kwargs: Additional keyword arguments for backward compatibility.\n",
      " |          Accepted values:\n",
      " |            inputs - Deprecated, will be automatically inferred.\n",
      " |  \n",
      " |  add_metric(self, value, name=None, **kwargs)\n",
      " |      Adds metric tensor to the layer.\n",
      " |      \n",
      " |      This method can be used inside the `call()` method of a subclassed layer\n",
      " |      or model.\n",
      " |      \n",
      " |      ```python\n",
      " |      class MyMetricLayer(tf.keras.layers.Layer):\n",
      " |        def __init__(self):\n",
      " |          super(MyMetricLayer, self).__init__(name='my_metric_layer')\n",
      " |          self.mean = tf.keras.metrics.Mean(name='metric_1')\n",
      " |      \n",
      " |        def call(self, inputs):\n",
      " |          self.add_metric(self.mean(inputs))\n",
      " |          self.add_metric(tf.reduce_sum(inputs), name='metric_2')\n",
      " |          return inputs\n",
      " |      ```\n",
      " |      \n",
      " |      This method can also be called directly on a Functional Model during\n",
      " |      construction. In this case, any tensor passed to this Model must\n",
      " |      be symbolic and be able to be traced back to the model's `Input`s. These\n",
      " |      metrics become part of the model's topology and are tracked when you\n",
      " |      save the model via `save()`.\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      model.add_metric(math_ops.reduce_sum(x), name='metric_1')\n",
      " |      ```\n",
      " |      \n",
      " |      Note: Calling `add_metric()` with the result of a metric object on a\n",
      " |      Functional Model, as shown in the example below, is not supported. This is\n",
      " |      because we cannot trace the metric result tensor back to the model's inputs.\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1')\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        value: Metric tensor.\n",
      " |        name: String metric name.\n",
      " |        **kwargs: Additional keyword arguments for backward compatibility.\n",
      " |          Accepted values:\n",
      " |          `aggregation` - When the `value` tensor provided is not the result of\n",
      " |          calling a `keras.Metric` instance, it will be aggregated by default\n",
      " |          using a `keras.Metric.Mean`.\n",
      " |  \n",
      " |  add_update(self, updates, inputs=None)\n",
      " |      Add update op(s), potentially dependent on layer inputs.\n",
      " |      \n",
      " |      Weight updates (for instance, the updates of the moving mean and variance\n",
      " |      in a BatchNormalization layer) may be dependent on the inputs passed\n",
      " |      when calling a layer. Hence, when reusing the same layer on\n",
      " |      different inputs `a` and `b`, some entries in `layer.updates` may be\n",
      " |      dependent on `a` and some on `b`. This method automatically keeps track\n",
      " |      of dependencies.\n",
      " |      \n",
      " |      This call is ignored when eager execution is enabled (in that case, variable\n",
      " |      updates are run on the fly and thus do not need to be tracked for later\n",
      " |      execution).\n",
      " |      \n",
      " |      Args:\n",
      " |        updates: Update op, or list/tuple of update ops, or zero-arg callable\n",
      " |          that returns an update op. A zero-arg callable should be passed in\n",
      " |          order to disable running the updates by setting `trainable=False`\n",
      " |          on this Layer, when executing in Eager mode.\n",
      " |        inputs: Deprecated, will be automatically inferred.\n",
      " |  \n",
      " |  add_variable(self, *args, **kwargs)\n",
      " |      Deprecated, do NOT use! Alias for `add_weight`.\n",
      " |  \n",
      " |  add_weight(self, name=None, shape=None, dtype=None, initializer=None, regularizer=None, trainable=None, constraint=None, use_resource=None, synchronization=<VariableSynchronization.AUTO: 0>, aggregation=<VariableAggregationV2.NONE: 0>, **kwargs)\n",
      " |      Adds a new variable to the layer.\n",
      " |      \n",
      " |      Args:\n",
      " |        name: Variable name.\n",
      " |        shape: Variable shape. Defaults to scalar if unspecified.\n",
      " |        dtype: The type of the variable. Defaults to `self.dtype`.\n",
      " |        initializer: Initializer instance (callable).\n",
      " |        regularizer: Regularizer instance (callable).\n",
      " |        trainable: Boolean, whether the variable should be part of the layer's\n",
      " |          \"trainable_variables\" (e.g. variables, biases)\n",
      " |          or \"non_trainable_variables\" (e.g. BatchNorm mean and variance).\n",
      " |          Note that `trainable` cannot be `True` if `synchronization`\n",
      " |          is set to `ON_READ`.\n",
      " |        constraint: Constraint instance (callable).\n",
      " |        use_resource: Whether to use `ResourceVariable`.\n",
      " |        synchronization: Indicates when a distributed a variable will be\n",
      " |          aggregated. Accepted values are constants defined in the class\n",
      " |          `tf.VariableSynchronization`. By default the synchronization is set to\n",
      " |          `AUTO` and the current `DistributionStrategy` chooses\n",
      " |          when to synchronize. If `synchronization` is set to `ON_READ`,\n",
      " |          `trainable` must not be set to `True`.\n",
      " |        aggregation: Indicates how a distributed variable will be aggregated.\n",
      " |          Accepted values are constants defined in the class\n",
      " |          `tf.VariableAggregation`.\n",
      " |        **kwargs: Additional keyword arguments. Accepted values are `getter`,\n",
      " |          `collections`, `experimental_autocast` and `caching_device`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The variable created.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: When giving unsupported dtype and no initializer or when\n",
      " |          trainable has been set to True with synchronization set as `ON_READ`.\n",
      " |  \n",
      " |  apply(self, inputs, *args, **kwargs)\n",
      " |      Deprecated, do NOT use!\n",
      " |      \n",
      " |      This is an alias of `self.__call__`.\n",
      " |      \n",
      " |      Args:\n",
      " |        inputs: Input tensor(s).\n",
      " |        *args: additional positional arguments to be passed to `self.call`.\n",
      " |        **kwargs: additional keyword arguments to be passed to `self.call`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor(s).\n",
      " |  \n",
      " |  compute_mask(self, inputs, mask=None)\n",
      " |      Computes an output mask tensor.\n",
      " |      \n",
      " |      Args:\n",
      " |          inputs: Tensor or list of tensors.\n",
      " |          mask: Tensor or list of tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |          None or a tensor (or list of tensors,\n",
      " |              one per output tensor of the layer).\n",
      " |  \n",
      " |  compute_output_signature(self, input_signature)\n",
      " |      Compute the output tensor signature of the layer based on the inputs.\n",
      " |      \n",
      " |      Unlike a TensorShape object, a TensorSpec object contains both shape\n",
      " |      and dtype information for a tensor. This method allows layers to provide\n",
      " |      output dtype information if it is different from the input dtype.\n",
      " |      For any layer that doesn't implement this function,\n",
      " |      the framework will fall back to use `compute_output_shape`, and will\n",
      " |      assume that the output dtype matches the input dtype.\n",
      " |      \n",
      " |      Args:\n",
      " |        input_signature: Single TensorSpec or nested structure of TensorSpec\n",
      " |          objects, describing a candidate input for the layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Single TensorSpec or nested structure of TensorSpec objects, describing\n",
      " |          how the layer would transform the provided input.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If input_signature contains a non-TensorSpec object.\n",
      " |  \n",
      " |  count_params(self)\n",
      " |      Count the total number of scalars composing the weights.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An integer count.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: if the layer isn't yet built\n",
      " |            (in which case its weights aren't yet defined).\n",
      " |  \n",
      " |  finalize_state(self)\n",
      " |      Finalizes the layers state after updating layer weights.\n",
      " |      \n",
      " |      This function can be subclassed in a layer and will be called after updating\n",
      " |      a layer weights. It can be overridden to finalize any additional layer state\n",
      " |      after a weight update.\n",
      " |      \n",
      " |      This function will be called after weights of a layer have been restored\n",
      " |      from a loaded model.\n",
      " |  \n",
      " |  get_input_at(self, node_index)\n",
      " |      Retrieves the input tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Args:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first input node of the layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tensor (or list of tensors if the layer has multiple inputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_input_mask_at(self, node_index)\n",
      " |      Retrieves the input mask tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Args:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A mask tensor\n",
      " |          (or list of tensors if the layer has multiple inputs).\n",
      " |  \n",
      " |  get_input_shape_at(self, node_index)\n",
      " |      Retrieves the input shape(s) of a layer at a given node.\n",
      " |      \n",
      " |      Args:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A shape tuple\n",
      " |          (or list of shape tuples if the layer has multiple inputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_losses_for(self, inputs)\n",
      " |      Deprecated, do NOT use!\n",
      " |      \n",
      " |      Retrieves losses relevant to a specific set of inputs.\n",
      " |      \n",
      " |      Args:\n",
      " |        inputs: Input tensor or list/tuple of input tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |        List of loss tensors of the layer that depend on `inputs`.\n",
      " |  \n",
      " |  get_output_at(self, node_index)\n",
      " |      Retrieves the output tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Args:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first output node of the layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tensor (or list of tensors if the layer has multiple outputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_output_mask_at(self, node_index)\n",
      " |      Retrieves the output mask tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Args:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A mask tensor\n",
      " |          (or list of tensors if the layer has multiple outputs).\n",
      " |  \n",
      " |  get_output_shape_at(self, node_index)\n",
      " |      Retrieves the output shape(s) of a layer at a given node.\n",
      " |      \n",
      " |      Args:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A shape tuple\n",
      " |          (or list of shape tuples if the layer has multiple outputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_updates_for(self, inputs)\n",
      " |      Deprecated, do NOT use!\n",
      " |      \n",
      " |      Retrieves updates relevant to a specific set of inputs.\n",
      " |      \n",
      " |      Args:\n",
      " |        inputs: Input tensor or list/tuple of input tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |        List of update ops of the layer that depend on `inputs`.\n",
      " |  \n",
      " |  get_weights(self)\n",
      " |      Returns the current weights of the layer, as NumPy arrays.\n",
      " |      \n",
      " |      The weights of a layer represent the state of the layer. This function\n",
      " |      returns both trainable and non-trainable weight values associated with this\n",
      " |      layer as a list of NumPy arrays, which can in turn be used to load state\n",
      " |      into similarly parameterized layers.\n",
      " |      \n",
      " |      For example, a `Dense` layer returns a list of two values: the kernel matrix\n",
      " |      and the bias vector. These can be used to set the weights of another\n",
      " |      `Dense` layer:\n",
      " |      \n",
      " |      >>> layer_a = tf.keras.layers.Dense(1,\n",
      " |      ...   kernel_initializer=tf.constant_initializer(1.))\n",
      " |      >>> a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]]))\n",
      " |      >>> layer_a.get_weights()\n",
      " |      [array([[1.],\n",
      " |             [1.],\n",
      " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      >>> layer_b = tf.keras.layers.Dense(1,\n",
      " |      ...   kernel_initializer=tf.constant_initializer(2.))\n",
      " |      >>> b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]]))\n",
      " |      >>> layer_b.get_weights()\n",
      " |      [array([[2.],\n",
      " |             [2.],\n",
      " |             [2.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      >>> layer_b.set_weights(layer_a.get_weights())\n",
      " |      >>> layer_b.get_weights()\n",
      " |      [array([[1.],\n",
      " |             [1.],\n",
      " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      \n",
      " |      Returns:\n",
      " |          Weights values as a list of NumPy arrays.\n",
      " |  \n",
      " |  set_weights(self, weights)\n",
      " |      Sets the weights of the layer, from NumPy arrays.\n",
      " |      \n",
      " |      The weights of a layer represent the state of the layer. This function\n",
      " |      sets the weight values from numpy arrays. The weight values should be\n",
      " |      passed in the order they are created by the layer. Note that the layer's\n",
      " |      weights must be instantiated before calling this function, by calling\n",
      " |      the layer.\n",
      " |      \n",
      " |      For example, a `Dense` layer returns a list of two values: the kernel matrix\n",
      " |      and the bias vector. These can be used to set the weights of another\n",
      " |      `Dense` layer:\n",
      " |      \n",
      " |      >>> layer_a = tf.keras.layers.Dense(1,\n",
      " |      ...   kernel_initializer=tf.constant_initializer(1.))\n",
      " |      >>> a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]]))\n",
      " |      >>> layer_a.get_weights()\n",
      " |      [array([[1.],\n",
      " |             [1.],\n",
      " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      >>> layer_b = tf.keras.layers.Dense(1,\n",
      " |      ...   kernel_initializer=tf.constant_initializer(2.))\n",
      " |      >>> b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]]))\n",
      " |      >>> layer_b.get_weights()\n",
      " |      [array([[2.],\n",
      " |             [2.],\n",
      " |             [2.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      >>> layer_b.set_weights(layer_a.get_weights())\n",
      " |      >>> layer_b.get_weights()\n",
      " |      [array([[1.],\n",
      " |             [1.],\n",
      " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      \n",
      " |      Args:\n",
      " |        weights: a list of NumPy arrays. The number\n",
      " |          of arrays and their shape must match\n",
      " |          number of the dimensions of the weights\n",
      " |          of the layer (i.e. it should match the\n",
      " |          output of `get_weights`).\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If the provided weights list does not match the\n",
      " |          layer's specifications.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  from_config(config) from builtins.type\n",
      " |      Creates a layer from its config.\n",
      " |      \n",
      " |      This method is the reverse of `get_config`,\n",
      " |      capable of instantiating the same layer from the config\n",
      " |      dictionary. It does not handle layer connectivity\n",
      " |      (handled by Network), nor weights (handled by `set_weights`).\n",
      " |      \n",
      " |      Args:\n",
      " |          config: A Python dictionary, typically the\n",
      " |              output of get_config.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A layer instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  compute_dtype\n",
      " |      The dtype of the layer's computations.\n",
      " |      \n",
      " |      This is equivalent to `Layer.dtype_policy.compute_dtype`. Unless\n",
      " |      mixed precision is used, this is the same as `Layer.dtype`, the dtype of\n",
      " |      the weights.\n",
      " |      \n",
      " |      Layers automatically cast their inputs to the compute dtype, which causes\n",
      " |      computations and the output to be in the compute dtype as well. This is done\n",
      " |      by the base Layer class in `Layer.__call__`, so you do not have to insert\n",
      " |      these casts if implementing your own layer.\n",
      " |      \n",
      " |      Layers often perform certain internal computations in higher precision when\n",
      " |      `compute_dtype` is float16 or bfloat16 for numeric stability. The output\n",
      " |      will still typically be float16 or bfloat16 in such cases.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The layer's compute dtype.\n",
      " |  \n",
      " |  dtype\n",
      " |      The dtype of the layer weights.\n",
      " |      \n",
      " |      This is equivalent to `Layer.dtype_policy.variable_dtype`. Unless\n",
      " |      mixed precision is used, this is the same as `Layer.compute_dtype`, the\n",
      " |      dtype of the layer's computations.\n",
      " |  \n",
      " |  dtype_policy\n",
      " |      The dtype policy associated with this layer.\n",
      " |      \n",
      " |      This is an instance of a `tf.keras.mixed_precision.Policy`.\n",
      " |  \n",
      " |  dynamic\n",
      " |      Whether the layer is dynamic (eager-only); set in the constructor.\n",
      " |  \n",
      " |  inbound_nodes\n",
      " |      Deprecated, do NOT use! Only for compatibility with external Keras.\n",
      " |  \n",
      " |  input\n",
      " |      Retrieves the input tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one input,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input tensor or list of input tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |        AttributeError: If no inbound nodes are found.\n",
      " |  \n",
      " |  input_mask\n",
      " |      Retrieves the input mask tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input mask tensor (potentially None) or list of input\n",
      " |          mask tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  input_shape\n",
      " |      Retrieves the input shape(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one input,\n",
      " |      i.e. if it is connected to one incoming layer, or if all inputs\n",
      " |      have the same shape.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input shape, as an integer shape tuple\n",
      " |          (or list of shape tuples, one tuple per input tensor).\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer has no defined input_shape.\n",
      " |          RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  losses\n",
      " |      List of losses added using the `add_loss()` API.\n",
      " |      \n",
      " |      Variable regularization tensors are created when this property is accessed,\n",
      " |      so it is eager safe: accessing `losses` under a `tf.GradientTape` will\n",
      " |      propagate gradients back to the corresponding variables.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      >>> class MyLayer(tf.keras.layers.Layer):\n",
      " |      ...   def call(self, inputs):\n",
      " |      ...     self.add_loss(tf.abs(tf.reduce_mean(inputs)))\n",
      " |      ...     return inputs\n",
      " |      >>> l = MyLayer()\n",
      " |      >>> l(np.ones((10, 1)))\n",
      " |      >>> l.losses\n",
      " |      [1.0]\n",
      " |      \n",
      " |      >>> inputs = tf.keras.Input(shape=(10,))\n",
      " |      >>> x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      >>> outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      >>> model = tf.keras.Model(inputs, outputs)\n",
      " |      >>> # Activity regularization.\n",
      " |      >>> len(model.losses)\n",
      " |      0\n",
      " |      >>> model.add_loss(tf.abs(tf.reduce_mean(x)))\n",
      " |      >>> len(model.losses)\n",
      " |      1\n",
      " |      \n",
      " |      >>> inputs = tf.keras.Input(shape=(10,))\n",
      " |      >>> d = tf.keras.layers.Dense(10, kernel_initializer='ones')\n",
      " |      >>> x = d(inputs)\n",
      " |      >>> outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      >>> model = tf.keras.Model(inputs, outputs)\n",
      " |      >>> # Weight regularization.\n",
      " |      >>> model.add_loss(lambda: tf.reduce_mean(d.kernel))\n",
      " |      >>> model.losses\n",
      " |      [<tf.Tensor: shape=(), dtype=float32, numpy=1.0>]\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of tensors.\n",
      " |  \n",
      " |  metrics\n",
      " |      List of metrics added using the `add_metric()` API.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      >>> input = tf.keras.layers.Input(shape=(3,))\n",
      " |      >>> d = tf.keras.layers.Dense(2)\n",
      " |      >>> output = d(input)\n",
      " |      >>> d.add_metric(tf.reduce_max(output), name='max')\n",
      " |      >>> d.add_metric(tf.reduce_min(output), name='min')\n",
      " |      >>> [m.name for m in d.metrics]\n",
      " |      ['max', 'min']\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of `Metric` objects.\n",
      " |  \n",
      " |  name\n",
      " |      Name of the layer (string), set in the constructor.\n",
      " |  \n",
      " |  non_trainable_variables\n",
      " |      Sequence of non-trainable variables owned by this module and its submodules.\n",
      " |      \n",
      " |      Note: this method uses reflection to find variables on the current instance\n",
      " |      and submodules. For performance reasons you may wish to cache the result\n",
      " |      of calling this method if you don't expect the return value to change.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A sequence of variables for the current module (sorted by attribute\n",
      " |        name) followed by variables from all submodules recursively (breadth\n",
      " |        first).\n",
      " |  \n",
      " |  non_trainable_weights\n",
      " |      List of all non-trainable weights tracked by this layer.\n",
      " |      \n",
      " |      Non-trainable weights are *not* updated during training. They are expected\n",
      " |      to be updated manually in `call()`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of non-trainable variables.\n",
      " |  \n",
      " |  outbound_nodes\n",
      " |      Deprecated, do NOT use! Only for compatibility with external Keras.\n",
      " |  \n",
      " |  output\n",
      " |      Retrieves the output tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one output,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor or list of output tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |        AttributeError: if the layer is connected to more than one incoming\n",
      " |          layers.\n",
      " |        RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  output_mask\n",
      " |      Retrieves the output mask tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Output mask tensor (potentially None) or list of output\n",
      " |          mask tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  output_shape\n",
      " |      Retrieves the output shape(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has one output,\n",
      " |      or if all outputs have the same shape.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Output shape, as an integer shape tuple\n",
      " |          (or list of shape tuples, one tuple per output tensor).\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer has no defined output shape.\n",
      " |          RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  trainable_variables\n",
      " |      Sequence of trainable variables owned by this module and its submodules.\n",
      " |      \n",
      " |      Note: this method uses reflection to find variables on the current instance\n",
      " |      and submodules. For performance reasons you may wish to cache the result\n",
      " |      of calling this method if you don't expect the return value to change.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A sequence of variables for the current module (sorted by attribute\n",
      " |        name) followed by variables from all submodules recursively (breadth\n",
      " |        first).\n",
      " |  \n",
      " |  trainable_weights\n",
      " |      List of all trainable weights tracked by this layer.\n",
      " |      \n",
      " |      Trainable weights are updated via gradient descent during training.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of trainable variables.\n",
      " |  \n",
      " |  updates\n",
      " |  \n",
      " |  variable_dtype\n",
      " |      Alias of `Layer.dtype`, the dtype of the weights.\n",
      " |  \n",
      " |  variables\n",
      " |      Returns the list of all layer variables/weights.\n",
      " |      \n",
      " |      Alias of `self.weights`.\n",
      " |      \n",
      " |      Note: This will not track the weights of nested `tf.Modules` that are not\n",
      " |      themselves Keras layers.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of variables.\n",
      " |  \n",
      " |  weights\n",
      " |      Returns the list of all layer variables/weights.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of variables.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  activity_regularizer\n",
      " |      Optional regularizer function for the output of this layer.\n",
      " |  \n",
      " |  input_spec\n",
      " |      `InputSpec` instance(s) describing the input format for this layer.\n",
      " |      \n",
      " |      When you create a layer subclass, you can set `self.input_spec` to enable\n",
      " |      the layer to run input compatibility checks when it is called.\n",
      " |      Consider a `Conv2D` layer: it can only be called on a single input tensor\n",
      " |      of rank 4. As such, you can set, in `__init__()`:\n",
      " |      \n",
      " |      ```python\n",
      " |      self.input_spec = tf.keras.layers.InputSpec(ndim=4)\n",
      " |      ```\n",
      " |      \n",
      " |      Now, if you try to call the layer on an input that isn't rank 4\n",
      " |      (for instance, an input of shape `(2,)`, it will raise a nicely-formatted\n",
      " |      error:\n",
      " |      \n",
      " |      ```\n",
      " |      ValueError: Input 0 of layer conv2d is incompatible with the layer:\n",
      " |      expected ndim=4, found ndim=1. Full shape received: [2]\n",
      " |      ```\n",
      " |      \n",
      " |      Input checks that can be specified via `input_spec` include:\n",
      " |      - Structure (e.g. a single input, a list of 2 inputs, etc)\n",
      " |      - Shape\n",
      " |      - Rank (ndim)\n",
      " |      - Dtype\n",
      " |      \n",
      " |      For more information, see `tf.keras.layers.InputSpec`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `tf.keras.layers.InputSpec` instance, or nested structure thereof.\n",
      " |  \n",
      " |  stateful\n",
      " |  \n",
      " |  supports_masking\n",
      " |      Whether this layer supports computing a mask using `compute_mask`.\n",
      " |  \n",
      " |  trainable\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from tensorflow.python.module.module.Module:\n",
      " |  \n",
      " |  with_name_scope(method) from builtins.type\n",
      " |      Decorator to automatically enter the module name scope.\n",
      " |      \n",
      " |      >>> class MyModule(tf.Module):\n",
      " |      ...   @tf.Module.with_name_scope\n",
      " |      ...   def __call__(self, x):\n",
      " |      ...     if not hasattr(self, 'w'):\n",
      " |      ...       self.w = tf.Variable(tf.random.normal([x.shape[1], 3]))\n",
      " |      ...     return tf.matmul(x, self.w)\n",
      " |      \n",
      " |      Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose\n",
      " |      names included the module name:\n",
      " |      \n",
      " |      >>> mod = MyModule()\n",
      " |      >>> mod(tf.ones([1, 2]))\n",
      " |      <tf.Tensor: shape=(1, 3), dtype=float32, numpy=..., dtype=float32)>\n",
      " |      >>> mod.w\n",
      " |      <tf.Variable 'my_module/Variable:0' shape=(2, 3) dtype=float32,\n",
      " |      numpy=..., dtype=float32)>\n",
      " |      \n",
      " |      Args:\n",
      " |        method: The method to wrap.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The original method wrapped such that it enters the module's name scope.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from tensorflow.python.module.module.Module:\n",
      " |  \n",
      " |  name_scope\n",
      " |      Returns a `tf.name_scope` instance for this class.\n",
      " |  \n",
      " |  submodules\n",
      " |      Sequence of all sub-modules.\n",
      " |      \n",
      " |      Submodules are modules which are properties of this module, or found as\n",
      " |      properties of modules which are properties of this module (and so on).\n",
      " |      \n",
      " |      >>> a = tf.Module()\n",
      " |      >>> b = tf.Module()\n",
      " |      >>> c = tf.Module()\n",
      " |      >>> a.b = b\n",
      " |      >>> b.c = c\n",
      " |      >>> list(a.submodules) == [b, c]\n",
      " |      True\n",
      " |      >>> list(b.submodules) == [c]\n",
      " |      True\n",
      " |      >>> list(c.submodules) == []\n",
      " |      True\n",
      " |      \n",
      " |      Returns:\n",
      " |        A sequence of all submodules.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.training.tracking.base.Trackable:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from keras.utils.version_utils.LayerVersionSelector:\n",
      " |  \n",
      " |  __new__(cls, *args, **kwargs)\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(units=30,activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(units=15,activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(units=1,activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/600\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.7229 - val_loss: 0.6894\n",
      "Epoch 2/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.6831 - val_loss: 0.6721\n",
      "Epoch 3/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.6803 - val_loss: 0.6578\n",
      "Epoch 4/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.6593 - val_loss: 0.6436\n",
      "Epoch 5/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.6383 - val_loss: 0.6220\n",
      "Epoch 6/600\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6244 - val_loss: 0.6021\n",
      "Epoch 7/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.6069 - val_loss: 0.5870\n",
      "Epoch 8/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5837 - val_loss: 0.5629\n",
      "Epoch 9/600\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5754 - val_loss: 0.5249\n",
      "Epoch 10/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5448 - val_loss: 0.4892\n",
      "Epoch 11/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5302 - val_loss: 0.4461\n",
      "Epoch 12/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4761 - val_loss: 0.4092\n",
      "Epoch 13/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4507 - val_loss: 0.3781\n",
      "Epoch 14/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4424 - val_loss: 0.3607\n",
      "Epoch 15/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4542 - val_loss: 0.3411\n",
      "Epoch 16/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4077 - val_loss: 0.3198\n",
      "Epoch 17/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3941 - val_loss: 0.3051\n",
      "Epoch 18/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3598 - val_loss: 0.2813\n",
      "Epoch 19/600\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3571 - val_loss: 0.2642\n",
      "Epoch 20/600\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3413 - val_loss: 0.2560\n",
      "Epoch 21/600\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3194 - val_loss: 0.2392\n",
      "Epoch 22/600\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3135 - val_loss: 0.2331\n",
      "Epoch 23/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3216 - val_loss: 0.2170\n",
      "Epoch 24/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3132 - val_loss: 0.2115\n",
      "Epoch 25/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3064 - val_loss: 0.2024\n",
      "Epoch 26/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3044 - val_loss: 0.1864\n",
      "Epoch 27/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2794 - val_loss: 0.1797\n",
      "Epoch 28/600\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.2679 - val_loss: 0.1732\n",
      "Epoch 29/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2797 - val_loss: 0.1674\n",
      "Epoch 30/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2590 - val_loss: 0.1686\n",
      "Epoch 31/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2219 - val_loss: 0.1577\n",
      "Epoch 32/600\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.2286 - val_loss: 0.1487\n",
      "Epoch 33/600\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.2423 - val_loss: 0.1530\n",
      "Epoch 34/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2178 - val_loss: 0.1404\n",
      "Epoch 35/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1902 - val_loss: 0.1354\n",
      "Epoch 36/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2195 - val_loss: 0.1285\n",
      "Epoch 37/600\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.2324 - val_loss: 0.1308\n",
      "Epoch 38/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2105 - val_loss: 0.1272\n",
      "Epoch 39/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2059 - val_loss: 0.1249\n",
      "Epoch 40/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2073 - val_loss: 0.1347\n",
      "Epoch 41/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1949 - val_loss: 0.1270\n",
      "Epoch 42/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1782 - val_loss: 0.1178\n",
      "Epoch 43/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1837 - val_loss: 0.1195\n",
      "Epoch 44/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1795 - val_loss: 0.1164\n",
      "Epoch 45/600\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.1741 - val_loss: 0.1145\n",
      "Epoch 46/600\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.1935 - val_loss: 0.1059\n",
      "Epoch 47/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1867 - val_loss: 0.1064\n",
      "Epoch 48/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1775 - val_loss: 0.1078\n",
      "Epoch 49/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1497 - val_loss: 0.1122\n",
      "Epoch 50/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1696 - val_loss: 0.1098\n",
      "Epoch 51/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1442 - val_loss: 0.1081\n",
      "Epoch 52/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1404 - val_loss: 0.0999\n",
      "Epoch 53/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1743 - val_loss: 0.0984\n",
      "Epoch 54/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1569 - val_loss: 0.1010\n",
      "Epoch 55/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1586 - val_loss: 0.0996\n",
      "Epoch 56/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1472 - val_loss: 0.1064\n",
      "Epoch 57/600\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.1289 - val_loss: 0.0972\n",
      "Epoch 58/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1317 - val_loss: 0.0944\n",
      "Epoch 59/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1422 - val_loss: 0.1072\n",
      "Epoch 60/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1314 - val_loss: 0.1083\n",
      "Epoch 61/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1629 - val_loss: 0.0916\n",
      "Epoch 62/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1435 - val_loss: 0.0933\n",
      "Epoch 63/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1392 - val_loss: 0.0923\n",
      "Epoch 64/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1517 - val_loss: 0.1000\n",
      "Epoch 65/600\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.1270 - val_loss: 0.0922\n",
      "Epoch 66/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1257 - val_loss: 0.0955\n",
      "Epoch 67/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1354 - val_loss: 0.0952\n",
      "Epoch 68/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1475 - val_loss: 0.0995\n",
      "Epoch 69/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1263 - val_loss: 0.1005\n",
      "Epoch 70/600\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.1105 - val_loss: 0.0925\n",
      "Epoch 71/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1027 - val_loss: 0.0899\n",
      "Epoch 72/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1233 - val_loss: 0.0875\n",
      "Epoch 73/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1142 - val_loss: 0.0895\n",
      "Epoch 74/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1145 - val_loss: 0.1062\n",
      "Epoch 75/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1187 - val_loss: 0.0901\n",
      "Epoch 76/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1172 - val_loss: 0.0872\n",
      "Epoch 77/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1093 - val_loss: 0.0857\n",
      "Epoch 78/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1267 - val_loss: 0.0874\n",
      "Epoch 79/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1257 - val_loss: 0.0943\n",
      "Epoch 80/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1305 - val_loss: 0.1017\n",
      "Epoch 81/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1134 - val_loss: 0.0941\n",
      "Epoch 82/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1174 - val_loss: 0.0876\n",
      "Epoch 83/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1207 - val_loss: 0.0955\n",
      "Epoch 84/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1022 - val_loss: 0.0887\n",
      "Epoch 85/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0956 - val_loss: 0.0825\n",
      "Epoch 86/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1149 - val_loss: 0.0867\n",
      "Epoch 87/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0978 - val_loss: 0.0888\n",
      "Epoch 88/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1122 - val_loss: 0.0951\n",
      "Epoch 89/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0969 - val_loss: 0.1008\n",
      "Epoch 90/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1131 - val_loss: 0.0807\n",
      "Epoch 91/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1114 - val_loss: 0.0945\n",
      "Epoch 92/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1201 - val_loss: 0.0994\n",
      "Epoch 93/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1037 - val_loss: 0.0825\n",
      "Epoch 94/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1196 - val_loss: 0.0850\n",
      "Epoch 95/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0991 - val_loss: 0.0987\n",
      "Epoch 96/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1041 - val_loss: 0.0824\n",
      "Epoch 97/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0948 - val_loss: 0.0852\n",
      "Epoch 98/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1167 - val_loss: 0.0856\n",
      "Epoch 99/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1199 - val_loss: 0.0893\n",
      "Epoch 100/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1218 - val_loss: 0.0846\n",
      "Epoch 101/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1052 - val_loss: 0.0830\n",
      "Epoch 102/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0961 - val_loss: 0.0820\n",
      "Epoch 103/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1055 - val_loss: 0.1019\n",
      "Epoch 104/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1150 - val_loss: 0.0810\n",
      "Epoch 105/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0894 - val_loss: 0.0809\n",
      "Epoch 106/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0989 - val_loss: 0.0908\n",
      "Epoch 107/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0868 - val_loss: 0.0835\n",
      "Epoch 108/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0786 - val_loss: 0.0805\n",
      "Epoch 109/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1034 - val_loss: 0.0834\n",
      "Epoch 110/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1045 - val_loss: 0.0877\n",
      "Epoch 111/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1141 - val_loss: 0.0925\n",
      "Epoch 112/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0801 - val_loss: 0.0845\n",
      "Epoch 113/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1070 - val_loss: 0.0910\n",
      "Epoch 114/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0961 - val_loss: 0.0857\n",
      "Epoch 115/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1105 - val_loss: 0.0893\n",
      "Epoch 116/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1106 - val_loss: 0.1036\n",
      "Epoch 117/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1049 - val_loss: 0.0826\n",
      "Epoch 118/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0848 - val_loss: 0.0897\n",
      "Epoch 119/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0844 - val_loss: 0.0914\n",
      "Epoch 120/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0876 - val_loss: 0.0973\n",
      "Epoch 121/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1137 - val_loss: 0.0916\n",
      "Epoch 122/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1014 - val_loss: 0.0836\n",
      "Epoch 123/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0826 - val_loss: 0.0845\n",
      "Epoch 124/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0797 - val_loss: 0.0969\n",
      "Epoch 125/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0791 - val_loss: 0.0854\n",
      "Epoch 126/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0748 - val_loss: 0.0895\n",
      "Epoch 127/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0856 - val_loss: 0.0825\n",
      "Epoch 128/600\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.1002 - val_loss: 0.0834\n",
      "Epoch 129/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0852 - val_loss: 0.0939\n",
      "Epoch 130/600\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.1006 - val_loss: 0.0874\n",
      "Epoch 131/600\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0736 - val_loss: 0.0797\n",
      "Epoch 132/600\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0791 - val_loss: 0.0868\n",
      "Epoch 133/600\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0825 - val_loss: 0.0821\n",
      "Epoch 134/600\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0924 - val_loss: 0.0939\n",
      "Epoch 135/600\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0884 - val_loss: 0.1047\n",
      "Epoch 136/600\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0849 - val_loss: 0.0870\n",
      "Epoch 137/600\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0723 - val_loss: 0.0881\n",
      "Epoch 138/600\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0805 - val_loss: 0.0902\n",
      "Epoch 139/600\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0853 - val_loss: 0.1058\n",
      "Epoch 140/600\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0758 - val_loss: 0.0882\n",
      "Epoch 141/600\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0923 - val_loss: 0.0821\n",
      "Epoch 142/600\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.1007 - val_loss: 0.0813\n",
      "Epoch 143/600\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0839 - val_loss: 0.0950\n",
      "Epoch 144/600\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0774 - val_loss: 0.0877\n",
      "Epoch 145/600\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0776 - val_loss: 0.0828\n",
      "Epoch 146/600\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0619 - val_loss: 0.0939\n",
      "Epoch 147/600\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0706 - val_loss: 0.0949\n",
      "Epoch 148/600\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0794 - val_loss: 0.0918\n",
      "Epoch 149/600\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0753 - val_loss: 0.0929\n",
      "Epoch 150/600\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0781 - val_loss: 0.1047\n",
      "Epoch 151/600\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0749 - val_loss: 0.0940\n",
      "Epoch 152/600\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.1037 - val_loss: 0.0904\n",
      "Epoch 153/600\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0581 - val_loss: 0.0931\n",
      "Epoch 154/600\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0709 - val_loss: 0.1052\n",
      "Epoch 155/600\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0817 - val_loss: 0.0962\n",
      "Epoch 156/600\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0816 - val_loss: 0.1158\n",
      "Epoch 156: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x18822360580>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=X_train, \n",
    "          y=y_train, \n",
    "          epochs=600,\n",
    "          validation_data=(X_test, y_test), verbose=1,\n",
    "          callbacks=[early_stop]\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_loss = pd.DataFrame(model.history.history)\n",
    "model_loss.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.94212806e-01],\n",
       "       [9.86022830e-01],\n",
       "       [9.96451795e-01],\n",
       "       [3.97229195e-03],\n",
       "       [9.99595940e-01],\n",
       "       [9.99817610e-01],\n",
       "       [9.99729931e-01],\n",
       "       [1.80557450e-07],\n",
       "       [9.98340964e-01],\n",
       "       [9.98359680e-01],\n",
       "       [3.47912312e-04],\n",
       "       [9.94536817e-01],\n",
       "       [9.04713154e-01],\n",
       "       [9.99502003e-01],\n",
       "       [2.02596188e-04],\n",
       "       [9.98920202e-01],\n",
       "       [9.96129811e-01],\n",
       "       [9.99574959e-01],\n",
       "       [1.68503357e-06],\n",
       "       [4.69611585e-02],\n",
       "       [9.58807707e-01],\n",
       "       [9.97897267e-01],\n",
       "       [9.87447262e-01],\n",
       "       [9.99604702e-01],\n",
       "       [1.18839741e-03],\n",
       "       [9.30203557e-01],\n",
       "       [1.20411416e-04],\n",
       "       [9.99991298e-01],\n",
       "       [1.21911100e-04],\n",
       "       [1.77752972e-03],\n",
       "       [9.93224084e-02],\n",
       "       [4.75555658e-04],\n",
       "       [5.63483596e-01],\n",
       "       [9.76471603e-02],\n",
       "       [9.99822497e-01],\n",
       "       [9.99838471e-01],\n",
       "       [1.08426213e-02],\n",
       "       [2.92296708e-02],\n",
       "       [7.41904005e-05],\n",
       "       [1.69336056e-06],\n",
       "       [2.16439366e-03],\n",
       "       [9.89559650e-01],\n",
       "       [9.99472737e-01],\n",
       "       [7.94033945e-01],\n",
       "       [9.99975443e-01],\n",
       "       [9.96571064e-01],\n",
       "       [8.53957172e-05],\n",
       "       [5.44713914e-01],\n",
       "       [1.41409934e-02],\n",
       "       [9.98881221e-01],\n",
       "       [1.45584345e-04],\n",
       "       [9.02524948e-01],\n",
       "       [9.89546061e-01],\n",
       "       [1.05403268e-08],\n",
       "       [9.99458671e-01],\n",
       "       [9.99424815e-01],\n",
       "       [6.95153039e-06],\n",
       "       [2.15402588e-06],\n",
       "       [9.92457449e-01],\n",
       "       [9.99015689e-01],\n",
       "       [3.41475010e-04],\n",
       "       [6.53900802e-02],\n",
       "       [9.73774552e-01],\n",
       "       [9.99109030e-01],\n",
       "       [1.80859456e-06],\n",
       "       [7.95527101e-01],\n",
       "       [9.04881239e-01],\n",
       "       [2.23333743e-08],\n",
       "       [2.26140022e-04],\n",
       "       [9.93039608e-01],\n",
       "       [5.11031576e-05],\n",
       "       [9.99863386e-01],\n",
       "       [9.99949574e-01],\n",
       "       [9.99434948e-01],\n",
       "       [2.15828419e-04],\n",
       "       [1.95022949e-07],\n",
       "       [9.95645821e-01],\n",
       "       [9.97928917e-01],\n",
       "       [4.64322784e-06],\n",
       "       [2.08053321e-01],\n",
       "       [6.07566833e-01],\n",
       "       [9.51755762e-01],\n",
       "       [9.99816597e-01],\n",
       "       [9.98270392e-01],\n",
       "       [9.89735365e-01],\n",
       "       [9.99995828e-01],\n",
       "       [3.95172834e-03],\n",
       "       [9.95962739e-01],\n",
       "       [7.09878150e-05],\n",
       "       [1.08658075e-01],\n",
       "       [9.94908333e-01],\n",
       "       [1.61160231e-02],\n",
       "       [2.48289108e-03],\n",
       "       [6.50328219e-01],\n",
       "       [9.99899030e-01],\n",
       "       [9.99855757e-01],\n",
       "       [9.93935347e-01],\n",
       "       [9.03121352e-01],\n",
       "       [7.05599785e-04],\n",
       "       [1.53368711e-03],\n",
       "       [1.86723471e-03],\n",
       "       [9.99956965e-01],\n",
       "       [9.99068677e-01],\n",
       "       [9.99998868e-01],\n",
       "       [9.76743698e-01],\n",
       "       [9.92731929e-01],\n",
       "       [9.95132864e-01],\n",
       "       [9.83409047e-01],\n",
       "       [9.95385408e-01],\n",
       "       [9.44462657e-01],\n",
       "       [9.99982357e-01],\n",
       "       [9.99870360e-01],\n",
       "       [8.70934725e-01],\n",
       "       [1.00177526e-03],\n",
       "       [7.63909459e-01],\n",
       "       [9.99674439e-01],\n",
       "       [9.96709347e-01],\n",
       "       [1.03187740e-01],\n",
       "       [6.42630458e-03],\n",
       "       [8.59962802e-06],\n",
       "       [8.77050440e-15],\n",
       "       [2.17813595e-05],\n",
       "       [2.46024811e-05],\n",
       "       [9.99531746e-01],\n",
       "       [2.75933832e-01],\n",
       "       [9.99978781e-01],\n",
       "       [9.99803782e-01],\n",
       "       [9.99678373e-01],\n",
       "       [9.88467813e-01],\n",
       "       [9.98665214e-01],\n",
       "       [1.01790263e-07],\n",
       "       [4.06974323e-07],\n",
       "       [5.00849664e-01],\n",
       "       [9.80997205e-01],\n",
       "       [9.99759555e-01],\n",
       "       [9.91975069e-01],\n",
       "       [4.44531441e-04],\n",
       "       [1.56605244e-03],\n",
       "       [1.10483452e-05],\n",
       "       [9.99997258e-01],\n",
       "       [8.18371773e-04],\n",
       "       [8.95487666e-01],\n",
       "       [2.33500873e-06]], dtype=float32)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_class(x):\n",
    "    if x>= 0.5:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.DataFrame(predictions)[0].apply(assign_class).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.98      0.96        55\n",
      "           1       0.99      0.97      0.98        88\n",
      "\n",
      "    accuracy                           0.97       143\n",
      "   macro avg       0.97      0.97      0.97       143\n",
      "weighted avg       0.97      0.97      0.97       143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://en.wikipedia.org/wiki/Precision_and_recall\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[54  1]\n",
      " [ 3 85]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test,predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
