{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IEOR 4650 - Business Analytics\n",
    "\n",
    "## Assignment 3\n",
    "\n",
    "### Team 29: Yufei Jin (yj2691) & Pierre Counathe (pc2977)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(ggplot2)\n",
    "library(glmnet)\n",
    "library(MASS)\n",
    "library(caret)\n",
    "library(class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 6 × 14</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>X</th><th scope=col>Purchase</th><th scope=col>WeekofPurchase</th><th scope=col>StoreID</th><th scope=col>PriceCH</th><th scope=col>PriceMM</th><th scope=col>DiscCH</th><th scope=col>DiscMM</th><th scope=col>SpecialCH</th><th scope=col>SpecialMM</th><th scope=col>LoyalCH</th><th scope=col>SalePriceMM</th><th scope=col>SalePriceCH</th><th scope=col>PriceDiff</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>1</td><td>CH</td><td>237</td><td>1</td><td>1.75</td><td>1.99</td><td>0.00</td><td>0.0</td><td>0</td><td>0</td><td>0.500000</td><td>1.99</td><td>1.75</td><td> 0.24</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>2</td><td>CH</td><td>239</td><td>1</td><td>1.75</td><td>1.99</td><td>0.00</td><td>0.3</td><td>0</td><td>1</td><td>0.600000</td><td>1.69</td><td>1.75</td><td>-0.06</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>3</td><td>CH</td><td>245</td><td>1</td><td>1.86</td><td>2.09</td><td>0.17</td><td>0.0</td><td>0</td><td>0</td><td>0.680000</td><td>2.09</td><td>1.69</td><td> 0.40</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>4</td><td>MM</td><td>227</td><td>1</td><td>1.69</td><td>1.69</td><td>0.00</td><td>0.0</td><td>0</td><td>0</td><td>0.400000</td><td>1.69</td><td>1.69</td><td> 0.00</td></tr>\n",
       "\t<tr><th scope=row>5</th><td>5</td><td>CH</td><td>228</td><td>7</td><td>1.69</td><td>1.69</td><td>0.00</td><td>0.0</td><td>0</td><td>0</td><td>0.956535</td><td>1.69</td><td>1.69</td><td> 0.00</td></tr>\n",
       "\t<tr><th scope=row>6</th><td>6</td><td>CH</td><td>230</td><td>7</td><td>1.69</td><td>1.99</td><td>0.00</td><td>0.0</td><td>0</td><td>1</td><td>0.965228</td><td>1.99</td><td>1.69</td><td> 0.30</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 6 × 14\n",
       "\\begin{tabular}{r|llllllllllllll}\n",
       "  & X & Purchase & WeekofPurchase & StoreID & PriceCH & PriceMM & DiscCH & DiscMM & SpecialCH & SpecialMM & LoyalCH & SalePriceMM & SalePriceCH & PriceDiff\\\\\n",
       "  & <int> & <chr> & <int> & <int> & <dbl> & <dbl> & <dbl> & <dbl> & <int> & <int> & <dbl> & <dbl> & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t1 & 1 & CH & 237 & 1 & 1.75 & 1.99 & 0.00 & 0.0 & 0 & 0 & 0.500000 & 1.99 & 1.75 &  0.24\\\\\n",
       "\t2 & 2 & CH & 239 & 1 & 1.75 & 1.99 & 0.00 & 0.3 & 0 & 1 & 0.600000 & 1.69 & 1.75 & -0.06\\\\\n",
       "\t3 & 3 & CH & 245 & 1 & 1.86 & 2.09 & 0.17 & 0.0 & 0 & 0 & 0.680000 & 2.09 & 1.69 &  0.40\\\\\n",
       "\t4 & 4 & MM & 227 & 1 & 1.69 & 1.69 & 0.00 & 0.0 & 0 & 0 & 0.400000 & 1.69 & 1.69 &  0.00\\\\\n",
       "\t5 & 5 & CH & 228 & 7 & 1.69 & 1.69 & 0.00 & 0.0 & 0 & 0 & 0.956535 & 1.69 & 1.69 &  0.00\\\\\n",
       "\t6 & 6 & CH & 230 & 7 & 1.69 & 1.99 & 0.00 & 0.0 & 0 & 1 & 0.965228 & 1.99 & 1.69 &  0.30\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 6 × 14\n",
       "\n",
       "| <!--/--> | X &lt;int&gt; | Purchase &lt;chr&gt; | WeekofPurchase &lt;int&gt; | StoreID &lt;int&gt; | PriceCH &lt;dbl&gt; | PriceMM &lt;dbl&gt; | DiscCH &lt;dbl&gt; | DiscMM &lt;dbl&gt; | SpecialCH &lt;int&gt; | SpecialMM &lt;int&gt; | LoyalCH &lt;dbl&gt; | SalePriceMM &lt;dbl&gt; | SalePriceCH &lt;dbl&gt; | PriceDiff &lt;dbl&gt; |\n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| 1 | 1 | CH | 237 | 1 | 1.75 | 1.99 | 0.00 | 0.0 | 0 | 0 | 0.500000 | 1.99 | 1.75 |  0.24 |\n",
       "| 2 | 2 | CH | 239 | 1 | 1.75 | 1.99 | 0.00 | 0.3 | 0 | 1 | 0.600000 | 1.69 | 1.75 | -0.06 |\n",
       "| 3 | 3 | CH | 245 | 1 | 1.86 | 2.09 | 0.17 | 0.0 | 0 | 0 | 0.680000 | 2.09 | 1.69 |  0.40 |\n",
       "| 4 | 4 | MM | 227 | 1 | 1.69 | 1.69 | 0.00 | 0.0 | 0 | 0 | 0.400000 | 1.69 | 1.69 |  0.00 |\n",
       "| 5 | 5 | CH | 228 | 7 | 1.69 | 1.69 | 0.00 | 0.0 | 0 | 0 | 0.956535 | 1.69 | 1.69 |  0.00 |\n",
       "| 6 | 6 | CH | 230 | 7 | 1.69 | 1.99 | 0.00 | 0.0 | 0 | 1 | 0.965228 | 1.99 | 1.69 |  0.30 |\n",
       "\n"
      ],
      "text/plain": [
       "  X Purchase WeekofPurchase StoreID PriceCH PriceMM DiscCH DiscMM SpecialCH\n",
       "1 1 CH       237            1       1.75    1.99    0.00   0.0    0        \n",
       "2 2 CH       239            1       1.75    1.99    0.00   0.3    0        \n",
       "3 3 CH       245            1       1.86    2.09    0.17   0.0    0        \n",
       "4 4 MM       227            1       1.69    1.69    0.00   0.0    0        \n",
       "5 5 CH       228            7       1.69    1.69    0.00   0.0    0        \n",
       "6 6 CH       230            7       1.69    1.99    0.00   0.0    0        \n",
       "  SpecialMM LoyalCH  SalePriceMM SalePriceCH PriceDiff\n",
       "1 0         0.500000 1.99        1.75         0.24    \n",
       "2 1         0.600000 1.69        1.75        -0.06    \n",
       "3 0         0.680000 2.09        1.69         0.40    \n",
       "4 0         0.400000 1.69        1.69         0.00    \n",
       "5 0         0.956535 1.69        1.69         0.00    \n",
       "6 1         0.965228 1.99        1.69         0.30    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0"
      ],
      "text/latex": [
       "0"
      ],
      "text/markdown": [
       "0"
      ],
      "text/plain": [
       "[1] 0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import data and set seed for repeatability\n",
    "set.seed(4650)\n",
    "orange_data <- read.csv(\"OrangeJuice.csv\")\n",
    "head(orange_data)\n",
    "sum(is.na(orange_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "split <- sample(c(rep(0, 0.5 * nrow(orange_data)), \n",
    "                  rep(1, 0.25 * nrow(orange_data)), \n",
    "                  rep(2, 0.25 * nrow(orange_data))))\n",
    "\n",
    "train_set <- orange_data[split == 0, ]\n",
    "validation_set <- orange_data[split == 1, ]\n",
    "test_set <- orange_data[split == 2, ]     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       X            Purchase         WeekofPurchase     StoreID     \n",
       " Min.   :   1.0   Length:536         Min.   :227.0   Min.   :1.000  \n",
       " 1st Qu.: 278.8   Class :character   1st Qu.:239.0   1st Qu.:2.000  \n",
       " Median : 549.5   Mode  :character   Median :256.5   Median :3.000  \n",
       " Mean   : 547.1                      Mean   :254.0   Mean   :4.013  \n",
       " 3rd Qu.: 810.2                      3rd Qu.:267.0   3rd Qu.:7.000  \n",
       " Max.   :1070.0                      Max.   :278.0   Max.   :7.000  \n",
       "    PriceCH         PriceMM          DiscCH            DiscMM     \n",
       " Min.   :1.690   Min.   :1.690   Min.   :0.00000   Min.   :0.000  \n",
       " 1st Qu.:1.790   1st Qu.:1.990   1st Qu.:0.00000   1st Qu.:0.000  \n",
       " Median :1.860   Median :2.090   Median :0.00000   Median :0.000  \n",
       " Mean   :1.865   Mean   :2.079   Mean   :0.05401   Mean   :0.138  \n",
       " 3rd Qu.:1.990   3rd Qu.:2.180   3rd Qu.:0.00000   3rd Qu.:0.300  \n",
       " Max.   :2.090   Max.   :2.290   Max.   :0.50000   Max.   :0.800  \n",
       "   SpecialCH        SpecialMM         LoyalCH          SalePriceMM   \n",
       " Min.   :0.0000   Min.   :0.0000   Min.   :0.000014   Min.   :1.190  \n",
       " 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.320000   1st Qu.:1.690  \n",
       " Median :0.0000   Median :0.0000   Median :0.584000   Median :2.090  \n",
       " Mean   :0.1549   Mean   :0.1679   Mean   :0.553022   Mean   :1.941  \n",
       " 3rd Qu.:0.0000   3rd Qu.:0.0000   3rd Qu.:0.836160   3rd Qu.:2.130  \n",
       " Max.   :1.0000   Max.   :1.0000   Max.   :0.999947   Max.   :2.290  \n",
       "  SalePriceCH      PriceDiff      \n",
       " Min.   :1.390   Min.   :-0.6700  \n",
       " 1st Qu.:1.750   1st Qu.: 0.0000  \n",
       " Median :1.860   Median : 0.2000  \n",
       " Mean   :1.811   Mean   : 0.1304  \n",
       " 3rd Qu.:1.890   3rd Qu.: 0.3000  \n",
       " Max.   :2.090   Max.   : 0.6400  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summary(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Covariance analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A matrix: 12 × 12 of type dbl</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>WeekofPurchase</th><th scope=col>StoreID</th><th scope=col>PriceCH</th><th scope=col>PriceMM</th><th scope=col>DiscCH</th><th scope=col>DiscMM</th><th scope=col>SpecialCH</th><th scope=col>SpecialMM</th><th scope=col>LoyalCH</th><th scope=col>SalePriceMM</th><th scope=col>SalePriceCH</th><th scope=col>PriceDiff</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>WeekofPurchase</th><td>1.00000000</td><td> 0.075895145</td><td> 0.72772771</td><td> 0.62056969</td><td> 0.37019101</td><td> 0.217898836</td><td> 0.05430032</td><td> 0.06150050</td><td> 0.15856567</td><td> 0.13556579</td><td> 0.20825004</td><td> 0.01828848</td></tr>\n",
       "\t<tr><th scope=row>StoreID</th><td>0.07589514</td><td> 1.000000000</td><td>-0.01682954</td><td> 0.06675979</td><td> 0.32000072</td><td> 0.008418251</td><td> 0.37227842</td><td>-0.15608507</td><td> 0.23401598</td><td> 0.02753165</td><td>-0.27442898</td><td> 0.16907171</td></tr>\n",
       "\t<tr><th scope=row>PriceCH</th><td>0.72772771</td><td>-0.016829542</td><td> 1.00000000</td><td> 0.62721464</td><td> 0.14583830</td><td> 0.093074190</td><td>-0.14024001</td><td>-0.02655727</td><td> 0.03390598</td><td> 0.24661025</td><td> 0.58391912</td><td>-0.07380543</td></tr>\n",
       "\t<tr><th scope=row>PriceMM</th><td>0.62056969</td><td> 0.066759788</td><td> 0.62721464</td><td> 1.00000000</td><td> 0.07160843</td><td> 0.015956885</td><td>-0.06149618</td><td>-0.04711281</td><td> 0.06650592</td><td> 0.50732508</td><td> 0.38254230</td><td> 0.27559971</td></tr>\n",
       "\t<tr><th scope=row>DiscCH</th><td>0.37019101</td><td> 0.320000724</td><td> 0.14583830</td><td> 0.07160843</td><td> 1.00000000</td><td> 0.015316581</td><td> 0.53220322</td><td>-0.05682903</td><td> 0.13473325</td><td> 0.02411274</td><td>-0.71797454</td><td> 0.39743951</td></tr>\n",
       "\t<tr><th scope=row>DiscMM</th><td>0.21789884</td><td> 0.008418251</td><td> 0.09307419</td><td> 0.01595688</td><td> 0.01531658</td><td> 1.000000000</td><td> 0.17288920</td><td> 0.50647039</td><td>-0.02887638</td><td>-0.85354971</td><td> 0.05291768</td><td>-0.82733311</td></tr>\n",
       "\t<tr><th scope=row>SpecialCH</th><td>0.05430032</td><td> 0.372278420</td><td>-0.14024001</td><td>-0.06149618</td><td> 0.53220322</td><td> 0.172889199</td><td> 1.00000000</td><td>-0.15089296</td><td> 0.12337308</td><td>-0.18105135</td><td>-0.53538980</td><td> 0.10989197</td></tr>\n",
       "\t<tr><th scope=row>SpecialMM</th><td>0.06150050</td><td>-0.156085071</td><td>-0.02655727</td><td>-0.04711281</td><td>-0.05682903</td><td> 0.506470389</td><td>-0.15089296</td><td> 1.00000000</td><td>-0.09860898</td><td>-0.46105828</td><td> 0.02794758</td><td>-0.44656453</td></tr>\n",
       "\t<tr><th scope=row>LoyalCH</th><td>0.15856567</td><td> 0.234015985</td><td> 0.03390598</td><td> 0.06650592</td><td> 0.13473325</td><td>-0.028876378</td><td> 0.12337308</td><td>-0.09860898</td><td> 1.00000000</td><td> 0.05954228</td><td>-0.08670414</td><td> 0.10105361</td></tr>\n",
       "\t<tr><th scope=row>SalePriceMM</th><td>0.13556579</td><td> 0.027531649</td><td> 0.24661025</td><td> 0.50732508</td><td> 0.02411274</td><td>-0.853549715</td><td>-0.18105135</td><td>-0.46105828</td><td> 0.05954228</td><td> 1.00000000</td><td> 0.15372641</td><td> 0.85665791</td></tr>\n",
       "\t<tr><th scope=row>SalePriceCH</th><td>0.20825004</td><td>-0.274428979</td><td> 0.58391912</td><td> 0.38254230</td><td>-0.71797454</td><td> 0.052917684</td><td>-0.53538980</td><td> 0.02794758</td><td>-0.08670414</td><td> 0.15372641</td><td> 1.00000000</td><td>-0.37806187</td></tr>\n",
       "\t<tr><th scope=row>PriceDiff</th><td>0.01828848</td><td> 0.169071705</td><td>-0.07380543</td><td> 0.27559971</td><td> 0.39743951</td><td>-0.827333108</td><td> 0.10989197</td><td>-0.44656453</td><td> 0.10105361</td><td> 0.85665791</td><td>-0.37806187</td><td> 1.00000000</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A matrix: 12 × 12 of type dbl\n",
       "\\begin{tabular}{r|llllllllllll}\n",
       "  & WeekofPurchase & StoreID & PriceCH & PriceMM & DiscCH & DiscMM & SpecialCH & SpecialMM & LoyalCH & SalePriceMM & SalePriceCH & PriceDiff\\\\\n",
       "\\hline\n",
       "\tWeekofPurchase & 1.00000000 &  0.075895145 &  0.72772771 &  0.62056969 &  0.37019101 &  0.217898836 &  0.05430032 &  0.06150050 &  0.15856567 &  0.13556579 &  0.20825004 &  0.01828848\\\\\n",
       "\tStoreID & 0.07589514 &  1.000000000 & -0.01682954 &  0.06675979 &  0.32000072 &  0.008418251 &  0.37227842 & -0.15608507 &  0.23401598 &  0.02753165 & -0.27442898 &  0.16907171\\\\\n",
       "\tPriceCH & 0.72772771 & -0.016829542 &  1.00000000 &  0.62721464 &  0.14583830 &  0.093074190 & -0.14024001 & -0.02655727 &  0.03390598 &  0.24661025 &  0.58391912 & -0.07380543\\\\\n",
       "\tPriceMM & 0.62056969 &  0.066759788 &  0.62721464 &  1.00000000 &  0.07160843 &  0.015956885 & -0.06149618 & -0.04711281 &  0.06650592 &  0.50732508 &  0.38254230 &  0.27559971\\\\\n",
       "\tDiscCH & 0.37019101 &  0.320000724 &  0.14583830 &  0.07160843 &  1.00000000 &  0.015316581 &  0.53220322 & -0.05682903 &  0.13473325 &  0.02411274 & -0.71797454 &  0.39743951\\\\\n",
       "\tDiscMM & 0.21789884 &  0.008418251 &  0.09307419 &  0.01595688 &  0.01531658 &  1.000000000 &  0.17288920 &  0.50647039 & -0.02887638 & -0.85354971 &  0.05291768 & -0.82733311\\\\\n",
       "\tSpecialCH & 0.05430032 &  0.372278420 & -0.14024001 & -0.06149618 &  0.53220322 &  0.172889199 &  1.00000000 & -0.15089296 &  0.12337308 & -0.18105135 & -0.53538980 &  0.10989197\\\\\n",
       "\tSpecialMM & 0.06150050 & -0.156085071 & -0.02655727 & -0.04711281 & -0.05682903 &  0.506470389 & -0.15089296 &  1.00000000 & -0.09860898 & -0.46105828 &  0.02794758 & -0.44656453\\\\\n",
       "\tLoyalCH & 0.15856567 &  0.234015985 &  0.03390598 &  0.06650592 &  0.13473325 & -0.028876378 &  0.12337308 & -0.09860898 &  1.00000000 &  0.05954228 & -0.08670414 &  0.10105361\\\\\n",
       "\tSalePriceMM & 0.13556579 &  0.027531649 &  0.24661025 &  0.50732508 &  0.02411274 & -0.853549715 & -0.18105135 & -0.46105828 &  0.05954228 &  1.00000000 &  0.15372641 &  0.85665791\\\\\n",
       "\tSalePriceCH & 0.20825004 & -0.274428979 &  0.58391912 &  0.38254230 & -0.71797454 &  0.052917684 & -0.53538980 &  0.02794758 & -0.08670414 &  0.15372641 &  1.00000000 & -0.37806187\\\\\n",
       "\tPriceDiff & 0.01828848 &  0.169071705 & -0.07380543 &  0.27559971 &  0.39743951 & -0.827333108 &  0.10989197 & -0.44656453 &  0.10105361 &  0.85665791 & -0.37806187 &  1.00000000\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A matrix: 12 × 12 of type dbl\n",
       "\n",
       "| <!--/--> | WeekofPurchase | StoreID | PriceCH | PriceMM | DiscCH | DiscMM | SpecialCH | SpecialMM | LoyalCH | SalePriceMM | SalePriceCH | PriceDiff |\n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| WeekofPurchase | 1.00000000 |  0.075895145 |  0.72772771 |  0.62056969 |  0.37019101 |  0.217898836 |  0.05430032 |  0.06150050 |  0.15856567 |  0.13556579 |  0.20825004 |  0.01828848 |\n",
       "| StoreID | 0.07589514 |  1.000000000 | -0.01682954 |  0.06675979 |  0.32000072 |  0.008418251 |  0.37227842 | -0.15608507 |  0.23401598 |  0.02753165 | -0.27442898 |  0.16907171 |\n",
       "| PriceCH | 0.72772771 | -0.016829542 |  1.00000000 |  0.62721464 |  0.14583830 |  0.093074190 | -0.14024001 | -0.02655727 |  0.03390598 |  0.24661025 |  0.58391912 | -0.07380543 |\n",
       "| PriceMM | 0.62056969 |  0.066759788 |  0.62721464 |  1.00000000 |  0.07160843 |  0.015956885 | -0.06149618 | -0.04711281 |  0.06650592 |  0.50732508 |  0.38254230 |  0.27559971 |\n",
       "| DiscCH | 0.37019101 |  0.320000724 |  0.14583830 |  0.07160843 |  1.00000000 |  0.015316581 |  0.53220322 | -0.05682903 |  0.13473325 |  0.02411274 | -0.71797454 |  0.39743951 |\n",
       "| DiscMM | 0.21789884 |  0.008418251 |  0.09307419 |  0.01595688 |  0.01531658 |  1.000000000 |  0.17288920 |  0.50647039 | -0.02887638 | -0.85354971 |  0.05291768 | -0.82733311 |\n",
       "| SpecialCH | 0.05430032 |  0.372278420 | -0.14024001 | -0.06149618 |  0.53220322 |  0.172889199 |  1.00000000 | -0.15089296 |  0.12337308 | -0.18105135 | -0.53538980 |  0.10989197 |\n",
       "| SpecialMM | 0.06150050 | -0.156085071 | -0.02655727 | -0.04711281 | -0.05682903 |  0.506470389 | -0.15089296 |  1.00000000 | -0.09860898 | -0.46105828 |  0.02794758 | -0.44656453 |\n",
       "| LoyalCH | 0.15856567 |  0.234015985 |  0.03390598 |  0.06650592 |  0.13473325 | -0.028876378 |  0.12337308 | -0.09860898 |  1.00000000 |  0.05954228 | -0.08670414 |  0.10105361 |\n",
       "| SalePriceMM | 0.13556579 |  0.027531649 |  0.24661025 |  0.50732508 |  0.02411274 | -0.853549715 | -0.18105135 | -0.46105828 |  0.05954228 |  1.00000000 |  0.15372641 |  0.85665791 |\n",
       "| SalePriceCH | 0.20825004 | -0.274428979 |  0.58391912 |  0.38254230 | -0.71797454 |  0.052917684 | -0.53538980 |  0.02794758 | -0.08670414 |  0.15372641 |  1.00000000 | -0.37806187 |\n",
       "| PriceDiff | 0.01828848 |  0.169071705 | -0.07380543 |  0.27559971 |  0.39743951 | -0.827333108 |  0.10989197 | -0.44656453 |  0.10105361 |  0.85665791 | -0.37806187 |  1.00000000 |\n",
       "\n"
      ],
      "text/plain": [
       "               WeekofPurchase StoreID      PriceCH     PriceMM     DiscCH     \n",
       "WeekofPurchase 1.00000000      0.075895145  0.72772771  0.62056969  0.37019101\n",
       "StoreID        0.07589514      1.000000000 -0.01682954  0.06675979  0.32000072\n",
       "PriceCH        0.72772771     -0.016829542  1.00000000  0.62721464  0.14583830\n",
       "PriceMM        0.62056969      0.066759788  0.62721464  1.00000000  0.07160843\n",
       "DiscCH         0.37019101      0.320000724  0.14583830  0.07160843  1.00000000\n",
       "DiscMM         0.21789884      0.008418251  0.09307419  0.01595688  0.01531658\n",
       "SpecialCH      0.05430032      0.372278420 -0.14024001 -0.06149618  0.53220322\n",
       "SpecialMM      0.06150050     -0.156085071 -0.02655727 -0.04711281 -0.05682903\n",
       "LoyalCH        0.15856567      0.234015985  0.03390598  0.06650592  0.13473325\n",
       "SalePriceMM    0.13556579      0.027531649  0.24661025  0.50732508  0.02411274\n",
       "SalePriceCH    0.20825004     -0.274428979  0.58391912  0.38254230 -0.71797454\n",
       "PriceDiff      0.01828848      0.169071705 -0.07380543  0.27559971  0.39743951\n",
       "               DiscMM       SpecialCH   SpecialMM   LoyalCH     SalePriceMM\n",
       "WeekofPurchase  0.217898836  0.05430032  0.06150050  0.15856567  0.13556579\n",
       "StoreID         0.008418251  0.37227842 -0.15608507  0.23401598  0.02753165\n",
       "PriceCH         0.093074190 -0.14024001 -0.02655727  0.03390598  0.24661025\n",
       "PriceMM         0.015956885 -0.06149618 -0.04711281  0.06650592  0.50732508\n",
       "DiscCH          0.015316581  0.53220322 -0.05682903  0.13473325  0.02411274\n",
       "DiscMM          1.000000000  0.17288920  0.50647039 -0.02887638 -0.85354971\n",
       "SpecialCH       0.172889199  1.00000000 -0.15089296  0.12337308 -0.18105135\n",
       "SpecialMM       0.506470389 -0.15089296  1.00000000 -0.09860898 -0.46105828\n",
       "LoyalCH        -0.028876378  0.12337308 -0.09860898  1.00000000  0.05954228\n",
       "SalePriceMM    -0.853549715 -0.18105135 -0.46105828  0.05954228  1.00000000\n",
       "SalePriceCH     0.052917684 -0.53538980  0.02794758 -0.08670414  0.15372641\n",
       "PriceDiff      -0.827333108  0.10989197 -0.44656453  0.10105361  0.85665791\n",
       "               SalePriceCH PriceDiff  \n",
       "WeekofPurchase  0.20825004  0.01828848\n",
       "StoreID        -0.27442898  0.16907171\n",
       "PriceCH         0.58391912 -0.07380543\n",
       "PriceMM         0.38254230  0.27559971\n",
       "DiscCH         -0.71797454  0.39743951\n",
       "DiscMM          0.05291768 -0.82733311\n",
       "SpecialCH      -0.53538980  0.10989197\n",
       "SpecialMM       0.02794758 -0.44656453\n",
       "LoyalCH        -0.08670414  0.10105361\n",
       "SalePriceMM     0.15372641  0.85665791\n",
       "SalePriceCH     1.00000000 -0.37806187\n",
       "PriceDiff      -0.37806187  1.00000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "non_numerical_columns <- c(\"X\", \"Purchase\")\n",
    "cor(train_set[!(names(train_set) %in% non_numerical_columns)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no perfect covariance at first sight, but let's check the relation between Prices, SalePrices and Discounts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "1.50990331349021e-14"
      ],
      "text/latex": [
       "1.50990331349021e-14"
      ],
      "text/markdown": [
       "1.50990331349021e-14"
      ],
      "text/plain": [
       "[1] 1.509903e-14"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "9.54791801177635e-15"
      ],
      "text/latex": [
       "9.54791801177635e-15"
      ],
      "text/markdown": [
       "9.54791801177635e-15"
      ],
      "text/plain": [
       "[1] 9.547918e-15"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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\nhAQB/gES+a7GAJvfywAAAABJRU5ErkJggg==",
      "text/plain": [
       "Plot with title \"Price against Sale Price (MM)\""
      ]
     },
     "metadata": {
      "image/png": {
       "height": 420,
       "width": 420
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We plot Price against SalePrice for MM\n",
    "plot(train_set$PriceMM, train_set$SalePriceMM,\n",
    "    xlab = \"PriceMM\", ylab = \"SalePriceMM\", \n",
    "    main = \"Price against Sale Price (MM)\")\n",
    "\n",
    "\n",
    "# As values of SalePrice are under Price values, \n",
    "# we try to see if the SalePrice + Discount sums to Price\n",
    "sum(abs(train_set$PriceMM - (train_set$SalePriceMM + train_set$DiscMM)))\n",
    "sum(abs(train_set$PriceCH - (train_set$SalePriceCH + train_set$DiscCH)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between `train_set$PriceMM` and `train_set$SalePriceMM + train_set$DiscMM` is so small that we consider it to be the same (approximation errors)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Qualitative variables are:\n",
    "    - `Purchase` ( Although some methods could take care of it, it is better to be transformed to binary 0 / 1 )\n",
    "    - `StoreID`\n",
    "    - `SpecialMM` and `SpecialCH`\n",
    "\n",
    "- Variables to be dropped are:\n",
    "    - `PriceDiff` (difference of two other columns, introduces multicolinearity in the dataset)\n",
    "    - `SalePriceCH` and `SalePriceMM` (or discount columns, for the same reason)\n",
    "    - `X`, that is the index\n",
    "\n",
    "- We keep `WeekOfPurchase` for now as continuous, it might have an effect on the model if there is a trend in the choice for CH or MM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We do the trnasformation we just talked about on the whole dataset \n",
    "# and then split again\n",
    "\n",
    "# Create factors for qualitative variables or non ordered quantitative variables\n",
    "orange_data$Purchase <- factor(orange_data$Purchase)\n",
    "orange_data$Purchase.MM <- as.integer(as.logical(orange_data$Purchase == 'MM'))\n",
    "orange_data$SpecialCH <- factor(orange_data$SpecialCH)\n",
    "orange_data$SpecialMM <- factor(orange_data$SpecialMM)\n",
    "orange_data$StoreID <- factor(orange_data$StoreID)\n",
    "\n",
    "# Drop irrelevant columns\n",
    "to_drop <- c(\"X\", \"PriceDiff\", \"Purchase\", \"SalePriceCH\", \"SalePriceMM\")\n",
    "orange_data <- orange_data[!(names(orange_data) %in% to_drop)]\n",
    "\n",
    "split2 <- sample(c(rep(0, 0.5 * nrow(orange_data)), \n",
    "                   rep(1, 0.25 * nrow(orange_data)), \n",
    "                   rep(2, 0.25 * nrow(orange_data))))\n",
    "\n",
    "final_train_set <- orange_data[split2 == 0, ]\n",
    "final_validation_set <- orange_data[split2 == 1, ]\n",
    "final_test_set <- orange_data[split2 == 2, ]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 6 × 10</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>WeekofPurchase</th><th scope=col>StoreID</th><th scope=col>PriceCH</th><th scope=col>PriceMM</th><th scope=col>DiscCH</th><th scope=col>DiscMM</th><th scope=col>SpecialCH</th><th scope=col>SpecialMM</th><th scope=col>LoyalCH</th><th scope=col>Purchase.MM</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;fct&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;fct&gt;</th><th scope=col>&lt;fct&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;int&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>2</th><td>239</td><td>1</td><td>1.75</td><td>1.99</td><td>0.00</td><td>0.3</td><td>0</td><td>1</td><td>0.600000</td><td>0</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>245</td><td>1</td><td>1.86</td><td>2.09</td><td>0.17</td><td>0.0</td><td>0</td><td>0</td><td>0.680000</td><td>0</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>227</td><td>1</td><td>1.69</td><td>1.69</td><td>0.00</td><td>0.0</td><td>0</td><td>0</td><td>0.400000</td><td>1</td></tr>\n",
       "\t<tr><th scope=row>7</th><td>232</td><td>7</td><td>1.69</td><td>1.99</td><td>0.00</td><td>0.4</td><td>1</td><td>1</td><td>0.972182</td><td>0</td></tr>\n",
       "\t<tr><th scope=row>8</th><td>234</td><td>7</td><td>1.75</td><td>1.99</td><td>0.00</td><td>0.4</td><td>1</td><td>0</td><td>0.977746</td><td>0</td></tr>\n",
       "\t<tr><th scope=row>16</th><td>278</td><td>7</td><td>2.06</td><td>2.13</td><td>0.00</td><td>0.0</td><td>0</td><td>0</td><td>0.795200</td><td>0</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 6 × 10\n",
       "\\begin{tabular}{r|llllllllll}\n",
       "  & WeekofPurchase & StoreID & PriceCH & PriceMM & DiscCH & DiscMM & SpecialCH & SpecialMM & LoyalCH & Purchase.MM\\\\\n",
       "  & <int> & <fct> & <dbl> & <dbl> & <dbl> & <dbl> & <fct> & <fct> & <dbl> & <int>\\\\\n",
       "\\hline\n",
       "\t2 & 239 & 1 & 1.75 & 1.99 & 0.00 & 0.3 & 0 & 1 & 0.600000 & 0\\\\\n",
       "\t3 & 245 & 1 & 1.86 & 2.09 & 0.17 & 0.0 & 0 & 0 & 0.680000 & 0\\\\\n",
       "\t4 & 227 & 1 & 1.69 & 1.69 & 0.00 & 0.0 & 0 & 0 & 0.400000 & 1\\\\\n",
       "\t7 & 232 & 7 & 1.69 & 1.99 & 0.00 & 0.4 & 1 & 1 & 0.972182 & 0\\\\\n",
       "\t8 & 234 & 7 & 1.75 & 1.99 & 0.00 & 0.4 & 1 & 0 & 0.977746 & 0\\\\\n",
       "\t16 & 278 & 7 & 2.06 & 2.13 & 0.00 & 0.0 & 0 & 0 & 0.795200 & 0\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 6 × 10\n",
       "\n",
       "| <!--/--> | WeekofPurchase &lt;int&gt; | StoreID &lt;fct&gt; | PriceCH &lt;dbl&gt; | PriceMM &lt;dbl&gt; | DiscCH &lt;dbl&gt; | DiscMM &lt;dbl&gt; | SpecialCH &lt;fct&gt; | SpecialMM &lt;fct&gt; | LoyalCH &lt;dbl&gt; | Purchase.MM &lt;int&gt; |\n",
       "|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| 2 | 239 | 1 | 1.75 | 1.99 | 0.00 | 0.3 | 0 | 1 | 0.600000 | 0 |\n",
       "| 3 | 245 | 1 | 1.86 | 2.09 | 0.17 | 0.0 | 0 | 0 | 0.680000 | 0 |\n",
       "| 4 | 227 | 1 | 1.69 | 1.69 | 0.00 | 0.0 | 0 | 0 | 0.400000 | 1 |\n",
       "| 7 | 232 | 7 | 1.69 | 1.99 | 0.00 | 0.4 | 1 | 1 | 0.972182 | 0 |\n",
       "| 8 | 234 | 7 | 1.75 | 1.99 | 0.00 | 0.4 | 1 | 0 | 0.977746 | 0 |\n",
       "| 16 | 278 | 7 | 2.06 | 2.13 | 0.00 | 0.0 | 0 | 0 | 0.795200 | 0 |\n",
       "\n"
      ],
      "text/plain": [
       "   WeekofPurchase StoreID PriceCH PriceMM DiscCH DiscMM SpecialCH SpecialMM\n",
       "2  239            1       1.75    1.99    0.00   0.3    0         1        \n",
       "3  245            1       1.86    2.09    0.17   0.0    0         0        \n",
       "4  227            1       1.69    1.69    0.00   0.0    0         0        \n",
       "7  232            7       1.69    1.99    0.00   0.4    1         1        \n",
       "8  234            7       1.75    1.99    0.00   0.4    1         0        \n",
       "16 278            7       2.06    2.13    0.00   0.0    0         0        \n",
       "   LoyalCH  Purchase.MM\n",
       "2  0.600000 0          \n",
       "3  0.680000 0          \n",
       "4  0.400000 1          \n",
       "7  0.972182 0          \n",
       "8  0.977746 0          \n",
       "16 0.795200 0          "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(final_train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "glm(formula = Purchase.MM ~ ., family = \"binomial\", data = final_train_set)\n",
       "\n",
       "Deviance Residuals: \n",
       "    Min       1Q   Median       3Q      Max  \n",
       "-2.8793  -0.5545  -0.2601   0.5669   2.7741  \n",
       "\n",
       "Coefficients:\n",
       "               Estimate Std. Error z value Pr(>|z|)    \n",
       "(Intercept)     0.97576    2.64650   0.369 0.712354    \n",
       "WeekofPurchase  0.01697    0.01556   1.091 0.275181    \n",
       "StoreID2        0.51292    0.40179   1.277 0.201743    \n",
       "StoreID3        0.50065    0.54789   0.914 0.360832    \n",
       "StoreID4        0.27029    0.58106   0.465 0.641804    \n",
       "StoreID7       -0.18345    0.40456  -0.453 0.650214    \n",
       "PriceCH         3.48544    2.55390   1.365 0.172330    \n",
       "PriceMM        -4.42241    1.29922  -3.404 0.000664 ***\n",
       "DiscCH         -4.44460    1.45730  -3.050 0.002289 ** \n",
       "DiscMM          1.65330    0.74403   2.222 0.026278 *  \n",
       "SpecialCH1      0.01574    0.47320   0.033 0.973469    \n",
       "SpecialMM1      0.21249    0.37317   0.569 0.569068    \n",
       "LoyalCH        -6.25335    0.58299 -10.726  < 2e-16 ***\n",
       "---\n",
       "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
       "\n",
       "(Dispersion parameter for binomial family taken to be 1)\n",
       "\n",
       "    Null deviance: 712.19  on 534  degrees of freedom\n",
       "Residual deviance: 419.44  on 522  degrees of freedom\n",
       "AIC: 445.44\n",
       "\n",
       "Number of Fisher Scoring iterations: 5\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "logit1 <- glm(Purchase.MM ~., data = final_train_set, family = \"binomial\")\n",
    "summary(logit1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Two factors are highly significant: `PriceMM` (negative effect on probability of purchasing at MM, which seems to be relevant), `LoyalCH` (negative effect on probability of purchasing at MM, which seems to be relevant too)\n",
    "- One factor is significant `DiscCH`, negative effect on probability of purchasing at MM, which is relevant\n",
    "- One factor is a bit significant `DiscMM`, positive effect on probability of purchasing at MM, which is relevant\n",
    "- The other factors in this model are not significant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.001"
      ],
      "text/latex": [
       "0.001"
      ],
      "text/markdown": [
       "0.001"
      ],
      "text/plain": [
       "[1] 0.001"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "10 x 1 sparse Matrix of class \"dgCMatrix\"\n",
       "                         s0\n",
       "(Intercept)     0.055351790\n",
       "WeekofPurchase  0.007507834\n",
       "StoreID        -0.109458712\n",
       "PriceCH         4.985522948\n",
       "PriceMM        -4.018996880\n",
       "DiscCH         -4.323224722\n",
       "DiscMM          1.510671551\n",
       "SpecialCH       .          \n",
       "SpecialMM       0.192670649\n",
       "LoyalCH        -6.190131340"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y <- final_train_set$Purchase.MM\n",
    "x <- data.matrix(final_train_set[,-10])\n",
    "\n",
    "lmbda <- c(0.001, 0.01, 0.1, 1, 10, 100, 1000)\n",
    "# alpha = 1 is lasso regularization\n",
    "\n",
    "# we have to chose type.measure between \"auc\" and \"class\"\n",
    "# we choose \"class\" for now, and will change the threshold if needed \n",
    "# in further exploration\n",
    "\n",
    "cv.out <- cv.glmnet(x,y, alpha = 1, lambda = lmbda, type.measure = \"class\", \n",
    "                    nfolds = 5, family = \"binomial\")\n",
    "\n",
    "bestlam <- cv.out$lambda.min\n",
    "bestlam\n",
    "\n",
    "# final model on training data\n",
    "best.lasso <- glmnet(x,y, alpha = 1, lambda = bestlam, type.measure = \"class\",\n",
    "                     family = \"binomial\")\n",
    "coef(best.lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best linear regression is obtained using $\\lambda = 0.001$ and gives coefficients as can be seen above (shuts out 1 coefficient)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Confusion Matrix and Statistics\n",
       "\n",
       "          Reference\n",
       "Prediction   0   1\n",
       "         0 288  48\n",
       "         1  42 157\n",
       "                                          \n",
       "               Accuracy : 0.8318          \n",
       "                 95% CI : (0.7973, 0.8625)\n",
       "    No Information Rate : 0.6168          \n",
       "    P-Value [Acc > NIR] : <2e-16          \n",
       "                                          \n",
       "                  Kappa : 0.6421          \n",
       "                                          \n",
       " Mcnemar's Test P-Value : 0.5982          \n",
       "                                          \n",
       "            Sensitivity : 0.8727          \n",
       "            Specificity : 0.7659          \n",
       "         Pos Pred Value : 0.8571          \n",
       "         Neg Pred Value : 0.7889          \n",
       "             Prevalence : 0.6168          \n",
       "         Detection Rate : 0.5383          \n",
       "   Detection Prevalence : 0.6280          \n",
       "      Balanced Accuracy : 0.8193          \n",
       "                                          \n",
       "       'Positive' Class : 0               \n",
       "                                          "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "'The training classification error is: 0.168224299065421'"
      ],
      "text/latex": [
       "'The training classification error is: 0.168224299065421'"
      ],
      "text/markdown": [
       "'The training classification error is: 0.168224299065421'"
      ],
      "text/plain": [
       "[1] \"The training classification error is: 0.168224299065421\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "LDA <- lda(y ~ x)\n",
    "train_predictions <- predict(LDA, final_train_set[,-10])\n",
    "train_predictions <- as.integer(train_predictions$class) - 1 \n",
    "# formating to compare to y\n",
    "classification_error <- sum(abs(y - train_predictions))/length(y)\n",
    "# Output a confusion matrix using caret package\n",
    "\n",
    "confusionMatrix(factor(train_predictions), factor(y))\n",
    "sprintf(\"The training classification error is: %s\", classification_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'Best k is 39 with error of 0.186915887850467 on the training dataset'"
      ],
      "text/latex": [
       "'Best k is 39 with error of 0.186915887850467 on the training dataset'"
      ],
      "text/markdown": [
       "'Best k is 39 with error of 0.186915887850467 on the training dataset'"
      ],
      "text/plain": [
       "[1] \"Best k is 39 with error of 0.186915887850467 on the training dataset\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Here we use LOOCV(leave one out cross validation), \n",
    "# and try all values for k from 1 to 100 \n",
    "# (we assume that afterwards the bias is too high)\n",
    "# we need to scale the data for knn\n",
    "x.knn <- scale(x, center = TRUE, scale = TRUE)\n",
    "error <- 1\n",
    "best.k <- 1\n",
    "for (k in seq(1, 100, 1)){\n",
    "    knn.cv.out <- knn.cv(x.knn,y, k = k)\n",
    "    knn.cv.out <- as.integer(knn.cv.out) - 1\n",
    "    current.error <- sum(abs(y - knn.cv.out))/length(y)\n",
    "    if (current.error < error){\n",
    "        error <- current.error\n",
    "        best.k <- k\n",
    "    }\n",
    "}\n",
    "sprintf(\"Best k is %s with error of %s on the training dataset\", best.k, error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'The logistic regression classification error on the validation set is : 0.258858025474912'"
      ],
      "text/latex": [
       "'The logistic regression classification error on the validation set is : 0.258858025474912'"
      ],
      "text/markdown": [
       "'The logistic regression classification error on the validation set is : 0.258858025474912'"
      ],
      "text/plain": [
       "[1] \"The logistic regression classification error on the validation set is : 0.258858025474912\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Best logistic regression, fitted on training set, tested on validation set\n",
    "\n",
    "#data\n",
    "y <- final_train_set$Purchase.MM\n",
    "x <- final_train_set[,-10]\n",
    "\n",
    "val_y <- final_validation_set$Purchase.MM\n",
    "val_x <- final_validation_set[,-10]\n",
    "\n",
    "# formula\n",
    "fmla <- paste(\"Purchase.MM ~\", paste(names(final_train_set[, -10]), collapse = \" + \"))\n",
    "\n",
    "#model\n",
    "logit1 <- glm(fmla, data = final_train_set, family = \"binomial\")\n",
    "#predictions\n",
    "val_predictions <- predict(logit1, val_x, type = \"response\")\n",
    "#error\n",
    "logit_val_error <- sum(abs(val_y - val_predictions))/length(val_y)\n",
    "sprintf(\"The logistic regression classification error on the validation set is : %s\",\n",
    "        logit_val_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'The lasso regression classification error on the validation set is : 0.287272788246781'"
      ],
      "text/latex": [
       "'The lasso regression classification error on the validation set is : 0.287272788246781'"
      ],
      "text/markdown": [
       "'The lasso regression classification error on the validation set is : 0.287272788246781'"
      ],
      "text/plain": [
       "[1] \"The lasso regression classification error on the validation set is : 0.287272788246781\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Best Logistic with LASSO regression, fitted on training set, tested on validation set\n",
    "\n",
    "#data\n",
    "y <- final_train_set$Purchase.MM\n",
    "x <- data.matrix(final_train_set[,-10])\n",
    "\n",
    "val_y <- final_validation_set$Purchase.MM\n",
    "val_x <- data.matrix(final_validation_set[,-10])\n",
    "\n",
    "#model\n",
    "best.lasso <- glmnet(x,y, alpha = 1, lambda = bestlam)\n",
    "\n",
    "#prediction\n",
    "lasso_predictions <- predict(best.lasso, newx = val_x)\n",
    "\n",
    "#error\n",
    "lasso_val_error <- sum(abs(val_y - lasso_predictions))/length(val_y)\n",
    "sprintf(\"The lasso regression classification error on the validation set is : %s\", \n",
    "        lasso_val_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'The LDA classification error on the validation set is : 0.191011235955056'"
      ],
      "text/latex": [
       "'The LDA classification error on the validation set is : 0.191011235955056'"
      ],
      "text/markdown": [
       "'The LDA classification error on the validation set is : 0.191011235955056'"
      ],
      "text/plain": [
       "[1] \"The LDA classification error on the validation set is : 0.191011235955056\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# LDA, fitted on training set, tested on validation set\n",
    "\n",
    "#data\n",
    "y <- final_train_set$Purchase.MM\n",
    "x <- data.matrix(final_train_set[,-10])\n",
    "\n",
    "val_y <- final_validation_set$Purchase.MM\n",
    "val_x <- data.matrix(final_validation_set[,-10])\n",
    "\n",
    "#model\n",
    "LDA <- lda(y~x)\n",
    "\n",
    "x <- val_x # trick to avoid an error due to the names of the variables\n",
    "\n",
    "#prediction\n",
    "lda_predictions <- predict(LDA, data.frame(x))\n",
    "lda_predictions <- as.integer(lda_predictions$class) - 1 \n",
    "# formating to compare to val_y\n",
    "lda_val_error <- sum(abs(val_y - lda_predictions))/length(val_y)\n",
    "sprintf(\"The LDA classification error on the validation set is : %s\", lda_val_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'The KNN classification error on the validation set is : 0.213483146067416'"
      ],
      "text/latex": [
       "'The KNN classification error on the validation set is : 0.213483146067416'"
      ],
      "text/markdown": [
       "'The KNN classification error on the validation set is : 0.213483146067416'"
      ],
      "text/plain": [
       "[1] \"The KNN classification error on the validation set is : 0.213483146067416\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Best KNN (k = 39) on training set, tested on validation set\n",
    "\n",
    "#data\n",
    "y <- final_train_set$Purchase.MM\n",
    "x <- scale(data.matrix(final_train_set[,-10]))\n",
    "\n",
    "val_y <- final_validation_set$Purchase.MM\n",
    "val_x <- scale(data.matrix(final_validation_set[,-10]))\n",
    "\n",
    "#model and prediction\n",
    "knn_predictions <- knn(x, val_x, y, k = best.k)\n",
    "knn_predictions <- as.integer(knn_predictions) - 1\n",
    "#error\n",
    "knn_val_error <- sum(abs(val_y - knn_predictions))/length(val_y)\n",
    "sprintf(\"The KNN classification error on the validation set is : %s\", knn_val_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the validation set classification errors, the ranking of the models is the following:\n",
    "1. LDA\n",
    "2. KNN\n",
    "3. Logit\n",
    "4. Logit with LASSO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "535"
      ],
      "text/latex": [
       "535"
      ],
      "text/markdown": [
       "535"
      ],
      "text/plain": [
       "[1] 535"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "267"
      ],
      "text/latex": [
       "267"
      ],
      "text/markdown": [
       "267"
      ],
      "text/plain": [
       "[1] 267"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "802"
      ],
      "text/latex": [
       "802"
      ],
      "text/markdown": [
       "802"
      ],
      "text/plain": [
       "[1] 802"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0"
      ],
      "text/latex": [
       "0"
      ],
      "text/markdown": [
       "0"
      ],
      "text/plain": [
       "[1] 0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# build a dataframe that has train and validation data\n",
    "final_train_and_validation_set <- rbind(final_train_set, final_validation_set)\n",
    "# check that everything went well\n",
    "length(final_train_set[, 1])\n",
    "length(final_validation_set[, 1])\n",
    "length(final_train_and_validation_set[, 1])\n",
    "sum(is.na(final_train_and_validation_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'The LDA classification error on the test set is : 0.138059701492537'"
      ],
      "text/latex": [
       "'The LDA classification error on the test set is : 0.138059701492537'"
      ],
      "text/markdown": [
       "'The LDA classification error on the test set is : 0.138059701492537'"
      ],
      "text/plain": [
       "[1] \"The LDA classification error on the test set is : 0.138059701492537\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# LDA, fitted on training + validation set, tested on test set\n",
    "\n",
    "#data\n",
    "y <- final_train_and_validation_set$Purchase.MM\n",
    "x <- data.matrix(final_train_and_validation_set[,-10])\n",
    "\n",
    "test_y <- final_test_set$Purchase.MM\n",
    "test_x <- data.matrix(final_test_set[,-10])\n",
    "\n",
    "#model\n",
    "final_LDA <- lda(y~x)\n",
    "\n",
    "x <- test_x # trick to avoid an error due to the names of the variables\n",
    "\n",
    "#prediction\n",
    "lda_test_predictions <- predict(final_LDA, data.frame(x))\n",
    "lda_test_predictions <- as.integer(lda_test_predictions$class) -1 \n",
    "# formating to compare to test_y\n",
    "\n",
    "lda_test_error <- sum(abs(test_y - lda_test_predictions))/length(test_y)\n",
    "sprintf(\"The LDA classification error on the test set is : %s\", lda_test_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying to predict CH customers (label = 0 thus negative)\n",
    "# a True Negative (predict = CH, and CH customer) leads to $3.50 profit\n",
    "# a False Negative (Predict = CH, and MM customer) is a waste of $0.50 \n",
    "\n",
    "# LDA, fitted on training + validation set, tested on test set\n",
    "\n",
    "#data\n",
    "y <- final_train_and_validation_set$Purchase.MM\n",
    "x <- data.matrix(final_train_and_validation_set[,-10])\n",
    "\n",
    "test_y <- final_test_set$Purchase.MM\n",
    "test_x <- data.matrix(final_test_set[,-10])\n",
    "\n",
    "#model\n",
    "final_LDA <- lda(y~x)\n",
    "\n",
    "x <- test_x # trick to avoid an error due to the names of the variables\n",
    "\n",
    "#prediction\n",
    "lda_test_predictions <- predict(final_LDA, data.frame(x))\n",
    "MM.prob <- lda_test_predictions$posterior[, 2]\n",
    "\n",
    "step <- 0.005\n",
    "thresholds <- seq(0, 1, step)\n",
    "FP_points <- seq(0, 1, step)\n",
    "TP_points <- seq(0, 1, step)\n",
    "possible_profits <- seq(0, 1, step)\n",
    "\n",
    "i <- 0\n",
    "\n",
    "for (threshold in thresholds){\n",
    "    lda_test_predictions <- predict(final_LDA, data.frame(x))\n",
    "    MM.prob <- lda_test_predictions$posterior[, 2]\n",
    "    lda_test_predictions <- as.integer(MM.prob > threshold)\n",
    "    # create a vector where\n",
    "    # -0.5 is False Negative\n",
    "    # 0 is True Negative\n",
    "    # 0.5 is True Positive\n",
    "    # 1 False Positive\n",
    "    classification_vector <- lda_test_predictions - 0.5*test_y\n",
    "    FN <- sum(classification_vector == -0.5)\n",
    "    TN <- sum(classification_vector == 0)\n",
    "    TP <- sum(classification_vector == 0.5)\n",
    "    FP <- sum(classification_vector == 1)\n",
    "    \n",
    "    profit <- 3.50*TN - 0.5*FN\n",
    "\n",
    "    FP_points[i] <- FP/(FP + TN)\n",
    "    TP_points[i] <- TP/(TP + FN)\n",
    "    possible_profits[i] <- profit\n",
    "    i <- i + 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAM1BMVEUAAABNTU1oaGh8fHyM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\nRU5ErkJggg==",
      "text/plain": [
       "Plot with title \"ROC curve\""
      ]
     },
     "metadata": {
      "image/png": {
       "height": 420,
       "width": 420
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(FP_points, TP_points, \n",
    "    xlab = \"False Positive Rate\", ylab = \"True Positive Rate\", \n",
    "    main = \"ROC curve\", col = 2)\n",
    "abline(0, 1, col = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'The best threshold is 0.9 and gives a max payoff of 527'"
      ],
      "text/latex": [
       "'The best threshold is 0.9 and gives a max payoff of 527'"
      ],
      "text/markdown": [
       "'The best threshold is 0.9 and gives a max payoff of 527'"
      ],
      "text/plain": [
       "[1] \"The best threshold is 0.9 and gives a max payoff of 527\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAM1BMVEUAAABNTU1oaGh8fHyM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",
      "text/plain": [
       "Plot with title \"Payoff vs. Classification Thresholds\""
      ]
     },
     "metadata": {
      "image/png": {
       "height": 420,
       "width": 420
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(thresholds, possible_profits, \n",
    "    xlab = \"Thresholds\", ylab = \"Payoff\", \n",
    "    main = \"Payoff vs. Classification Thresholds\")\n",
    "best.threshold <- thresholds[which.max(possible_profits)]\n",
    "max.profit <- max(possible_profits)\n",
    "abline(v = best.threshold, col = 2)\n",
    "legend(\"topleft\", c(\"Best threshold\"), lty = c(1), col = c(2))\n",
    "sprintf(\"The best threshold is %s and gives a max payoff of %s\", best.threshold,\n",
    "        max.profit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are multiple ways of building recommendation systems using logistic regression and shrinkage. For all of them we imagine that we have our dataset with the music represented in features with weights from 1 to 5. \n",
    "- **The personnal recommendation system** : Let's suppose that a user has liked 1000 songs, and disliked 1000. We can join our previous dataset to this list of liked songs in order to build a logistic regression / lasso model. This model will predict the probability that this particular user likes another song that is not liked yet\n",
    "- **The grouped recommendation system** : The problem of this previous method is that it requires every user to like/dislike a lot of songs (build the target column of the dataset in fact). Another way could be to clusterize listeners based on their tastes and then use the information on the whole cluster as a target variable (if in the cluster of people there are more likes than dislikes for a particular song, then we assign to this song a 1; and the opposite way). But this assumes that we are able to cluster people (and algorithms are close to KNN, so we might know KNN as well)\n",
    "\n",
    "\n",
    "**Cons of logistic regression**\n",
    "- In either case, as Logistic Regression is supervised learning, we need to create a target variable. This can be liked songs as mentionned, or maybe a binary variable derived from the number of time someone has listened to a song, etc.\n",
    "- For each user or group of user we need to create a different model\n",
    "- The boundaries for this method are linear, but we are not sure that it reflects true musical tastes\n",
    "- Logistics regression is based on the assumption of linearity between the logit of the outcome and the independent variables which might not be true in reality.\n",
    "\n",
    "**Pros of logistic regression**\n",
    "- Easy to use and efficient to train\n",
    "- Good first model\n",
    "- Gives statistical significance of variables\n",
    "- It can easily extend to multiple classes and a natural probabilistic view of class predictions\n",
    "- It makes no assumptions about distribution of classes in feature space"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7bef2f2c4380e55013fb26998d0961761eb21b9fedf02e94003da56b94b0b2db"
  },
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
