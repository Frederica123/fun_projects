---
title: "IEOR4650 Assignment 2"

output:
  pdf_document: default
  html_document: default
---

## Question 1

```{r}
# Read data
df <- read.csv("GE.csv")
df
```


### (a) 1D/3D/5D returns

```{r}
library("dplyr")

df$oneday.return <- ((df$Opening.Price-lag(df$Opening.Price,1))/lag(df$Opening.Price,1))
df$threeday.return <- ((df$Opening.Price-lag(df$Opening.Price,3))/lag(df$Opening.Price,3))
df$fiveday.return <- ((df$Opening.Price-lag(df$Opening.Price,5))/lag(df$Opening.Price,5))

print(paste("Average 1D return is", mean(df$oneday.return[2:length(df$Opening.Price)])))
print(paste("Average 3D return is", mean(df$threeday.return[4:length(df$Opening.Price)])))
print(paste("Average 5D return is", mean(df$fiveday.return[6:length(df$Opening.Price)])))

```


### (b) Predicting next-day return with three features
```{r}
# Compute next-day return
df$nextday.return <- (lead(df$Opening.Price,1) - df$Opening.Price)/df$Opening.Price

# Convert Date variable
df$Date <- as.character(df$Date)
df$Date <- as.Date(df$Date, format="%m/%d/%Y")
df$Year <- as.numeric(format(df$Date, '%Y'))

# Derive training data
train <- df[(df$Year=='2016') 
            & (!is.na(df$oneday.return)) 
            & (!is.na(df$threeday.return)) 
            & (!is.na(df$fiveday.return))
            & (!is.na(df$nextday.return)),]

# Linear regression
stock.model1 <- lm(nextday.return ~ oneday.return + threeday.return + fiveday.return, data = train)
summary(stock.model1)

```

In the linear regression model, none of the three features has a significant effect and the adjusted R^2^ is as low as 0.00496, which implies that it is not a good fit.

### (c) Predicting next-day return with one feature
```{r}
# Linear regression
stock.model2 <- lm(nextday.return ~ oneday.return, data = train)
summary(stock.model2)

```

In the linear regression model with only one-day return, the model is slightly better than the previous model, with R^2^ = 0.01032 and one-day return as a feature has some significance. However, overall the model is not good as we can see from the low adjusted R^2^.

### (d) Return on investment with long-short strategy

```{r}
library('comprehenr')

# Derive validation set
df$Month <- as.numeric(format(df$Date, '%m'))
val <- df[df$Year == '2017' & df$Month <= 6,]

# Implement long-short strategy with first model
val.fitted.1 <- predict(stock.model1, newdata = val)
val$strategy1 <- to_vec(for (i in 1:length(val.fitted.1)) if (val.fitted.1[i]>=0) 1 else -1)
val$long.short.return1 <- val$strategy1 * val$nextday.return
model1.value <- 1
for (i in 1:length(val.fitted.1)) {
  model1.value <- model1.value * (1+val$long.short.return1[i])
}
model1.return <- (model1.value - 1)
print(paste("Return on investment of model 1 is ", model1.return))

# Implement long-short strategy with second model
val.fitted.2 <- predict(stock.model2, newdata = val)
val$strategy2 <- to_vec(for (i in 1:length(val.fitted.2)) if (val.fitted.2[i]>=0) 1 else -1)
val$long.short.return2 <- val$strategy2 * val$nextday.return
model2.value <- 1
for (i in 1:length(val.fitted.2)) {
  model2.value <- model2.value * (1+val$long.short.return2[i])
}
model2.return <- (model2.value - 1)
print(paste("Return on investment of model 2 is ", model2.return))

```

### (e) Retrain the model with 1.5 years of data

```{r}

# Derive training set and validation set
train1 <- rbind(df[df$Year=='2016',], df[df$Year=='2017' & df$Month<=6,])
val1 <- df[df$Year=='2017' & df$Month>6,]

# Train the model
stock.model3 <- lm(nextday.return ~ oneday.return + threeday.return + fiveday.return, data = train1)
summary(stock.model3)

# Implement the long-short strategy
val.fitted.3 <- predict(stock.model3, newdata = val1)
val1$strategy <- to_vec(for (i in 1:length(val.fitted.3)) if (val.fitted.3[i]>=0) 1 else -1)
val1$long.short.return <- val1$strategy * val1$nextday.return

values <- c(1)
model3.value <- 1
for (i in 1:(length(val.fitted.3)-1)) { # last element in nextday.return is NA
  model3.value <- model3.value * (1+val1$long.short.return[i])
  values <- append(values, model3.value)
}

# Average value and standard deviation of 1D return
oneday.return <- ((values-lag(values,1))/lag(values,1))
print(paste("Average 1D return is ", mean(oneday.return[2:length(oneday.return)])))
print(paste("Standard deviation of 1D return is ", sd(oneday.return[2:length(oneday.return)])))

# Return on investment
model3.return <- (model3.value - 1)
print(paste("Return on investment of model 2 is", model3.return))


```

### (f) Evaluating strategy performance
The strategy performs better if we take 1.5-year data as training set instead of taking 1-year data as training set.

We may improve the model by including other features that may have higher correlations with stock prices, such as, financial information, employer information, and competitor stock prices. Additionally, we may try different models, besides linear regression, to capture the relationship between the above features with future returns.

### (g) Choice of an investor
I would prefer to take 1.5 years of training data and apply it for the next 0.5 years. If the training data is smaller compared with test data, the training model is less convincing.

## Question 2

### (a) Data preprocessing and description
```{r}
# Read data
college.data <- read.csv("CollegeData.csv")

# Remove rows with missing entries
college.data <- na.omit(college.data)

# Add square root of dollar amount
college.data$COSTT4_A.sqrt <- sqrt(college.data$COSTT4_A)
college.data$TUITIONFEE_OUT.sqrt <- sqrt(college.data$TUITIONFEE_OUT)
college.data$TUITFTE.sqrt <- sqrt(college.data$TUITFTE)
college.data$AVGFACSAL.sqrt <- sqrt(college.data$AVGFACSAL)

# Add interaction terms (as.numeric to prevent integer overflow)
college.data$COSTT4_A.TUITIONFEE_OUT.int <- 
  as.numeric(college.data$COSTT4_A)*as.numeric(college.data$TUITIONFEE_OUT)
college.data$COSTT4_A.TUITFTE.int <- 
  as.numeric(college.data$COSTT4_A)*as.numeric(college.data$TUITFTE)
college.data$COSTT4_A.AVGFACSAL.int <- 
  as.numeric(college.data$COSTT4_A)*as.numeric(college.data$AVGFACSAL)
college.data$TUITIONFEE_OUT.TUITFTE.int <- 
  as.numeric(college.data$TUITIONFEE_OUT)*as.numeric(college.data$TUITFTE)
college.data$TUITIONFEE_OUT.AVGFACSAL.int <- 
  as.numeric(college.data$TUITIONFEE_OUT)*as.numeric(college.data$AVGFACSAL)
college.data$TUITFTE.AVGFACSAL.int <- 
  as.numeric(college.data$TUITFTE)*as.numeric(college.data$AVGFACSAL)

# Divide data into two parts
set.seed(1234)
train <- sample(1:nrow(college.data), 0.75*nrow(college.data))
train.college.dat <- college.data[train,2:ncol(college.data)] 
# exclude the first column (which is the names of schools)
test.college.dat <- college.data[-train,2:ncol(college.data)]

# Data description
print(paste("Total rows of data:", nrow(train.college.dat)+nrow(test.college.dat)))
print(paste("Number of covariates: ", (ncol(train.college.dat))-1))
print(paste("Mean of SAT_AVG:", mean(train.college.dat$SAT_AVG)))
```

### (b) Best subset size via 5-fold validation and forward stepwise selection

```{r}

library("caret")
library("leaps")
library("comprehenr")
library("glmnet")

# Normalization
for (col in 1:ncol(train.college.dat)) {
train.college.dat[col] <- (train.college.dat[col]-min(train.college.dat[col]))/
  (max(train.college.dat[col])-min(train.college.dat[col]))
}

for (col in 1:ncol(test.college.dat)) {
test.college.dat[col] <- (test.college.dat[col]-min(test.college.dat[col]))/
  (max(test.college.dat[col])-min(test.college.dat[col]))
}

# Prediction function used in subset selection
predict.regsubsets <- function(model,newdata,t){
  form <- as.formula(model$call[[2]]) 
  mat <- model.matrix(form, newdata)  
  coefi <- coef(model,id=t)           # obtain the coefficients of the model corresponding to t
  xvars <- names(coefi)               # names of variables in the best subset
  pred <- mat[,xvars]%*%coefi         # %*% performs matrix multiplication
  return(pred)
}

set.seed(1234)
nfolds <- 5 # number of folds in cross validation
folds <- sample(1:nfolds, nrow(train.college.dat), replace=TRUE) #Assign fold to each row
total.features <- 8

cv.errors <- matrix(NA, nrow=nfolds, ncol=total.features)

for(j in 1:nfolds){
  
  best.fit <- regsubsets(SAT_AVG~., data=train.college.dat[folds!=j,], nvmax=total.features)
  actual <- train.college.dat[folds==j,]$SAT_AVG
  
  for(t in 1:total.features){
    pred <- predict.regsubsets(best.fit, newdata = train.college.dat[folds==j,], t = t)
    cv.errors[j,t] <- mean((actual - pred)^2)
  }
}

# Find the best number of features with the lowest mse
mean.cv.errors <- apply(cv.errors, 2, mean) # apply() used to 'apply' a function along an axis
mean.cv.errors
plot(mean.cv.errors)

best.size <- which.min(mean.cv.errors)
print(paste("The best subset size is ",best.size))




```


### (c) Find the bests model with cv lasso regression

```{r}
# Set of lambda values we want to consider
lambda.grid <- c(0,10^(-4:3))

# Prepare the training data
x <- model.matrix(SAT_AVG ~ ., train.college.dat)
y <- train.college.dat$SAT_AVG

# cv.glmnet() performs k-fold cross validation for glmnet()
cv.out <- cv.glmnet(x, y, alpha=1, lambda=lambda.grid, nfolds=5) 

# plot of MSE vs Log(Lambda)
plot(cv.out)

# One can extract the best lambda
bestlam <- cv.out$lambda.min
print(paste("The best choice of lambda is ",bestlam))
```

### (d) Retrain the best models and test the models

```{r}

# Prediction function used in subset selection
predict.regsubsets <- function(model,newdata,t){
  form <- as.formula(model$call[[2]]) 
  mat <- model.matrix(form, newdata)  
  coefi <- coef(model,id=t)           # obtain the coefficients of the model corresponding to t
  xvars <- names(coefi)               # names of variables in the best subset
  pred <- mat[,xvars]%*%coefi         # %*% performs matrix multiplication
  return(pred)
}

# PART(B) Retrain the best forward-selection model with best subset size
best.subset.full <- regsubsets(SAT_AVG ~ ., data=train.college.dat, nvmax=best.size)

# evaluate MSE of final chosen model on test dataset (step 5)
pred <- predict.regsubsets(best.subset.full, test.college.dat, best.size)
actual <- test.college.dat$SAT_AVG
print(paste("The MSE of best forward-selection model is: ", mean((actual - pred)^2)))


# PART(C) Retrain the lasso regression model with best lambda
lasso.full <- glmnet(x, y, alpha=0, lambda=bestlam)

# Evaluate this model on the test set
x_test <- model.matrix(SAT_AVG ~ ., test.college.dat)
y_test <- test.college.dat$SAT_AVG
pred <- predict(lasso.full, newx = x_test) # predict.glmnet also requires model matrix as input
actual <- y_test
print(paste("The MSE of lasso regression model is",mean((actual-pred)^2)))

```

### (e) Insights on final models

```{r}

# Adjusted R-square of the best subset-selection model
print(paste("Adjusted R-square of the best subset-selection model: ", 
            max(summary(best.subset.full)$adjr2)))

# Coefficient of the best subset-selection model
coef(best.subset.full, best.size)

# Coefficient of the lasso regression model
lasso.full$beta

```

The best subset-selection model has a relatively good fit with R^2^ 0.72. In this model, the number of enrollment of undergrad degree-seeking students (UGDS), proportion of faculty that is full-time (PSTFAC), completion rate for first-time, full-time students at four-year institutions (C150_4), and average cost of attendance (COSTT4_A) all have a positive effect on the school's average SAT score of admitted students. Out-of-state tuition and fees (TUITIONFEE_OUT) and average faculty salary (AVGFACSAL) are negatively affecting average SAT.

The regression results imply that 
1) As a future parent, the features mentioned above can be some indicators of a school's measure of quality. Especially features like proportion of full-time faculty and completion rate for first-time and full-time students are good indicators. Moreover, for a future parent, it can be also useful to investigate factors related to career, like employment rate of admitted students, proportion of students seeking for higher degrees after graduation, and factors related to the study environment, like how many hours on average a student spend in library.
2) If secretary of education is to fund schools of high quality, then he or she may consider schools with high average cost of attendance (COSTT4_A) because COSTT4_A has a high positive correlation with SAT_AVG, which is a indicator of school quality. Moreover, further investigation can be done among schools which has high educational performances but awful financial situations.






